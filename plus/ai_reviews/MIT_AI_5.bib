@techreport{AITR-1370,
 author       = {Balasubramanian, Vijay},
 title        = {Equivalence and Reduction of Hidden Markov Models},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report studies when and why two Hidden Markov Models (HMMs) may represent the same stochastic process. HMMs are characterized in terms of equivalence classes whose elements represent identical stochastic processes. This characterization yields polynomial time algorithms to detect equivalent HMMs. We also find fast algorithms to reduce HMMs to essentially unique and minimal canonical representations. The reduction to a canonical form leads to the definition of 'Generalized Markov Models' which are essentially HMMs without the positivity constraint on their parameters. We discuss how this generalization can yield more parsimonious representations of stochastic processes at the cost of the probabilistic interpretation of the model parameters.},
 month        = {jan~6},
 number       = {AITR-1370},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1370.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1370.pdf},
}

@techreport{AITR-1371,
 author       = {Rappole, Jr. B. Whitney},
 title        = {Minimizing Residual Vibrations in Flexible Systems},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Residual vibrations degrade the performance of many systems. Due to the lightweight and flexible nature of space structures, controlling residual vibrations is especially difficult. Also, systems such as the Space Shuttle remote Manipulator System have frequencies that vary significantly based upon configuration and loading. Recently, a technique of minimizing vibrations in flexible structures by command input shaping was developed. This document presents research completed in developing a simple, closed- form method of calculating input shaping sequences for two-mode systems and a system to adapt the command input shaping technique to known changes in system frequency about the workspace. The new techniques were tested on a three-link, flexible manipulator.},
 month        = {jun~6},
 number       = {AITR-1371},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1371.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1371.pdf},
}

@techreport{AITR-1374,
 author       = {Breuel, Thomas M.},
 title        = {Geometric Aspects of Visual Object Recognition},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents there important results in visual object recognition based on shape. (1) A new algorithm (RAST; Recognition by Adaptive Sudivisions of Tranformation space) is presented that has lower average-case complexity than any known recognition algorithm. (2) It is shown, both theoretically and empirically, that representing 3D objects as collections of 2D views (the ''View-Based Approximation'') is feasible and affects the reliability of 3D recognition systems no more than other commonly made approximations. (3) The problem of recognition in cluttered scenes is considered from a Bayesian perspective; the commonly-used ''bounded- error errorsmeasure'' is demonstrated to correspond to an independence assumption. It is shown that by modeling the statistical properties of real-scenes better, objects can be recognized more reliably.},
 month        = {may~6},
 number       = {AITR-1374},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1374.pdf; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1374.pdf},
}

@techreport{AITR-1377,
 author       = {Surati, Rajeev},
 title        = {A Parallelizing Compiler Based on Partial Evaluation},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We constructed a parallelizing compiler that utilizes partial evaluation to achieve efficient parallel object code from very high-level data independent source programs. On several important scientific applications, the compiler attains parallel performance equivalent to or better than the best observed results from the manual restructuring of code. This is the first attempt to capitalize on partial evaluation's ability to expose low-level parallelism. New static scheduling techniques are used to utilize the fine-grained parallelism of the computations. The compiler maps the computation graph resulting from partial evaluation onto the Supercomputer Toolkit, an eight VLIW processor parallel computer.},
 month        = {jul~6},
 number       = {AITR-1377},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1377.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1377.pdf},
}

@techreport{AITR-1384,
 author       = {Taalebinezhaad, M. Ali},
 title        = {Robot Motion Vision by Fixation},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In many motion-vision scenarios, a camera (mounted on a moving vehicle) takes images of an environment to find the ''motion'' and shape. We introduce a direct-method called fixation for solving this motion-vision problem in its general case. Fixation uses neither feature-correspondence nor optical-flow. Instead, spatio-temporal brightness gradients are used directly. In contrast to previous direct methods, fixation does not restrict the motion or the environment. Moreover, fixation method neither requires tracked images as its input nor uses mechanical tracking for obtaining fixated images. The experimental results on real images are presented and the implementation issues and techniques are discussed.},
 month        = {sep~6},
 number       = {AITR-1384},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1384.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1384.pdf},
}

@techreport{AITR-1385,
 author       = {Zhao, Feng},
 title        = {Automatic Analysis and Synthesis of Controllers for Dynamical Systems Based On P},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {I present a novel design methodology for the synthesis of automatic controllers, together with a computational environment{\textemdash}the Control Engineer's Workbench{\textemdash}integrating a suite of programs that automatically analyze and design controllers for high-performance, global control of nonlinear systems. This work demonstrates that difficult control synthesis tasks can be automated, using programs that actively exploit and efficiently represent knowledge of nonlinear dynamics and phase space and effectively use the representation to guide and perform the control design. The Control Engineer's Workbench combines powerful numerical and symbolic computations with artificial intelligence reasoning techniques. As a demonstration, the Workbench automatically designed a high-quality maglev controller that outperforms a previous linear design by a factor of 20.},
 month        = {sep~6},
 number       = {AITR-1385},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1385.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1385.pdf},
}

@techreport{AITR-1388,
 author       = {Bradley, Elizabeth},
 title        = {Taming Chaotic Circuits},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Control algorithms that exploit chaotic behavior can vastly improve the performance of many practical and useful systems. The program Perfect Moment is built around a collection of such techniques. It autonomously explores a dynamical system's behavior, using rules embodying theorems and definitions from nonlinear dynamics to zero in on interesting and useful parameter ranges and state-space regions. It then constructs a reference trajectory based on that information and causes the system to follow it. This program and its results are illustrated with several examples, among them the phase- locked loop, where sections of chaotic attractors are used to increase the capture range of the circuit.},
 month        = {sep~6},
 number       = {AITR-1388},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1388.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1388.pdf},
}

@techreport{AITR-1396,
 author       = {Jones, Michael J.},
 title        = {Using Recurrent Networks for Dimensionality Reduction},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report explores how recurrent neural networks can be exploited for learning high- dimensional mappings. Since recurrent networks are as powerful as Turing machines, an interesting question is how recurrent networks can be used to simplify the problem of learning from examples. The main problem with learning high-dimensional functions is the curse of dimensionality which roughly states that the number of examples needed to learn a function increases exponentially with input dimension. This thesis proposes a way of avoiding this problem by using a recurrent network to decompose a high-dimensional function into many lower dimensional functions connected in a feedback loop.},
 month        = {sep~6},
 number       = {AITR-1396},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1396.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1396.pdf},
}

@techreport{AITR-1398,
 author       = {III, William M. Wells},
 title        = {Statistical Object Recognition},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Two formulations of model-based object recognition are described. MAP Model Matching evaluates joint hypotheses of match and pose, while Posterior Marginal Pose Estimation evaluates the pose only. Local search in pose space is carried out with the Expectation{\textendash}Maximization (EM) algorithm. Recognition experiments are described where the EM algorithm is used to refine and evaluate pose hypotheses in 2D and 3D. Initial hypotheses for the 2D experiments were generated by a simple indexing method: Angle Pair Indexing. The Linear Combination of Views method of Ullman and Basri is employed as the projection model in the 3D experiments.},
 month        = {jan~6},
 number       = {AITR-1398},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1398.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1398.pdf},
}

@techreport{AITR-1401,
 author       = {Shashua, Amnon},
 title        = {Geometry and Photometry in 3D Visual Recognition},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The report addresses the problem of visual recognition under two sources of variability: geometric and photometric. The geometric deals with the relation between 3D objects and their views under orthographic and perspective projection. The photometric deals with the relation between 3D matte objects and their images under changing illumination conditions. Taken together, an alignment- based method is presented for recognizing objects viewed from arbitrary viewing positions and illuminated by arbitrary settings of light sources.},
 month        = {nov~6},
 number       = {AITR-1401},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1401.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1401.pdf},
}

@techreport{AITR-1408,
 author       = {Greenspun, Philip},
 title        = {Site Controller},
 subtitle     = {A System for Computer-Aided Civil Engineering and Construction},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A revolution in earthmoving, a \$100 billion industry, can be achieved with three components: the GPS location system, sensors and computers in bulldozers, and SITE CONTROLLER, a central computer system that maintains design data and directs operations. The first two components are widely available; I built SITE CONTROLLER to complete the triangle and describe it here. SITE CONTROLLER assists civil engineers in the design, estimation, and construction of earthworks, including hazardous waste site remediation. The core of SITE CONTROLLER is a site modelling system that represents existing and prospective terrain shapes, roads, hydrology, etc. Around this core are analysis, simulation, and vehicle control tools. Integrating these modules into one program enables civil engineers and contractors to use a single interface and database throughout the life of a project.},
 month        = {feb~6},
 number       = {AITR-1408},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1408.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1408.pdf},
}

@techreport{AITR-1410,
 author       = {Alter, Tao Daniel},
 title        = {Robust and Efficient 3D Recognition by Alignment},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Alignment is a prevalent approach for recognizing 3D objects in 2D images. A major problem with current implementations is how to robustly handle errors that propagate from uncertainties in the locations of image features. This thesis gives a technique for bounding these errors. The technique makes use of a new solution to the problem of recovering 3D pose from three matching point pairs under weak-perspective projection. Furthermore, the error bounds are used to demonstrate that using line segments for features instead of points significantly reduces the false positive rate, to the extent that alignment can remain reliable even in cluttered scenes.},
 month        = {sep~6},
 number       = {AITR-1410},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1410.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1410.pdf},
}

@techreport{AITR-1411,
 author       = {Thompson, Clay Matthew},
 title        = {Robust Photo-topography by Fusing Shape-from-Shading and Stereo},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Methods for fusing two computer vision methods are discussed and several example algorithms are presented to illustrate the variational method of fusing algorithms. The example algorithms seek to determine planet topography given two images taken from two different locations with two different lighting conditions. The algorithms each employ assingle cost function that combines the computer vision methods of shape-from- shading and stereo in different ways. The algorithms are closely coupled and take into account all the constraints of the photo- topography problem. The algorithms are run on four synthetic test image sets of varying difficulty.},
 month        = {feb~6},
 number       = {AITR-1411},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1411.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1411.pdf},
}

@techreport{AITR-1412,
 author       = {Amsterdam, Jonathan},
 title        = {Automatic Qualitative Modeling of Dynamic Physical Systems},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report describes MM, a computer program that can model a variety of mechanical and fluid systems. Given a system's structure and qualitative behavior, MM searches for models using an energy- based modeling framework. MM uses general facts about physical systems to relate behavioral and model properties. These facts enable a more focussed search for models than would be obtained by mere comparison of desired and predicted behaviors. When these facts do not apply, MM uses behavior- constrained qualitative simulation to verify candidate models efficiently. MM can also design experiments to distinguish among multiple candidate models.},
 month        = {jan~6},
 number       = {AITR-1412},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1412.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1412.pdf},
}

@techreport{AITR-1416,
 author       = {Jacobs, David W.},
 title        = {Recognizing 3-D Objects Using 2-D Images},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We discuss a strategy for visual recognition by forming groups of salient image features, and then using these groups to index into a data base to find all of the matching groups of model features. We discuss the most space efficient possible method of representing 3-D models for indexing from 2-D data, and show how to account for sensing error when indexing. We also present a convex grouping method that is robust and efficient, both theoretically and in practice. Finally, we combine these modules into a complete recognition system, and test its performance on many real images.},
 month        = {apr~6},
 number       = {AITR-1416},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1416.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1416.pdf},
}

@techreport{AITR-1417,
 author       = {Sobalvarro, Patrick},
 title        = {A Lifetime-based Garbage Collector for LISP Systems on General-Purpose Computers},
 year         = {1988},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Garbage collector performance in LISP systems on custom hardware has been substantially improved by the adoption of lifetime-based garbage collection techniques. To date, however, successful lifetime-based garbage collectors have required special- purpose hardware, or at least privileged access to data structures maintained by the virtual memory system. I present here a lifetime-based garbage collector requiring no special-purpose hardware or virtual memory system support, and discuss its performance.},
 month        = {feb~6},
 number       = {AITR-1417},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1417.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1417.pdf},
}

@techreport{AITR-1420,
 author       = {Mahmood, S. Tanveer F.},
 title        = {Attentional Selection in Object Recognition},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A key problem in object recognition is selection, namely, the problem of identifying regions in an image within which to start the recognition process, ideally by isolating regions that are likely to come from a single object. Such a selection mechanism has been found to be crucial in reducing the combinatorial search involved in the matching stage of object recognition. Even though selection is of help in recognition, it has largely remained unsolved because of the difficulty in isolating regions belonging to objects under complex imaging conditions involving occlusions, changing illumination, and object appearances. This thesis presents a novel approach to the selection problem by proposing a computational model of visual attentional selection as a paradigm for selection in recognition. In particular, it proposes two modes of attentional selection, namely, attracted and pay attention modes as being appropriate for data and model-driven selection in recognition. An implementation of this model has led to new ways of extracting color, texture and line group information in images, and their subsequent use in isolating areas of the scene likely to contain the model object. Among the specific results in this thesis are: a method of specifying color by perceptual color categories for fast color region segmentation and color-based localization of objects, and a result showing that the recognition of texture patterns on model objects is possible under changes in orientation and occlusions without detailed segmentation. The thesis also presents an evaluation of the proposed model by integrating with a 3D from 2D object recognition system and recording the improvement in performance. These results indicate that attentional selection can significantly overcome the computational bottleneck in object recognition, both due to a reduction in the number of features, and due to a reduction in the number of matches during recognition using the information derived during selection. Finally, these studies have revealed a surprising use of selection, namely, in the partial solution of the pose of a 3D object.},
 month        = {mar~6},
 number       = {AITR-1420},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1420.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1420.pdf},
}

@techreport{AITR-1424,
 author       = {Isbell, Charles L.},
 title        = {Explorations of the Practical Issues of Learning Prediction-Control Tasks Using Temporal Difference Learning Methods},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {There has been recent interest in using temporal difference learning methods to attack problems of prediction and control. While these algorithms have been brought to bear on many problems, they remain poorly understood. It is the purpose of this thesis to further explore these algorithms, presenting a framework for viewing them and raising a number of practical issues and exploring those issues in the context of several case studies. This includes applying the TD(lambda) algorithm to: 1) learning to play tic-tac-toe from the outcome of self-play and of play against a perfectly-playing opponent and 2) learning simple one-dimensional segmentation tasks.},
 month        = {dec~6},
 number       = {AITR-1424},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1424.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1424.pdf},
}

@techreport{AITR-1425,
 author       = {Caine, Michael E.},
 title        = {The Design of Shape from Motion Constraints},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report presents a set of representations methodologies and tools for the purpose of visualizing, analyzing and designing functional shapes in terms of constraints on motion. The core of the research is an interactive computational environment that provides an explicit visual representation of motion constraints produced by shape interactions, and a series of tools that allow for the manipulation of motion constraints and their underlying shapes for the purpose of design.},
 month        = {sep~6},
 number       = {AITR-1425},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1425.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1425.pdf},
}

@techreport{AITR-1426,
 author       = {Stein, Gideon P.},
 title        = {Internal Camera Calibration Using Rotation and Geometric Shapes},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This paper describes a simple method for internal camera calibration for computer vision. This method is based on tracking image features through a sequence of images while the camera undergoes pure rotation. The location of the features relative to the camera or to each other need not be known and therefore this method can be used both for laboratory calibration and for self calibration in autonomous robots working in unstructured environments. A second method of calibration is also presented. This method uses simple geometric objects such as spheres and straight lines to The camera parameters. Calibration is performed using both methods and the results compared.},
 month        = {feb~6},
 number       = {AITR-1426},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1426.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1426.pdf},
}

@techreport{AITR-1427,
 author       = {Rozas, Guillermo J.},
 title        = {Translucent Procedures, Abstraction without Opacity},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report introduces TRANSLUCENT PROCEDURES as a new mechanism for implementing behavioral abstractions. Like an ordinary procedure, a translucent procedure can be invoked, and thus provides an obvious way to capture a BEHAVIOR. Translucent procedures, like ordinary procedures, can be manipulated as first-class objects and combined using functional composition. But unlike ordinary procedures, translucent procedures have structure that can be examined in well-specified non- destructive ways, without invoking the procedure.},
 month        = {oct~6},
 number       = {AITR-1427},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1427.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1427.pdf},
}

@techreport{AITR-1429,
 author       = {Tsien, Christine L.},
 title        = {Maygen},
 subtitle     = {A Symbolic Debugger Generation System},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {With the development of high-level languages for new computer architectures comes the need for appropriate debugging tools as well. One method for meeting this need would be to develop, from scratch, a symbolic debugger with the introduction of each new language implementation for any given architecture. This, however, seems to require unnecessary duplication of effort among developers. This paper describes Maygen, a ''debugger generation system,'' designed to efficiently provide the desired language-dependent and architecture-dependent debuggers. A prototype of the Maygen system has been implemented and is able to handle the semantically different languages of C and OPAL.},
 month        = {may~6},
 number       = {AITR-1429},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1429.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1429.pdf},
}

@techreport{AITR-1434,
 author       = {Chaney, Ronald D.},
 title        = {Feature Extraction Without Edge Detection},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Information representation is a critical issue in machine vision. The representation strategy in the primitive stages of a vision system has enormous implications for the performance in subsequent stages. Existing feature extraction paradigms, like edge detection, provide sparse and unreliable representations of the image information. In this thesis, we propose a novel feature extraction paradigm. The features consist of salient, simple parts of regions bounded by zero-crossings. The features are dense, stable, and robust. The primary advantage of the features is that they have abstract geometric attributes pertaining to their size and shape. To demonstrate the utility of the feature extraction paradigm, we apply it to passive navigation. We argue that the paradigm is applicable to other early vision problems.},
 month        = {sep~6},
 number       = {AITR-1434},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1434.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1434.pdf},
}

@techreport{AITR-1442,
 author       = {Subirana-Vilanova, J. Brian},
 title        = {Mid-Level Vision and Recognition of Non-Rigid Objects},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We address mid-level vision for the recognition of non-rigid objects. We align model and image using frame curves - which are object or ''figure/ground'' skeletons. Frame curves are computed, without discontinuities, using Curved Inertia Frames, a provably global scheme implemented on the Connection Machine, based on: non- cartisean networks; a definition of curved axis of inertia; and a ridge detector. I present evidence against frame alignment in human perception. This suggests: frame curves have a role in figure/ground segregation and in fuzzy boundaries; their outside/near/top/ incoming regions are more salient; and that perception begins by setting a reference frame (prior to early vision), and proceeds by processing convex structures.},
 month        = {apr~23},
 number       = {AITR-1442},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1442.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1442.pdf},
}

@techreport{AITR-1443,
 author       = {Ferrell, Cynthia},
 title        = {Robust Agent Control of an Autonomous Robot with Many Sensors and Actuators},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents methods for implementing robust hexpod locomotion on an autonomous robot with many sensors and actuators. The controller is based on the Subsumption Architecture and is fully distributed over approximately 1500 simple, concurrent processes. The robot, Hannibal, weighs approximately 6 pounds and is equipped with over 100 physical sensors, 19 degrees of freedom, and 8 on board computers. We investigate the following topics in depth: distributed control of a complex robot, insect-inspired locomotion control for gait generation and rough terrain mobility, and fault tolerance. The controller was implemented, debugged, and tested on Hannibal. Through a series of experiments, we examined Hannibal's gait generation, rough terrain locomotion, and fault tolerance performance. These results demonstrate that Hannibal exhibits robust, flexible, real-time locomotion over a variety of terrain and tolerates a multitude of hardware failures.},
 month        = {may~23},
 number       = {AITR-1443},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1443.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1443.pdf},
}

@techreport{AITR-1444,
 author       = {de la Maza, Michael},
 title        = {Synthesizing Regularity Exposing Attributes in Large Protein Databases},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis describes a system that synthesizes regularity exposing attributes from large protein databases. After processing primary and secondary structure data, this system discovers an amino acid representation that captures what are thought to be the three most important amino acid characteristics (size, charge, and hydrophobicity) for tertiary structure prediction. A neural network trained using this 16 bit representation achieves a performance accuracy on the secondary structure prediction problem that is comparable to the one achieved by a neural network trained using the standard 24 bit amino acid representation. In addition, the thesis describes bounds on secondary structure prediction accuracy, derived using an optimal learning algorithm and the probably approximately correct (PAC) model.},
 month        = {may~23},
 number       = {AITR-1444},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1444.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1444.pdf},
}

@techreport{AITR-1445,
 author       = {DeHon, Andre},
 title        = {Robust, High-Speed Network Design for Large-Scale Multiprocessing},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {As multiprocessor system size scales upward, two important aspects of multiprocessor systems will generally get worse rather than better: (1) interprocessor communication latency will increase and (2) the probability that some component in the system will fail will increase. These problems can prevent us from realizing the potential benefits of large-scale multiprocessing. In this report we consider the problem of designing networks which simultaneously minimize communication latency while maximizing fault tolerance. Using a synergy of techniques including connection topologies, routing protocols, signalling techniques, and packaging technologies we assemble integrated, system-level solutions to this network design problem.},
 month        = {sep~23},
 number       = {AITR-1445},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1445.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1445.pdf},
}

@techreport{AITR-1450,
 author       = {Albro, Daniel M.},
 title        = {AMAR},
 subtitle     = {A Computational Model of Autosegmental Phonology},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report describes a computational system with which phonologists may describe a natural language in terms of autosegmental phonology, currently the most advanced theory pertaining to the sound systems of human languages. This system allows linguists to easily test autosegmental hypotheses against a large corpus of data. The system was designed primarily with tonal systems in mind, but also provides support for tree or feature matrix representation of phonemes (as in The Sound Pattern of English), as well as syllable structures and other aspects of phonological theory. Underspecification is allowed, and trees may be specified before, during, and after rule application. The association convention is automatically applied, and other principles such as the conjunctivity condition are supported. The method of representation was designed such that rules are designated in as close a fashion as possible to the existing conventions of autosegmental theory while adhering to a textual constraint for maximum portability.},
 month        = {oct~23},
 number       = {AITR-1450},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1450.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1450.pdf},
}

@techreport{AITR-1451,
 author       = {Birkholz, Matthew},
 title        = {Emacs Lisp in Edwin SScheme},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The MIT-Scheme program development environment includes a general-purpose text editor, Edwin, that has an extension language, Edwin Scheme. Edwin is very similar to another general-purpose text editor, GNU Emacs, which also has an extension language, Emacs Lisp. The popularity of GNU Emacs has lead to a large library of tools written in Emacs Lisp. The goal of this thesis is to implement a useful subset of Emacs Lisp in Edwin Scheme. This subset was chosen to be sufficient for simple operation of the GNUS news reading program.},
 month        = {sep~23},
 number       = {AITR-1451},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1451.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1451.pdf},
}

@techreport{AITR-1453,
 author       = {de Marcken, Carl},
 title        = {Methods for Parallelizing Search Paths in Phrasing},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Many search problems are commonly solved with combinatoric algorithms that unnecessarily duplicate and serialize work at considerable computational expense. There are techniques available that can eliminate redundant computations and perform remaining operations concurrently, effectively reducing the branching factors of these algorithms. This thesis applies these techniques to the problem of parsing natural language. The result is an efficient programming language that can reduce some of the expense associated with principle- based parsing and other search problems. The language is used to implement various natural language parsers, and the improvements are compared to those that result from implementing more deterministic theories of language processing.},
 month        = {jan~23},
 number       = {AITR-1453},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1453.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1453.pdf},
}

@techreport{AITR-1455,
 author       = {Hiller, Martha J.},
 title        = {The Role of Chemical Mechanisms in Neural Computation and Learning},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Most computational models of neurons assume that their electrical characteristics are of paramount importance. However, all long- term changes in synaptic efficacy, as well as many short-term effects, are mediated by chemical mechanisms. This technical report explores the interaction between electrical and chemical mechanisms in neural learning and development. Two neural systems that exemplify this interaction are described and modelled. The first is the mechanisms underlying habituation, sensitization, and associative learning in the gill withdrawal reflex circuit in Aplysia, a marine snail. The second is the formation of retinotopic projections in the early visual pathway during embryonic development.},
 month        = {may~23},
 number       = {AITR-1455},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1455.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1455.pdf},
}

@techreport{AITR-1456,
 author       = {Siskind, Jeffrey M.},
 title        = {Naive Physics, Event Perception, Lexical Semantics, and Language Acquisition},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis proposes a computational model of how children may come to learn the meanings of words in their native language. The proposed model is divided into two separate components. One component produces semantic descriptions of visually observed events while the other correlates those descriptions with co-occurring descriptions of those events in natural language. The first part of this thesis describes three implementations of the correlation process whereby representations of the meanings of whole utterances can be decomposed into fragments assigned as representations of the meanings of individual words. The second part of this thesis describes an implemented computer program that recognizes the occurrence of simple spatial motion events in simulated video input.},
 month        = {apr~2},
 number       = {AITR-1456},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1456.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1456.pdf},
}

@techreport{AITR-1457,
 author       = {Hutchinson, James M.},
 title        = {A Radial Basis Function Approach to Financial Time Series Analysis},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Nonlinear multivariate statistical techniques on fast computers offer the potential to capture more of the dynamics of the high dimensional, noisy systems underlying financial markets than traditional models, while making fewer restrictive assumptions. This thesis presents a collection of practical techniques to address important estimation and confidence issues for Radial Basis Function networks arising from such a data driven approach, including efficient methods for parameter estimation and pruning, a pointwise prediction error estimator, and a methodology for controlling the ''data mining'' problem. Novel applications in the finance area are described, including customized, adaptive option pricing and stock price prediction.},
 month        = {dec~2},
 number       = {AITR-1457},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1457.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1457.pdf},
}

@techreport{AITR-1459,
 author       = {Nuth, Peter R.},
 title        = {The Named-State Register File},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis introduces the Named-State Register File, a fine-grain, fully-associative register file. The NSF allows fast context switching between concurrent threads as well as efficient sequential program performance. The NSF holds more live data than conventional register files, and requires less spill and reload traffic to switch between contexts. This thesis demonstrates an implementation of the Named-State Register File and estimates the access time and chip area required for different organizations. Architectural simulations of large sequential and parallel applications show that the NSF can reduce execution time by 9},
 month        = {aug~2},
 number       = {AITR-1459},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1459.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1459.pdf},
}

@techreport{AITR-1464,
 author       = {Pollard, Nancy S.},
 title        = {Parallel Methods for Synthesizing Whole-Hand Grasps from Generalized Prototypes},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report addresses the problem of acquiring objects using articulated robotic hands. Standard grasps are used to make the problem tractable, and a technique is developed for generalizing these standard grasps to increase their flexibility to variations in the problem geometry. A generalized grasp description is applied to a new problem situation using a parallel search through hand configuration space, and the result of this operation is a global overview of the space of good solutions. The techniques presented in this report have been implemented, and the results are verified using the Salisbury three- finger robotic hand.},
 month        = {jan~2},
 number       = {AITR-1464},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1464.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1464.pdf},
}

@techreport{AITR-1465,
 author       = {Parker, Lynne E.},
 title        = {Heterogeneous Multi-Robot Cooperation},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report addresses the problem of achieving cooperation within small- to medium- sized teams of heterogeneous mobile robots. I describe a software architecture I have developed, called ALLIANCE, that facilitates robust, fault tolerant, reliable, and adaptive cooperative control. In addition, an extended version of ALLIANCE, called L-ALLIANCE, is described, which incorporates a dynamic parameter update mechanism that allows teams of mobile robots to improve the efficiency of their mission performance through learning. A number of experimental results of implementing these architectures on both physical and simulated mobile robot teams are described. In addition, this report presents the results of studies of a number of issues in mobile robot cooperation, including fault tolerant cooperative control, adaptive action selection, distributed control, robot awareness of team member actions, improving efficiency through learning, inter- robot communication, action recognition, and local versus global control.},
 month        = {feb~2},
 number       = {AITR-1465},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1465.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1465.pdf},
}

@techreport{AITR-1469,
 author       = {Sarachik, Karen Beth},
 title        = {An Analysis of the Effect of Gaussian Error in Object Recognition},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Object recognition is complicated by clutter, occlusion, and sensor error. Since pose hypotheses are based on image feature locations, these effects can lead to false negatives and positives. In a typical recognition algorithm, pose hypotheses are tested against the image, and a score is assigned to each hypothesis. We use a statistical model to determine the score distribution associated with correct and incorrect pose hypotheses, and use binary hypothesis testing techniques to distinguish between them. Using this approach we can compare algorithms and noise models, and automatically choose values for internal system thresholds to minimize the probability of making a mistake.},
 month        = {feb~2},
 number       = {AITR-1469},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1469.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1469.pdf},
}

@techreport{AITR-1488,
 author       = {Wong, Leon},
 title        = {Automated Reasoning About Classical Mechanics},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In recent years, researchers in artificial intelligence have become interested in replicating human physical reasoning talents in computers. One of the most important skills in this area is predicting how physical systems will behave. This thesis discusses an implemented program that generates algebraic descriptions of how systems of rigid bodies evolve over time. Discussion about the design of this program identifies a physical reasoning paradigm and knowledge representation approach based on mathematical model construction and algebraic reasoning. This paradigm offers several advantages over methods that have become popular in the field, and seems promising for reasoning about a wide variety of classical mechanics problems.},
 month        = {may~2},
 number       = {AITR-1488},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1488.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1488.pdf},
}

@techreport{AITR-1492,
 author       = {Keen, John S.},
 title        = {Logging and Recovery in a Highly Concurrent Database},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report addresses the problem of fault tolerance to system failures for database systems that are to run on highly concurrent computers. It assumes that, in general, an application may have a wide distribution in the lifetimes of its transactions. Logging remains the method of choice for ensuring fault tolerance. Generational garbage collection techniques manage the limited disk space reserved for log information; this technique does not require periodic checkpoints and is well suited for applications with a broad range of transaction lifetimes. An arbitrarily large collection of parallel log streams provide the necessary disk bandwidth.},
 month        = {jun~2},
 number       = {AITR-1492},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1492.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1492.pdf},
}

@techreport{AITR-1493,
 author       = {Coen, Michael H.},
 title        = {SodaBot},
 subtitle     = {A Software Agent Environment and Construction System},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents SodaBot, a general- purpose software agent user-environment and construction system. Its primary component is the basic software agent {\textemdash} a computational framework for building agents which is essentially an agent operating system. We also present a new language for programming the basic software agent whose primitives are designed around human-level descriptions of agent activity. Via this programming language, users can easily implement a wide-range of typical software agent applications, e.g. personal on-line assistants and meeting scheduling agents. The SodaBot system has been implemented and tested, and its description comprises the bulk of this thesis.},
 month        = {nov~2},
 number       = {AITR-1493},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1493.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1493.pdf},
}

@techreport{AITR-1495,
 author       = {Mataric, Maja J.},
 title        = {Interaction and Intelligent Behavior},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We introduce basic behaviors as primitives for control and learning in situated, embodied agents interacting in complex domains. We propose methods for selecting, formally specifying, algorithmically implementing, empirically evaluating, and combining behaviors from a basic set. We also introduce a general methodology for automatically constructing higher{\textendash}level behaviors by learning to select from this set. Based on a formulation of reinforcement learning using conditions, behaviors, and shaped reinforcement, out approach makes behavior selection learnable in noisy, uncertain environments with stochastic dynamics. All described ideas are validated with groups of up to 20 mobile robots performing safe{\textendash} wandering, following, aggregation, dispersion, homing, flocking, foraging, and learning to forage.},
 month        = {aug~28},
 number       = {AITR-1495},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1495.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1495.pdf},
}

@techreport{AITR-1498,
 author       = {Dron, Lisa},
 title        = {Computing 3-D Motion in Custom Analog and Digital VLSI},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis examines a complete design framework for a real-time, autonomous system with specialized VLSI hardware for computing 3-D camera motion. In the proposed architecture, the first step is to determine point correspondences between two images. Two processors, a CCD array edge detector and a mixed analog/digital binary block correlator, are proposed for this task. The report is divided into three parts. Part I covers the algorithmic analysis; part II describes the design and test of a 32$ime $32 CCD edge detector fabricated through MOSIS; and part III compares the design of the mixed analog/digital correlator to a fully digital implementation.},
 month        = {nov~28},
 number       = {AITR-1498},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1498.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1498.pdf},
}

@techreport{AITR-1500,
 author       = {Younis, Saed G.},
 title        = {Asymptotically Zero Energy Computing Using Split-Level Charge Recovery Logic},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The dynamic power requirement of CMOS circuits is rapidly becoming a major concern in the design of personal information systems and large computers. In this work we present a number of new CMOS logic families, Charge Recovery Logic (CRL) as well as the much improved Split-Level Charge Recovery Logic (SCRL), within which the transfer of charge between the nodes occurs quasistatically. Operating quasistatically, these logic families have an energy dissipation that drops linearly with operating frequency, i.e., their power consumption drops quadratically with operating frequency as opposed to the linear drop of conventional CMOS. The circuit techniques in these new families rely on constructing an explicitly reversible pipelined logic gate, where the information necessary to recover the energy used to compute a value is provided by computing its logical inverse. Information necessary to uncompute the inverse is available from the subsequent inverse logic stage. We demonstrate the low energy operation of SCRL by presenting the results from the testing of the first fully quasistatic 8 x 8 multiplier chip (SCRL-1) employing SCRL circuit techniques.},
 month        = {jun~21},
 number       = {AITR-1500},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1500.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1500.pdf},
}

@techreport{AITR-1504,
 author       = {Playter, Robert},
 title        = {Passive Dynamics in the Control of Gymnastic Maneuvers},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The control of aerial gymnastic maneuvers is challenging because these maneuvers frequently involve complex rotational motion and because the performer has limited control of the maneuver during flight. A performer can influence a maneuver using a sequence of limb movements during flight. However, the same sequence may not produce reliable performances in the presence of off-nominal conditions. How do people compensate for variations in performance to reliably produce aerial maneuvers? In this report I explore the role that passive dynamic stability may play in making the performance of aerial maneuvers simple and reliable. I present a control strategy comprised of active and passive components for performing robot front somersaults in the laboratory. I show that passive dynamics can neutrally stabilize the layout somersault which involves an ''inherently unstable'' rotation about the intermediate principal axis. And I show that a strategy that uses open loop joint torques plus passive dynamics leads to more reliable 1 1/2 twisting front somersaults in simulation than a strategy that uses prescribed limb motion. Results are presented from laboratory experiments on gymnastic robots, from dynamic simulation of humans and robots, and from linear stability analyses of these systems.},
 month        = {mar~21},
 number       = {AITR-1504},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1504.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1504.pdf},
}

@techreport{AITR-1511,
 author       = {Horswill, Ian},
 title        = {Specialization of Perceptual Processes},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In this report, I discuss the use of vision to support concrete, everyday activity. I will argue that a variety of interesting tasks can be solved using simple and inexpensive vision systems. I will provide a number of working examples in the form of a state-of-the-art mobile robot, Polly, which uses vision to give primitive tours of the seventh floor of the MIT AI Laboratory. By current standards, the robot has a broad behavioral repertoire and is both simple and inexpensive (the complete robot was built for less than \$20,000 using commercial board-level components). The approach I will use will be to treat the structure of the agent's activity{\textemdash}its task and environment{\textemdash}as positive resources for the vision system designer. By performing a careful analysis of task and environment, the designer can determine a broad space of mechanisms which can perform the desired activity. My principal thesis is that for a broad range of activities, the space of applicable mechanisms will be broad enough to include a number mechanisms which are simple and economical. The simplest mechanisms that solve a given problem will typically be quite specialized to that problem. One thus worries that building simple vision systems will be require a great deal of it ad-hoc engineering that cannot be transferred to other problems. My second thesis is that specialized systems can be analyzed and understood in a principled manner, one that allows general lessons to be extracted from specialized systems. I will present a general approach to analyzing specialization through the use of transformations that provably improve performance. By demonstrating a sequence of transformations that derive a specialized system from a more general one, we can summarize the specialization of the former in a compact form that makes explicit the additional assumptions that it makes about its environment. The summary can be used to predict the performance of the system in novel environments. Individual transformations can be recycled in the design of future systems.},
 month        = {apr~22},
 number       = {AITR-1511},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1511.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1511.pdf},
}

@techreport{AITR-1513,
 author       = {Bergman, Ruth},
 title        = {Learning World Models in Environments with Manifest Causal Structure},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis examines the problem of an autonomous agent learning a causal world model of its environment. Previous approaches to learning causal world models have concentrated on environments that are too ''easy'' (deterministic finite state machines) or too ''hard'' (containing much hidden state). We describe a new domain {\textemdash} environments with manifest causal structure - {\textendash} for learning. In such environments the agent has an abundance of perceptions of its environment. Specifically, it perceives almost all the relevant information it needs to understand the environment. Many environments of interest have manifest causal structure and we show that an agent can learn the manifest aspects of these environments quickly using straightforward learning techniques. We present a new algorithm to learn a rule-based causal world model from observations in the environment. The learning algorithm includes (1) a low level rule-learning algorithm that converges on a good set of specific rules, (2) a concept learning algorithm that learns concepts by finding completely correlated perceptions, and (3) an algorithm that learns general rules. In addition this thesis examines the problem of finding a good expert from a sequence of experts. Each expert has an ''error rate''; we wish to find an expert with a low error rate. However, each expert's error rate and the distribution of error rates are unknown. A new expert-finding algorithm is presented and an upper bound on the expected error rate of the expert is derived.},
 month        = {may~5},
 number       = {AITR-1513},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1513.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1513.pdf},
}

@techreport{AITR-1524,
 author       = {Williamson, Matthew M.},
 title        = {Series Elastic Actuators},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents the design, construction, control and evaluation of a novel force controlled actuator. Traditional force controlled actuators are designed from the premise that ''Stiffer is better''. This approach gives a high bandwidth system, prone to problems of contact instability, noise, and low power density. The actuator presented in this thesis is designed from the premise that ''Stiffness isn't everything''. The actuator, which incorporates a series elastic element, trades off achievable bandwidth for gains in stable, low noise force control, and protection against shock loads. This thesis reviews related work in robot force control, presents theoretical descriptions of the control and expected performance from a series elastic actuator, and describes the design of a test actuator constructed to gather performance data. Finally the performance of the system is evaluated by comparing the performance data to theoretical predictions.},
 month        = {sep~7},
 number       = {AITR-1524},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1524.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1524.pdf},
}

@techreport{AITR-1527,
 author       = {Skordos, Panayotis A.},
 title        = {Modeling Flue Pipes},
 subtitle     = {Subsonic Flow, Lattice Boltzmann, and Parallel Distributed Computers},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The problem of simulating the hydrodynamics and the acoustic waves inside wind musical instruments such as the recorder, the organ, and the flute is considered. The problem is attacked by developing suitable local- interaction algorithms and a parallel simulation system on a cluster of non- dedicated workstations. Physical measurements of the acoustic signal of various flue pipes show good agreement with the simulations. Previous attempts at this problem have been frustrated because the modeling of acoustic waves requires small integration time steps which make the simulation very compute-intensive. In addition, the simulation of subsonic viscous compressible flow at high Reynolds numbers is susceptible to slow-growing numerical instabilities which are triggered by high- frequency acoustic modes. The numerical instabilities are mitigated by employing suitable explicit algorithms: lattice Boltzmann method, compressible finite differences, and fourth-order artificial-viscosity filter. Further, a technique for accurate initial and boundary conditions for the lattice Boltzmann method is developed, and the second-order accuracy of the lattice Boltzmann method is demonstrated. The compute-intensive requirements are handled by developing a parallel simulation system on a cluster of non-dedicated workstations. The system achieves 80 percent parallel efficiency (speedup/processors) using 20 HP-Apollo workstations. The system is built on UNIX and TCP/IP communication routines, and includes automatic process migration from busy hosts to free hosts.},
 month        = {apr~21},
 number       = {AITR-1527},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1527.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1527.pdf},
}

@techreport{AITR-1529,
 author       = {Ratan, Aparna Lakshmi},
 title        = {The Role of Fixation and Visual Attention in Object Recognition},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This research project is a study of the role of fixation and visual attention in object recognition. In this project, we build an active vision system which can recognize a target object in a cluttered scene efficiently and reliably. Our system integrates visual cues like color and stereo to perform figure/ground separation, yielding candidate regions on which to focus attention. Within each image region, we use stereo to extract features that lie within a narrow disparity range about the fixation position. These selected features are then used as input to an alignment-style recognition system. We show that visual attention and fixation significantly reduce the complexity and the false identifications in model-based recognition using Alignment methods. We also demonstrate that stereo can be used effectively as a figure/ground separator without the need for accurate camera calibration.},
 month        = {jul~21},
 number       = {AITR-1529},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1529.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1529.pdf},
}

@techreport{AITR-1541,
 author       = {Hung, Elmer S.},
 title        = {Parameter Estimation in Chaotic Systems},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report examines how to estimate the parameters of a chaotic system given noisy observations of the state behavior of the system. Investigating parameter estimation for chaotic systems is interesting because of possible applications for high-precision measurement and for use in other signal processing, communication, and control applications involving chaotic systems. In this report, we examine theoretical issues regarding parameter estimation in chaotic systems and develop an efficient algorithm to perform parameter estimation. We discover two properties that are helpful for performing parameter estimation on non-structurally stable systems. First, it turns out that most data in a time series of state observations contribute very little information about the underlying parameters of a system, while a few sections of data may be extraordinarily sensitive to parameter changes. Second, for one-parameter families of systems, we demonstrate that there is often a preferred direction in parameter space governing how easily trajectories of one system can ''shadow''' trajectories of nearby systems. This asymmetry of shadowing behavior in parameter space is proved for certain families of maps of the interval. Numerical evidence indicates that similar results may be true for a wide variety of other systems. Using the two properties cited above, we devise an algorithm for performing parameter estimation. Standard parameter estimation techniques such as the extended Kalman filter perform poorly on chaotic systems because of divergence problems. The proposed algorithm achieves accuracies several orders of magnitude better than the Kalman filter and has good convergence properties for large data sets.},
 month        = {apr~18},
 number       = {AITR-1541},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1541.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1541.pdf},
}

@techreport{AITR-1543,
 author       = {Eberman, Brian Scott},
 title        = {Contact Sensing},
 subtitle     = {A Sequential Decision Approach to Sensing Manipulation Contact},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This paper describes a new statistical, model-based approach to building a contact state observer. The observer uses measurements of the contact force and position, and prior information about the task encoded in a graph, to determine the current location of the robot in the task configuration space. Each node represents what the measurements will look like in a small region of configuration space by storing a predictive, statistical, measurement model. This approach assumes that the measurements are statistically block independent conditioned on knowledge of the model, which is a fairly good model of the actual process. Arcs in the graph represent possible transitions between models. Beam Viterbi search is used to match measurement history against possible paths through the model graph in order to estimate the most likely path for the robot. The resulting approach provides a new decision process that can be use as an observer for event driven manipulation programming. The decision procedure is significantly more robust than simple threshold decisions because the measurement history is used to make decisions. The approach can be used to enhance the capabilities of autonomous assembly machines and in quality control applications.},
 month        = {may~18},
 number       = {AITR-1543},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1543.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1543.pdf},
}

@techreport{AITR-1544,
 author       = {Mellor, J. P.},
 title        = {Enhanced Reality Visualization in a Surgical Environment},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Enhanced reality visualization is the process of enhancing an image by adding to it information which is not present in the original image. A wide variety of information can be added to an image ranging from hidden lines or surfaces to textual or iconic data about a particular part of the image. Enhanced reality visualization is particularly well suited to neurosurgery. By rendering brain structures which are not visible, at the correct location in an image of a patient's head, the surgeon is essentially provided with X-ray vision. He can visualize the spatial relationship between brain structures before he performs a craniotomy and during the surgery he can see what's under the next layer before he cuts through. Given a video image of the patient and a three dimensional model of the patient's brain the problem enhanced reality visualization faces is to render the model from the correct viewpoint and overlay it on the original image. The relationship between the coordinate frames of the patient, the patient's internal anatomy scans and the image plane of the camera observing the patient must be established. This problem is closely related to the camera calibration problem. This report presents a new approach to finding this relationship and develops a system for performing enhanced reality visualization in a surgical environment. Immediately prior to surgery a few circular fiducials are placed near the surgical site. An initial registration of video and internal data is performed using a laser scanner. Following this, our method is fully automatic, runs in nearly real-time, is accurate to within a pixel, allows both patient and camera motion, automatically corrects for changes to the internal camera parameters (focal length, focus, aperture, etc.) and requires only a single image.},
 month        = {jan~18},
 number       = {AITR-1544},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1544.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1544.pdf},
}

@techreport{AITR-1545,
 author       = {Fiske, James A. Stuart},
 title        = {Thread Scheduling Mechanisms for Multiple-Context Parallel Processors},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Scheduling tasks to efficiently use the available processor resources is crucial to minimizing the runtime of applications on shared-memory parallel processors. One factor that contributes to poor processor utilization is the idle time caused by long latency operations, such as remote memory references or processor synchronization operations. One way of tolerating this latency is to use a processor with multiple hardware contexts that can rapidly switch to executing another thread of computation whenever a long latency operation occurs, thus increasing processor utilization by overlapping computation with communication. Although multiple contexts are effective for tolerating latency, this effectiveness can be limited by memory and network bandwidth, by cache interference effects among the multiple contexts, and by critical tasks sharing processor resources with less critical tasks. This thesis presents techniques that increase the effectiveness of multiple contexts by intelligently scheduling threads to make more efficient use of processor pipeline, bandwidth, and cache resources. This thesis proposes thread prioritization as a fundamental mechanism for directing the thread schedule on a multiple-context processor. A priority is assigned to each thread either statically or dynamically and is used by the thread scheduler to decide which threads to load in the contexts, and to decide which context to switch to on a context switch. We develop a multiple-context model that integrates both cache and network effects, and shows how thread prioritization can both maintain high processor utilization, and limit increases in critical path runtime caused by multithreading. The model also shows that in order to be effective in bandwidth limited applications, thread prioritization must be extended to prioritize memory requests. We show how simple hardware can prioritize the running of threads in the multiple contexts, and the issuing of requests to both the local memory and the network. Simulation experiments show how thread prioritization is used in a variety of applications. Thread prioritization can improve the performance of synchronization primitives by minimizing the number of processor cycles wasted in spinning and devoting more cycles to critical threads. Thread prioritization can be used in combination with other techniques to improve cache performance and minimize cache interference between different working sets in the cache. For applications that are critical path limited, thread prioritization can improve performance by allowing processor resources to be devoted preferentially to critical threads. These experimental results show that thread prioritization is a mechanism that can be used to implement a wide range of scheduling policies.},
 month        = {jun~18},
 number       = {AITR-1545},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1545.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1545.pdf},
}

@techreport{AITR-1546,
 author       = {Matsuoka, Yoky},
 title        = {Embodiment and Manipulation Learning Process for a Humanoid Hand},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Babies are born with simple manipulation capabilities such as reflexes to perceived stimuli. Initial discoveries by babies are accidental until they become coordinated and curious enough to actively investigate their surroundings. This thesis explores the development of such primitive learning systems using an embodied light-weight hand with three fingers and a thumb. It is self- contained having four motors and 36 exteroceptor and proprioceptor sensors controlled by an on-palm microcontroller. Primitive manipulation is learned from sensory inputs using competitive learning, back-propagation algorithm and reinforcement learning strategies. This hand will be used for a humanoid being developed at the MIT Artificial Intelligence Laboratory.},
 month        = {may~18},
 number       = {AITR-1546},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1546.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1546.pdf},
}

@techreport{AITR-1548,
 author       = {Viola, Paul A.},
 title        = {Alignment by Maximization of Manual Information},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A new information-theoretic approach is presented for finding the pose of an object in an image. The technique does not require information about the surface properties of the object, besides its shape, and is robust with respect to variations of illumination. In our derivation, few assumptions are made about the nature of the imaging process. As a result the algorithms are quite general and can foreseeably be used in a wide variety of imaging situations. Experiments are presented that demonstrate the approach registering magnetic resonance (MR) images with computed tomography (CT) images, aligning a complex 3D object model to real scenes including clutter and occlusion, tracking a human head in a video sequence and aligning a view-based 2D object model to real images. The method is based on a formulation of the mutual information between the model and the image called EMMA. As applied here the technique is intensity-based, rather than feature-based. It works well in domains where edge or gradient-magnitude based methods have difficulty, yet it is more robust than traditional correlation. Additionally, it has an efficient implementation that is based on stochastic approximation. Finally, we will describe a number of additional real- world applications that can be solved efficiently and reliably using EMMA. EMMA can be used in machine learning to find maximally informative projections of high-dimensional data. EMMA can also be used to detect and correct corruption in magnetic resonance images (MRI).},
 month        = {mar~18},
 number       = {AITR-1548},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1548.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1548.pdf},
}

@techreport{AITR-1563,
 author       = {Morrell, John Bryant},
 title        = {Parallel Coupled Micro-Macro Actuators},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents a new actuator system consisting of a micro-actuator and a macro- actuator coupled in parallel via a compliant transmission. The system is called the Parallel Coupled Micro-Macro Actuator, or PaCMMA. In this system, the micro-actuator is capable of high bandwidth force control due to its low mass and direct-drive connection to the output shaft. The compliant transmission of the macro-actuator reduces the impedance (stiffness) at the output shaft and increases the dynamic range of force. Performance improvement over single actuator systems was expected in force control, impedance control, force distortion and reduction of transient impact forces. A set of quantitative measures is proposed and the actuator system is evaluated against them: Force Control Bandwidth, Position Bandwidth, Dynamic Range, Impact Force, Impedance (''Backdriveability'''), Force Distortion and Force Performance Space. Several theoretical performance limits are derived from the saturation limits of the system. A control law is proposed and control system performance is compared to the theoretical limits. A prototype testbed was built using permanenent magnet motors and an experimental comparison was performed between this actuator concept and two single actuator systems. The following performance was observed: Force bandwidth of 56Hz, Torque Dynamic Range of 800:1, Peak Torque of 1040mNm, Minimum Torque of 1.3mNm. Peak Impact Force was reduced by an order of magnitude. Distortion at small amplitudes was reduced substantially. Backdriven impedance was reduced by 2-3 orders of magnitude. This actuator system shows promise for manipulator design as well as psychophysical tests of human performance.},
 month        = {jan~13},
 number       = {AITR-1563},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1563.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1563.pdf},
}

@techreport{AITR-1566,
 author       = {Kapur, Tina},
 title        = {Segmentation of Brain Tissue from Magnetic Resonance Images},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Segmentation of medical imagery is a challenging problem due to the complexity of the images, as well as to the absence of models of the anatomy that fully capture the possible deformations in each structure. Brain tissue is a particularly complex structure, and its segmentation is an important step for studies in temporal change detection of morphology, as well as for 3D visualization in surgical planning. In this paper, we present a method for segmentation of brain tissue from magnetic resonance images that is a combination of three existing techniques from the Computer Vision literature: EM segmentation, binary morphology, and active contour models. Each of these techniques has been customized for the problem of brain tissue segmentation in a way that the resultant method is more robust than its components. Finally, we present the results of a parallel implementation of this method on IBM's supercomputer Power Visualization System for a database of 20 brain scans each with 256x256x124 voxels and validate those against segmentations generated by neuroanatomy experts.},
 month        = {jan~13},
 number       = {AITR-1566},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1566.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1566.pdf},
}

@techreport{AITR-1569,
 author       = {Yuret, Deniz},
 title        = {From Genetic Algorithms to Efficient Organization},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The work described in this thesis began as an inquiry into the nature and use of optimization programs based on ''genetic algorithms.'' That inquiry led, eventually, to three powerful heuristics that are broadly applicable in gradient-ascent programs: First, remember the locations of local maxima and restart the optimization program at a place distant from previously located local maxima. Second, adjust the size of probing steps to suit the local nature of the terrain, shrinking when probes do poorly and growing when probes do well. And third, keep track of the directions of recent successes, so as to probe preferentially in the direction of most rapid ascent. These algorithms lie at the core of a novel optimization program that illustrates the power to be had from deploying them together. The efficacy of this program is demonstrated on several test problems selected from a variety of fields, including De Jong's famous test-problem suite, the traveling salesman problem, the problem of coordinate registration for image guided surgery, the energy minimization problem for determining the shape of organic molecules, and the problem of assessing the structure of sedimentary deposits using seismic data.},
 month        = {may~13},
 number       = {AITR-1569},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1569.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1569.pdf},
}

@techreport{AITR-1572,
 author       = {Sung, Kah-Kay},
 title        = {Learning and Example Selection for Object and Pattern Detection},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents a learning based approach for detecting classes of objects and patterns with variable image appearance but highly predictable image boundaries. It consists of two parts. In part one, we introduce our object and pattern detection approach using a concrete human face detection example. The approach first builds a distribution-based model of the target pattern class in an appropriate feature space to describe the target's variable image appearance. It then learns from examples a similarity measure for matching new patterns against the distribution-based target model. The approach makes few assumptions about the target pattern class and should therefore be fairly general, as long as the target class has predictable image boundaries. Because our object and pattern detection approach is very much learning-based, how well a system eventually performs depends heavily on the quality of training examples it receives. The second part of this thesis looks at how one can select high quality examples for function approximation learning tasks. We propose an em active learning formulation for function approximation, and show for three specific approximation function classes, that the active example selection strategy learns its target with fewer data samples than random sampling. We then simplify the original active learning formulation, and show how it leads to a tractable example selection paradigm, suitable for use in many object and pattern detection problems.},
 month        = {mar~13},
 number       = {AITR-1572},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1572.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1572.pdf},
}

@techreport{AITR-1573,
 author       = {Stahovich, Thomas F.},
 title        = {SketchIT},
 subtitle     = {A Sketch Interpretation Tool for Conceptual Mechanical Design},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We describe a program called SketchIT capable of producing multiple families of designs from a single sketch. The program is given a rough sketch (drawn using line segments for part faces and icons for springs and kinematic joints) and a description of the desired behavior. The sketch is ''rough'' in the sense that taken literally, it may not work. From this single, perhaps flawed sketch and the behavior description, the program produces an entire family of working designs. The program also produces design variants, each of which is itself a family of designs. SketchIT represents each family of designs with a ''behavior ensuring parametric model'' (BEP-Model), a parametric model augmented with a set of constraints that ensure the geometry provides the desired behavior. The construction of the BEP-Model from the sketch and behavior description is the primary task and source of difficulty in this undertaking. SketchIT begins by abstracting the sketch to produce a qualitative configuration space (qc- space) which it then uses as its primary representation of behavior. SketchIT modifies this initial qc-space until qualitative simulation verifies that it produces the desired behavior. SketchIT's task is then to find geometries that implement this qc-space. It does this using a library of qc-space fragments. Each fragment is a piece of parametric geometry with a set of constraints that ensure the geometry implements a specific kind of boundary (qcs- curve) in qc-space. SketchIT assembles the fragments to produce the BEP-Model. SketchIT produces design variants by mapping the qc-space to multiple implementations, and by transforming rotating parts to translating parts and vice versa.},
 month        = {mar~13},
 number       = {AITR-1573},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1573.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1573.pdf},
}

@techreport{AITR-1574,
 author       = {Beymer, David},
 title        = {Pose-Invariant Face Recognition Using Real and Virtual Views},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The problem of automatic face recognition is to visually identify a person in an input image. This task is performed by matching the input face against the faces of known people in a database of faces. Most existing work in face recognition has limited the scope of the problem, however, by dealing primarily with frontal views, neutral expressions, and fixed lighting conditions. To help generalize existing face recognition systems, we look at the problem of recognizing faces under a range of viewpoints. In particular, we consider two cases of this problem: (i) many example views are available of each person, and (ii) only one view is available per person, perhaps a driver's license or passport photograph. Ideally, we would like to address these two cases using a simple view-based approach, where a person is represented in the database by using a number of views on the viewing sphere. While the view-based approach is consistent with case (i), for case (ii) we need to augment the single real view of each person with synthetic views from other viewpoints, views we call 'virtual views'. Virtual views are generated using prior knowledge of face rotation, knowledge that is 'learned' from images of prototype faces. This prior knowledge is used to effectively rotate in depth the single real view available of each person. In this thesis, I present the view- based face recognizer, techniques for synthesizing virtual views, and experimental results using real and virtual views in the recognizer.},
 month        = {mar~28},
 number       = {AITR-1574},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1574.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1574.pdf},
}

@techreport{AITR-1577,
 author       = {McQuirk, Ignacio Sean},
 title        = {An Analog VLSI Chip for Estimating the Focus of Expansion},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {For applications involving the control of moving vehicles, the recovery of relative motion between a camera and its environment is of high utility. This thesis describes the design and testing of a real- time analog VLSI chip which estimates the focus of expansion (FOE) from measured time-varying images. Our approach assumes a camera moving through a fixed world with translational velocity; the FOE is the projection of the translation vector onto the image plane. This location is the point towards which the camera is moving, and other points appear to be expanding outward from. By way of the camera imaging parameters, the location of the FOE gives the direction of 3-D translation. The algorithm we use for estimating the FOE minimizes the sum of squares of the differences at every pixel between the observed time variation of brightness and the predicted variation given the assumed position of the FOE. This minimization is not straightforward, because the relationship between the brightness derivatives depends on the unknown distance to the surface being imaged. However, image points where brightness is instantaneously constant play a critical role. Ideally, the FOE would be at the intersection of the tangents to the iso- brightness contours at these ''stationary'' points. In practice, brightness derivatives are hard to estimate accurately given that the image is quite noisy. Reliable results can nevertheless be obtained if the image contains many stationary points and the point is found that minimizes the sum of squares of the perpendicular distances from the tangents at the stationary points. The FOE chip calculates the gradient of this least-squares minimization sum, and the estimation is performed by closing a feedback loop around it. The chip has been implemented using an embedded CCD imager for image acquisition and a row-parallel processing scheme. A 64 x 64 version was fabricated in a 2um CCD/ BiCMOS process through MOSIS with a design goal of 200 mW of on-chip power, a top frame rate of 1000 frames/second, and a basic accuracy of 5},
 month        = {aug~21},
 number       = {AITR-1577},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1577.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1577.pdf},
}

@techreport{AITR-1579,
 author       = {LaMacchia, Brian A.},
 title        = {Internet Fish},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {I have invented ''Internet Fish,'' a novel class of resource-discovery tools designed to help users extract useful information from the Internet. Internet Fish (IFish) are semi- autonomous, persistent information brokers; users deploy individual IFish to gather and refine information related to a particular topic. An IFish will initiate research, continue to discover new sources of information, and keep tabs on new developments in that topic. As part of the information-gathering process the user interacts with his IFish to find out what it has learned, answer questions it has posed, and make suggestions for guidance. Internet Fish differ from other Internet resource discovery systems in that they are persistent, personal and dynamic. As part of the information-gathering process IFish conduct extended, long-term conversations with users as they explore. They incorporate deep structural knowledge of the organization and services of the net, and are also capable of on-the-fly reconfiguration, modification and expansion. Human users may dynamically change the IFish in response to changes in the environment, or IFish may initiate such changes itself. IFish maintain internal state, including models of its own structure, behavior, information environment and its user; these models permit an IFish to perform meta-level reasoning about its own structure. To facilitate rapid assembly of particular IFish I have created the Internet Fish Construction Kit. This system provides enabling technology for the entire class of Internet Fish tools; it facilitates both creation of new IFish as well as additions of new capabilities to existing ones. The Construction Kit includes a collection of encapsulated heuristic knowledge modules that may be combined in mix-and-match fashion to create a particular IFish; interfaces to new services written with the Construction Kit may be immediately added to ''live'' IFish. Using the Construction Kit I have created a demonstration IFish specialized for finding World-Wide Web documents related to a given group of documents. This ''Finder'' IFish includes heuristics that describe how to interact with the Web in general, explain how to take advantage of various public indexes and classification schemes, and provide a method for discovering similarity relationships among documents.},
 month        = {aug~1},
 number       = {AITR-1579},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1579.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1579.pdf},
}

@techreport{AITR-1581,
 author       = {Pratt, Jerry E.},
 title        = {Virtual Model Control of a Biped Walking Robot},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The transformation from high level task specification to low level motion control is a fundamental issue in sensorimotor control in animals and robots. This thesis develops a control scheme called virtual model control which addresses this issue. Virtual model control is a motion control language which uses simulations of imagined mechanical components to create forces, which are applied through joint torques, thereby creating the illusion that the components are connected to the robot. Due to the intuitive nature of this technique, designing a virtual model controller requires the same skills as designing the mechanism itself. A high level control system can be cascaded with the low level virtual model controller to modulate the parameters of the virtual mechanisms. Discrete commands from the high level controller would then result in fluid motion. An extension of Gardner's Partitioned Actuator Set Control method is developed. This method allows for the specification of constraints on the generalized forces which each serial path of a parallel mechanism can apply. Virtual model control has been applied to a bipedal walking robot. A simple algorithm utilizing a simple set of virtual components has successfully compelled the robot to walk eight consecutive steps.},
 month        = {dec~2},
 number       = {AITR-1581},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1581.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1581.pdf},
}

@techreport{AITR-1582,
 author       = {Torres, Ann L.},
 title        = {Virtual Model Control of a Hexapod Walking Robot},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Since robots are typically designed with an individual actuator at each joint, the control of these systems is often difficult and non- intuitive. This thesis explains a more intuitive control scheme called Virtual Model Control. This thesis also demonstrates the simplicity and ease of this control method by using it to control a simulated walking hexapod. Virtual Model Control uses imagined mechanical components to create virtual forces, which are applied through the joint torques of real actuators. This method produces a straightforward means of controlling joint torques to produce a desired robot behavior. Due to the intuitive nature of this control scheme, the design of a virtual model controller is similar to the design of a controller with basic mechanical components. The ease of this control scheme facilitates the use of a high level control system which can be used above the low level virtual model controllers to modulate the parameters of the imaginary mechanical components. In order to apply Virtual Model Control to parallel mechanisms, a solution to the force distribution problem is required. This thesis uses an extension of Gardner`s Partitioned Force Control method which allows for the specification of constrained degrees of freedom. This virtual model control technique was applied to a simulated hexapod robot. Although the hexapod is a highly non-linear, parallel mechanism, the virtual models allowed text-book control solutions to be used while the robot was walking. Using a simple linear control law, the robot walked while simultaneously balancing a pendulum and tracking an object.},
 month        = {dec~2},
 number       = {AITR-1582},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1582.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1582.pdf},
}

@techreport{AITR-1585,
 author       = {Hall, Miguel},
 title        = {Prototype of a Configurable Web-Based Assessment System},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The MIT Prototype Educational Assessment System provides subjects and courses at MIT with the ability to perform online assessment. The system includes polices to handle harassment and electronic ''flaming'' while protecting privacy. Within these frameworks, individual courses and subjects can make their own policy decisions about such matters as to when assessments can occur, who can submit assessments, and how anonymous assessments are. By allowing assessment to take place continually and allowing both students and staff to participate, the system can provide a forum for the online discussion of subjects. Even in the case of scheduled assessments, the system can provide advantages over end-of-term assessment, since the scheduled assessments can occur several times during the semester, allowing subjects to identify and adjust those areas that could use improvement. Subjects can also develop customized questionnaires, perhaps in response to previous assessments, to suit their needs.},
 month        = {jun~2},
 number       = {AITR-1585},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1585.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1585.pdf},
}

@techreport{AITR-1586,
 author       = {DeHon, Andre},
 title        = {Reconfigurable Architectures for General-Purpose Computing},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {General-purpose computing devices allow us to (1) customize computation after fabrication and (2) conserve area by reusing expensive active circuitry for different functions in time. We define RP-space, a restricted domain of the general-purpose architectural space focussed on reconfigurable computing architectures. Two dominant features differentiate reconfigurable from special- purpose architectures and account for most of the area overhead associated with RP devices: (1) instructions which tell the device how to behave, and (2) flexible interconnect which supports task dependent dataflow between operations. We can characterize RP- space by the allocation and structure of these resources and compare the efficiencies of architectural points across broad application characteristics. Conventional FPGAs fall at one extreme end of this space and their efficiency ranges over two orders of magnitude across the space of application characteristics. Understanding RP-space and its consequences allows us to pick the best architecture for a task and to search for more robust design points in the space. Our DPGA, a fine- grained computing device which adds small, on-chip instruction memories to FPGAs is one such design point. For typical logic applications and finite- state machines, a DPGA can implement tasks in one-third the area of a traditional FPGA. TSFPGA, a variant of the DPGA which focuses on heavily time- switched interconnect, achieves circuit densities close to the DPGA, while reducing typical physical mapping times from hours to seconds. Rigid, fabrication-time organization of instruction resources significantly narrows the range of efficiency for conventional architectures. To avoid this performance brittleness, we developed MATRIX, the first architecture to defer the binding of instruction resources until run-time, allowing the application to organize resources according to its needs. Our focus MATRIX design point is based on an array of 8-bit ALU and register- file building blocks interconnected via a byte- wide network. With today's silicon, a single chip MATRIX array can deliver over 10 Gop/s (8-bit ops). On sample image processing tasks, we show that MATRIX yields 10-20x the computational density of conventional processors. Understanding the cost structure of RP-space helps us identify these intermediate architectural points and may provide useful insight more broadly in guiding our continual search for robust and efficient general-purpose computing structures.},
 month        = {sep~2},
 number       = {AITR-1586},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1586.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1586.pdf},
}

@techreport{AITR-1587,
 author       = {Niyogi, Partha},
 title        = {The Informational Complexity of Learning from Examples},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis attempts to quantify the amount of information needed to learn certain tasks. The tasks chosen vary from learning functions in a Sobolev space using radial basis function networks to learning grammars in the principles and parameters framework of modern linguistic theory. These problems are analyzed from the perspective of computational learning theory and certain unifying perspectives emerge.},
 month        = {sep~2},
 number       = {AITR-1587},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1587.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1587.pdf},
}

@techreport{AITR-1590,
 author       = {Berlin, Andrew A.},
 title        = {Towards Intelligent Structures},
 subtitle     = {Active Control of Buckling},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The buckling of compressively-loaded members is one of the most important factors limiting the overall strength and stability of a structure. I have developed novel techniques for using active control to wiggle a structural element in such a way that buckling is prevented. I present the results of analysis, simulation, and experimentation to show that buckling can be prevented through computer- controlled adjustment of dynamical behavior.sI have constructed a small-scale railroad-style truss bridge that contains compressive members that actively resist buckling through the use of piezo-electric actuators. I have also constructed a prototype actively controlled column in which the control forces are applied by tendons, as well as a composite steel column that incorporates piezo-ceramic actuators that are used to counteract buckling. Active control of buckling allows this composite column to support 5.6 times more load than would otherwise be possible.sThese techniques promise to lead to intelligent physical structures that are both stronger and lighter than would otherwise be possible.},
 month        = {may~2},
 number       = {AITR-1590},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1590.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1590.pdf},
}

@techreport{AITR-1596,
 author       = {Salisbury, J. Kenneth and (editors), Mandayam A. Srinivasan},
 title        = {The Proceedings of the First PHANToM User's Group Workshop},
 year         = {1996},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {These proceedings summarize the results of the First PHANToM User's Group Workshop held September 27-30, 1996 MIT. The goal of the workshop was to bring together a group of active users of the PHANToM Haptic Interface to discuss the scientific and engineering challenges involved in bringing haptics into widespread use, and to explore the future possibilities of this exciting technology. With over 50 attendees and 25 presentations the workshop provided the first large forum for users of a common haptic interface to share results and engage in collaborative discussions. Short papers from the presenters are contained herein and address the following topics: Research Effort Overviews, Displays and Effects, Applications in Teleoperation and Training, Tools for Simulated Worlds and, Data Visualization.},
 month        = {dec~2},
 number       = {AITR-1596},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1596.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1596.pdf},
}

@techreport{AITR-1617,
 author       = {Salisbury, J. Kenneth and Srinivasan, Mandayam A.},
 title        = {Proceedings of the Second PHANToM User's Group Workshop},
 year         = {1997},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {On October 19-22, 1997 the Second PHANToM Users Group Workshop was held at the MIT Endicott House in Dedham, Massachusetts. Designed as a forum for sharing results and insights, the workshop was attended by more than 60 participants from 7 countries. These proceedings report on workshop presentations in diverse areas including rigid and compliant rendering, tool kits, development environments, techniques for scientific data visualization, multi-modal issues and a programming tutorial.},
 month        = {dec~27},
 number       = {AITR-1617},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1617.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1617.pdf},
}

@techreport{AITR-1627,
 author       = {Bawden, Alan},
 title        = {Implementing Distributed Systems Using Linear Naming},
 year         = {1993},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Linear graph reduction is a simple computational model in which the cost of naming things is explicitly represented. The key idea is the notion of ''linearity''. A name is linear if it is only used once, so with linear naming you cannot create more than one outstanding reference to an entity. As a result, linear naming is cheap to support and easy to reason about. Programs can be translated into the linear graph reduction model such that linear names in the program are implemented directly as linear names in the model. Nonlinear names are supported by constructing them out of linear names. The translation thus exposes those places where the program uses names in expensive, nonlinear ways. Two applications demonstrate the utility of using linear graph reduction: First, in the area of distributed computing, linear naming makes it easy to support cheap cross-network references and highly portable data structures, Linear naming also facilitates demand driven migration of tasks and data around the network without requiring explicit guidance from the programmer. Second, linear graph reduction reveals a new characterization of the phenomenon of state. Systems in which state appears are those which depend on certain - global- system properties. State is not a localizable phenomenon, which suggests that our usual object oriented metaphor for state is flawed.},
 month        = {mar~27},
 number       = {AITR-1627},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1627.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1627.pdf},
}

@techreport{AITR-1634,
 author       = {Flynn, Anita M.},
 title        = {Piezoelectric Ultrasonic Micromotors},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report describes development of micro- fabricated piezoelectric ultrasonic motors and bulk-ceramic piezoelectric ultrasonic motors. Ultrasonic motors offer the advantage of low speed, high torque operation without the need for gears. They can be made compact and lightweight and provide a holding torque in the absence of applied power, due to the traveling wave frictional coupling mechanism between the rotor and the stator. This report covers modeling, simulation, fabrication and testing of ultrasonic motors. Design of experiments methods were also utilized to find optimal motor parameters. A suite of 8 mm diameter x 3 mm tall motors were machined for these studies and maximum stall torques as large as 10{\textasciicircum}(- 3) Nm, maximum no-load speeds of 1710 rpm and peak power outputs of 27 mW were realized. Aditionally, this report describes the implementation of a microfabricated ultrasonic motor using thin- film lead zirconate titanate. In a joint project with the Pennsylvania State University Materials Research Laboratory and MIT Lincoln Laboratory, 2 mm and 5 mm diameter stator structures were fabricated on 1 micron thick silicon nitride membranes. Small glass lenses placed down on top spun at 100-300 rpm with 4 V excitation at 90 kHz. The large power densities and stall torques of these piezoelectric ultrasonic motors offer tremendous promis for integrated machines: complete intelligent, electro-mechanical autonomous systems mass-produced in a single fabrication process.},
 month        = {jun~27},
 number       = {AITR-1634},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1634.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1634.pdf},
}

@techreport{AITR-1639,
 author       = {Maron, Oded},
 title        = {Learning from Ambiguity},
 year         = {1998},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {There are many learning problems for which the examples given by the teacher are ambiguously labeled. In this thesis, we will examine one framework of learning from ambiguous examples known as Multiple- Instance learning. Each example is a bag, consisting of any number of instances. A bag is labeled negative if all instances in it are negative. A bag is labeled positive if at least one instance in it is positive. Because the instances themselves are not labeled, each positive bag is an ambiguous example. We would like to learn a concept which will correctly classify unseen bags. We have developed a measure called Diverse Density and algorithms for learning from multiple- instance examples. We have applied these techniques to problems in drug design, stock prediction, and image database retrieval. These serve as examples of how to translate the ambiguity in the application domain into bags, as well as successful examples of applying Diverse Density techniques.},
 month        = {dec~30},
 number       = {AITR-1639},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1639.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1639.pdf},
}

@techreport{AITR-1645,
 author       = {Wallach, Deborah A.},
 title        = {A Hierarchical Cache Coherent Protocol},
 year         = {1992},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {As the number of processors in distributed- memory multiprocessors grows, efficiently supporting a shared-memory programming model becomes difficult. We have designed the Protocol for Hierarchical Directories (PHD) to allow shared-memory support for systems containing massive numbers of processors. PHD eliminates bandwidth problems by using a scalable network, decreases hot-spots by not relying on a single point to distribute blocks, and uses a scalable amount of space for its directories. PHD provides a shared- memory model by synthesizing a global shared memory from the local memories of processors. PHD supports sequentially consistent read, write, and test- and-set operations. This thesis also introduces a method of describing locality for hierarchical protocols and employs this method in the derivation of an abstract model of the protocol behavior. An embedded model, based on the work of Johnson[ISCA19], describes the protocol behavior when mapped to a k-ary n- cube. The thesis uses these two models to study the average height in the hierarchy that operations reach, the longest path messages travel, the number of messages that operations generate, the inter-transaction issue time, and the protocol overhead for different locality parameters, degrees of multithreading, and machine sizes. We determine that multithreading is only useful for approximately two to four threads; any additional interleaving does not decrease the overall latency. For small machines and high locality applications, this limitation is due mainly to the length of the running threads. For large machines with medium to low locality, this limitation is due mainly to the protocol overhead being too large. Our study using the embedded model shows that in situations where the run length between references to shared memory is at least an order of magnitude longer than the time to process a single state transition in the protocol, applications exhibit good performance. If separate controllers for processing protocol requests are included, the protocol scales to 32k processor machines as long as the application exhibits hierarchical locality: at least 22},
 month        = {sep~11},
 number       = {AITR-1645},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1645.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1645.pdf},
}

@techreport{AITR-1668,
 author       = {Jaakkola, Marina Meila Tommi and Jebara, Tony},
 title        = {Maximum Entropy Discrimination},
 year         = {1999},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We present a general framework for discriminative estimation based on the maximum entropy principle and its extensions. All calculations involve distributions over structures and/or parameters rather than specific settings and reduce to relative entropy projections. This holds even when the data is not separable within the chosen parametric class, in the context of anomaly detection rather than classification, or when the labels in the training set are uncertain or incomplete. Support vector machines are naturally subsumed under this class and we provide several extensions. We are also able to estimate exactly and efficiently discriminative distributions over tree structures of class- conditional models within this framework. Preliminary experimental results are indicative of the potential in these techniques.},
 month        = {dec~10},
 number       = {AITR-1668},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1668.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1668.pdf},
}

@techreport{AITR-1674,
 author       = {Mellor, J. P.},
 title        = {Automatically Recovering Geometry and Texture from Large Sets of Calibrated Images},
 year         = {1999},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Three-dimensional models which contain both geometry and texture have numerous applications such as urban planning, physical simulation, and virtual environments. A major focus of computer vision (and recently graphics) research is the automatic recovery of three-dimensional models from two- dimensional images. After many years of research this goal is yet to be achieved. Most practical modeling systems require substantial human input and unlike automatic systems are not scalable. This thesis presents a novel method for automatically recovering dense surface patches using large sets (1000's) of calibrated images taken from arbitrary positions within the scene. Physical instruments, such as Global Positioning System (GPS), inertial sensors, and inclinometers, are used to estimate the position and orientation of each image. Essentially, the problem is to find corresponding points in each of the images. Once a correspondence has been established, calculating its three-dimensional position is simply a matter of geometry. Long baseline images improve the accuracy. Short baseline images and the large number of images greatly simplifies the correspondence problem. The initial stage of the algorithm is completely local and scales linearly with the number of images. Subsequent stages are global in nature, exploit geometric constraints, and scale quadratically with the complexity of the underlying scene. We describe techniques for: 1) detecting and localizing surface patches; 2) refining camera calibration estimates and rejecting false positive surfels; and 3) grouping surface patches into surfaces and growing the surface along a two-dimensional manifold. We also discuss a method for producing high quality, textured three-dimensional models from these surfaces. Some of the most important characteristics of this approach are that it: 1) uses and refines noisy calibration estimates; 2) compensates for large variations in illumination; 3) tolerates significant soft occlusion (e.g. tree branches); and 4) associates, at a fundamental level, an estimated normal (i.e. no frontal-planar assumption) and texture with each surface patch.},
 month        = {oct~22},
 number       = {AITR-1674},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1674.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1674.pdf},
}

@techreport{AITR-1675,
 author       = {Salisbury, Jr. J. Kenneth and (editors), Mandayam A. Srinivasan},
 title        = {Proceedings of the Fourth PHANTOM Users Group Workshop},
 year         = {1999},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This Report contains the proceedings of the Fourth Phantom Users Group Workshop contains 17 papers presented October 9-12, 1999 at MIT Endicott House in Dedham Massachusetts. The workshop included sessions on, Tools for Programmers, Dynamic Environments, Perception and Cognition, Haptic Connections, Collision Detection / Collision Response, Medical and Seismic Applications, and Haptics Going Mainstream. The proceedings include papers that cover a variety of subjects in computer haptics including rendering, contact determination, development libraries, and applications in medicine, path planning, data interaction and training.},
 month        = {nov~4},
 number       = {AITR-1675},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AITR-1675.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-1675.pdf},
}

@techreport{AITR-2001-001,
 author       = {Koile, Kimberle},
 title        = {The Architect's Collaborator},
 subtitle     = {Toward Intelligent Tools for Conceptual Design},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In early stages of architectural design, as in other design domains, the language used is often very abstract. In architectural design, for example, architects and their clients use experiential terms such as ''private'' or ''open'' to describe spaces. If we are to build programs that can help designers during this early-stage design, we must give those programs the capability to deal with concepts on the level of such abstractions. The work reported in this thesis sought to do that, focusing on two key questions: How are abstract terms such as ''private'' and ''open'' translated into physical form? How might one build a tool to assist designers with this process? The Architect's Collaborator (TAC) was built to explore these issues. It is a design assistant that supports iterative design refinement, and that represents and reasons about how experiential qualities are manifested in physical form. Given a starting design and a set of design goals, TAC explores the space of possible designs in search of solutions that satisfy the goals. It employs a strategy we've called dependency-directed redesign: it evaluates a design with respect to a set of goals, then uses an explanation of the evaluation to guide proposal and refinement of repair suggestions; it then carries out the repair suggestions to create new designs. A series of experiments was run to study TAC's behavior. Issues of control structure, goal set size, goal order, and modification operator capabilities were explored. In addition, TAC's use as a design assistant was studied in an experiment using a house in the process of being redesigned. TAC's use as an analysis tool was studied in an experiment using Frank Lloyd Wright's Prairie houses.},
 month        = {jan~20},
 number       = {AITR-2001-001},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-001.ps; ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-001.pdf},
}

@techreport{AITR-2001-002,
 author       = {Felzenszwalb, Pedro F.},
 title        = {Object Recognition with Pictorial Structures},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents a statistical framework for object recognition. The framework is motivated by the pictorial structure models introduced by Fischler and Elschlager nearly 30 years ago. The basic idea is to model an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. The problem of detecting an object in an image and the problem of learning an object model using training examples are naturally formulated under a statistical approach. We present efficient algorithms to solve these problems in our framework. We demonstrate our techniques by training models to represent faces and human bodies. The models are then used to locate the corresponding objects in novel images.},
 month        = {may~3},
 number       = {AITR-2001-002},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-002.ps; ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-002.pdf},
}

@techreport{AITR-2001-004,
 author       = {Rennie, Jason D. M.},
 title        = {Improving Multi-class Text Classification with Naive Bayes},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.},
 month        = {sep~17},
 number       = {AITR-2001-004},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-004.ps; ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-004.pdf},
}

@techreport{AITR-2001-005,
 author       = {Banks, Jessica},
 title        = {Design and Control of an Anthropomorphic Robotic Finger with Multi-point Tactile Sensation},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The goal of this research is to develop the prototype of a tactile sensing platform for anthropomorphic manipulation research. We investigate this problem through the fabrication and simple control of a planar 2-DOF robotic finger inspired by anatomic consistency, self-containment, and adaptability. The robot is equipped with a tactile sensor array based on optical transducer technology whereby localized changes in light intensity within an illuminated foam substrate correspond to the distribution and magnitude of forces applied to the sensor surface plane. The integration of tactile perception is a key component in realizing robotic systems which organically interact with the world. Such natural behavior is characterized by compliant performance that can initiate internal, and respond to external, force application in a dynamic environment. However, most of the current manipulators that support some form of haptic feedback either solely derive proprioceptive sensation or only limit tactile sensors to the mechanical fingertips. These constraints are due to the technological challenges involved in high resolution, multi-point tactile perception. In this work, however, we take the opposite approach, emphasizing the role of full-finger tactile feedback in the refinement of manual capabilities. To this end, we propose and implement a control framework for sensorimotor coordination analogous to infant-level grasping and fixturing reflexes. This thesis details the mechanisms used to achieve these sensory, actuation, and control objectives, along with the design philosophies and biological influences behind them. The results of behavioral experiments with a simple tactilely-modulated control scheme are also described. The hope is to integrate the modular finger into an },
 month        = {may~30},
 number       = {AITR-2001-005},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-005.ps; ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-005.pdf},
}

@techreport{AITR-2001-006,
 author       = {Ucko, Aaron Mark},
 title        = {Predicate Dispatching in the Common Lisp Object System},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.},
 month        = {may~30},
 number       = {AITR-2001-006},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-006.ps; ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-006.pdf},
}

@techreport{AITR-2001-007,
 author       = {Hong, Won},
 title        = {Modeling, Estimation, and Control of Robot-Soil Interactions},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents the development of hardware, theory, and experimental methods to enable a robotic manipulator arm to interact with soils and estimate soil properties from interaction forces. Unlike the majority of robotic systems interacting with soil, our objective is parameter estimation, not excavation. To this end, we design our manipulator with a flat plate for easy modeling of interactions. By using a flat plate, we take advantage of the wealth of research on the similar problem of earth pressure on retaining walls. There are a number of existing earth pressure models. These models typically provide estimates of force which are in uncertain relation to the true force. A recent technique, known as numerical limit analysis, provides upper and lower bounds on the true force. Predictions from the numerical limit analysis technique are shown to be in good agreement with other accepted models. Experimental methods for plate insertion, soil-tool interface friction estimation, and control of applied forces on the soil are presented. In addition, a novel graphical technique for inverting the soil models is developed, which is an improvement over standard nonlinear optimization. This graphical technique utilizes the uncertainties associated with each set of force measurements to obtain all possible parameters which could have produced the measured forces. The system is tested on three cohesionless soils, two in a loose state and one in a loose and dense state. The results are compared with friction angles obtained from direct shear tests. The results highlight a number of key points. Common assumptions are made in soil modeling. Most notably, the Mohr-Coulomb failure law and perfectly plastic behavior. In the direct shear tests, a marked dependence of friction angle on the normal stress at low stresses is found. This has ramifications for any study of friction done at low stresses. In addition, gradual failures are often observed for vertical tools and tools inclined away from the direction of motion. After accounting for the change in friction angle at low stresses, the results show good agreement with the direct shear values.},
 month        = {sep~7},
 number       = {AITR-2001-007},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-007.ps; ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-007.pdf},
}

@techreport{AITR-2001-008,
 author       = {Nagpal, Radhika},
 title        = {Programmable Self-Assembly},
 subtitle     = {Constructing Global Shape using Biologically-inspire},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In this thesis I present a language for instructing a sheet of identically-programmed, flexible, autonomous agents ({\textquotedblleft}cells'') to assemble themselves into a predetermined global shape, using local interactions. The global shape is described as a folding construction on a continuous sheet, using a set of axioms from paper-folding (origami). I provide a means of automatically deriving the cell program, executed by all cells, from the global shape description. With this language, a wide variety of global shapes and patterns can be synthesized, using only local interactions between identically-programmed cells. Examples include flat layered shapes, all plane Euclidean constructions, and a variety of tessellation patterns. In contrast to approaches based on cellular automata or evolution, the cell program is directly derived from the global shape description and is composed from a small number of biologically-inspired primitives: gradients, neighborhood query, polarity inversion, cell-to-cell contact and flexible folding. The cell programs are robust, without relying on regular cell placement, global coordinates, or synchronous operation and can tolerate a small amount of random cell death. I show that an average cell neighborhood of 15 is sufficient to reliably self-assemble complex shapes and geometric patterns on randomly distributed cells. The language provides many insights into the relationship between local and global descriptions of behavior, such as the advantage of constructive languages, mechanisms for achieving global robustness, and mechanisms for achieving scale- independent shapes from a single cell program. The language suggests a mechanism by which many related shapes can be created by the same cell program, in the manner of D'Arcy Thompson's famous coordinate transformations. The thesis illuminates how complex morphology and pattern can emerge from local interactions, and how one can engineer robust self-assembly.},
 month        = {jun~7},
 number       = {AITR-2001-008},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-008.ps; ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-008.pdf},
}

@techreport{AITR-2001-009,
 author       = {Sezgin, Tevfik Metin},
 title        = {Feature Point Detection and Curve Approximation for Early Processing of Freehand Sketches},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Freehand sketching is both a natural and crucial part of design, yet is unsupported by current design automation software. We are working to combine the flexibility and ease of use of paper and pencil with the processing power of a computer to produce a design environment that feels as natural as paper, yet is considerably smarter. One of the most basic steps in accomplishing this is converting the original digitized pen strokes in the sketch into the intended geometric objects using feature point detection and approximation. We demonstrate how multiple sources of information can be combined for feature detection in strokes and apply this technique using two approaches to signal processing, one using simple average based thresholding and a second using scale space.},
 month        = {may~7},
 number       = {AITR-2001-009},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-009.ps; ftp://publications.ai.mit.edu/ai-publications/2001/AITR-2001-009.pdf},
}

@techreport{AITR-2002-001,
 author       = {Zollei, Lilla},
 title        = {2D-3D Rigid-Body Registration of X-Ray Fluoroscopy and CT Images},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The registration of pre-operative volumetric datasets to intra- operative two-dimensional images provides an improved way of verifying patient position and medical instrument loca- tion. In applications from orthopedics to neurosurgery, it has a great value in maintaining up-to-date information about changes due to intervention. We propose a mutual information- based registration algorithm to establish the proper align- ment. For optimization purposes, we compare the perfor- mance of the non-gradient Powell method and two slightly di erent versions of a stochastic gradient ascent strategy: one using a sparsely sampled histogramming approach and the other Parzen windowing to carry out probability density approximation. Our main contribution lies in adopting the stochastic ap- proximation scheme successfully applied in 3D-3D registra- tion problems to the 2D-3D scenario, which obviates the need for the generation of full DRRs at each iteration of pose op- timization. This facilitates a considerable savings in compu- tation expense. We also introduce a new probability density estimator for image intensities via sparse histogramming, de- rive gradient estimates for the density measures required by the maximization procedure and introduce the framework for a multiresolution strategy to the problem. Registration results are presented on uoroscopy and CT datasets of a plastic pelvis and a real skull, and on a high-resolution CT- derived simulated dataset of a real skull, a plastic skull, a plastic pelvis and a plastic lumbar spine segment.},
 month        = {aug~10},
 number       = {AITR-2002-001},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-001.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-001.pdf},
}

@techreport{AITR-2002-002,
 author       = {Beal, Jacob},
 title        = {Generating Communications Systems Through Shared Context},
 year         = {2002},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In a distributed model of intelligence, peer components need to communicate with one another. I present a system which enables two agents connected by a thick twisted bundle of wires to bootstrap a simple communication system from observations of a shared environment. The agents learn a large vocabulary of symbols, as well as inflections on those symbols which allow thematic role-frames to be transmitted. Language acquisition time is rapid and linear in the number of symbols and inflections. The final communication system is robust and performance degrades gradually in the face of problems.},
 month        = {jan~15},
 number       = {AITR-2002-002},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-002.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-002.pdf},
}

@techreport{AITR-2002-003,
 author       = {Bryson, Joanna J.},
 title        = {Intelligence by Design},
 subtitle     = {Principles of Modularity and Coordination for Engineerin},
 year         = {2001},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {All intelligence relies on search {\textemdash} for example, the search for an intelligent agent's next action. Search is only likely to succeed in resource-bounded agents if they have already been biased towards finding the right answer. In artificial agents, the primary source of bias is engineering. This dissertation describes an approach, Behavior-Oriented Design (BOD) for engineering complex agents. A complex agent is one that must arbitrate between potentially conflicting goals or behaviors. Behavior-oriented design builds on work in behavior-based and hybrid architectures for agents, and the object oriented approach to software engineering. The primary contributions of this dissertation are: 1.The BOD architecture: a modular architecture with each module providing specialized representations to facilitate learning. This includes one pre-specified module and representation for action selection or behavior arbitration. The specialized representation underlying BOD action selection is Parallel-rooted, Ordered, Slip-stack Hierarchical (POSH) reactive plans. 2.The BOD development process: an iterative process that alternately scales the agent's capabilities then optimizes the agent for simplicity, exploiting tradeoffs between the component representations. This ongoing process for controlling complexity not only provides bias for the behaving agent, but also facilitates its maintenance and extendibility. The secondary contributions of this dissertation include two implementations of POSH action selection, a procedure for identifying useful idioms in agent architectures and using them to distribute knowledge across agent paradigms, several examples of applying BOD idioms to established architectures, an analysis and comparison of the attributes and design trends of a large number of agent architectures, a comparison of biological (particularly mammalian) intelligence to artificial agent architectures, a novel model of primate transitive inference, and many other examples of BOD agents and BOD development.},
 month        = {sep~15},
 number       = {AITR-2002-003},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-003.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-003.pdf},
}

@techreport{AITR-2002-004,
 author       = {III, Teodoro Arvizo},
 title        = {A Virtual Machine for a Type-omega Denotational Proof Language},
 year         = {2002},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In this thesis, I designed and implemented a virtual machine (VM) for a monomorphic variant of Athena, a type-omega denotational proof language (DPL). This machine attempts to maintain the minimum state required to evaluate Athena phrases. This thesis also includes the design and implementation of a compiler for monomorphic Athena that compiles to the VM. Finally, it includes details on my implementation of a read-eval-print loop that glues together the VM core and the compiler to provide a full, user-accessible interface to monomorphic Athena. The Athena VM provides the same basis for DPLs that the SECD machine does for pure, functional programming and the Warren Abstract Machine does for Prolog.},
 month        = {jun~15},
 number       = {AITR-2002-004},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-004.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-004.pdf},
}

@techreport{AITR-2002-005,
 author       = {Brown, Jeremy Hanford},
 title        = {Sparsely Faceted Arrays},
 subtitle     = {A Mechanism Supporting Parallel Allocation, Communication, and Garbage Collection},
 year         = {2002},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Conventional parallel computer architectures do not provide support for non-uniformly distributed objects. In this thesis, I introduce sparsely faceted arrays (SFAs), a new low- level mechanism for naming regions of memory, or facets, on different processors in a distributed, shared memory parallel processing system. Sparsely faceted arrays address the disconnect between the global distributed arrays provided by conventional architectures (e.g. the Cray T3 series), and the requirements of high-level parallel programming methods that wish to use objects that are distributed over only a subset of processing elements. A sparsely faceted array names a virtual globally-distributed array, but actual facets are lazily allocated. By providing simple semantics and making efficient use of memory, SFAs enable efficient implementation of a variety of non-uniformly distributed data structures and related algorithms. I present example applications which use SFAs, and describe and evaluate simple hardware mechanisms for implementing SFAs. Keeping track of which nodes have allocated facets for a particular SFA is an important task that suggests the need for automatic memory management, including garbage collection. To address this need, I first argue that conventional tracing techniques such as mark/sweep and copying GC are inherently unscalable in parallel systems. I then present a parallel memory-management strategy, based on reference-counting, that is capable of garbage collecting sparsely faceted arrays. I also discuss opportunities for hardware support of this garbage collection strategy. I have implemented a high-level hardware/OS simulator featuring hardware support for sparsely faceted arrays and automatic garbage collection. I describe the simulator and outline a few of the numerous details associated with a ''real'' implementation of SFAs and SFA-aware garbage collection. Simulation results are used throughout this thesis in the evaluation of hardware support mechanisms.},
 month        = {jun~22},
 number       = {AITR-2002-005},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-005.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-005.pdf},
}

@techreport{AITR-2002-006,
 author       = {''bunnie'' Huang, Andrew},
 title        = {ADAM},
 subtitle     = {A Decentralized Parallel Computer Architecture Featuring Fast Thread and Data Migration and a Uniform Hardware Abstraction},
 year         = {2002},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The furious pace of Moore's Law is driving computer architecture into a realm where the the speed of light is the dominant factor in system latencies. The number of clock cycles to span a chip are increasing, while the number of bits that can be accessed within a clock cycle is decreasing. Hence, it is becoming more difficult to hide latency. One alternative solution is to reduce latency by migrating threads and data, but the overhead of existing implementations has previously made migration an unserviceable solution so far. I present an architecture, implementation, and mechanisms that reduces the overhead of migration to the point where migration is a viable supplement to other latency hiding mechanisms, such as multithreading. The architecture is abstract, and presents programmers with a simple, uniform fine-grained multithreaded parallel programming model with implicit memory management. In other words, the spatial nature and implementation details (such as the number of processors) of a parallel machine are entirely hidden from the programmer. Compiler writers are encouraged to devise programming languages for the machine that guide a programmer to express their ideas in terms of objects, since objects exhibit an inherent physical locality of data and code. The machine implementation can then leverage this locality to automatically distribute data and threads across the physical machine by using a set of high performance migration mechanisms. An implementation of this architecture could migrate a null thread in 66 cycles {\textendash} over a factor of 1000 improvement over previous work. Performance also scales well; the time required to move a typical thread is only 4 to 5 times that of a null thread. Data migration performance is similar, and scales linearly with data block size. Since the performance of the migration mechanism is on par with that of an L2 cache, the implementation simulated in my work has no data caches and relies instead on multithreading and the migration mechanism to hide and reduce access latencies.},
 month        = {jun~18},
 number       = {AITR-2002-006},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-006.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-006.pdf},
}

@techreport{AITR-2002-007,
 author       = {Steinbach, Carl},
 title        = {A Reinforcement-Learning Approach to Power Management},
 year         = {2002},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We describe an adaptive, mid-level approach to the wireless device power management problem. Our approach is based on reinforcement learning, a machine learning framework for autonomous agents. We describe how our framework can be applied to the power management problem in both infrastructure and ad~hoc wireless networks. From this thesis we conclude that mid-level power management policies can outperform low-level policies and are more convenient to implement than high-level policies. We also conclude that power management policies need to adapt to the user and network, and that a mid-level power management framework based on reinforcement learning fulfills these requirements.},
 month        = {may~26},
 number       = {AITR-2002-007},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-007.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-007.pdf},
}

@techreport{AITR-2002-009,
 author       = {Dror, Ron O.},
 title        = {Surface Reflectance Recognition and Real-World Illumination Statistics},
 year         = {2002},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Humans distinguish materials such as metal, plastic, and paper effortlessly at a glance. Traditional computer vision systems cannot solve this problem at all. Recognizing surface reflectance properties from a single photograph is difficult because the observed image depends heavily on the amount of light incident from every direction. A mirrored sphere, for example, produces a different image in every environment. To make matters worse, two surfaces with different reflectance properties could produce identical images. The mirrored sphere simply reflects its surroundings, so in the right artificial setting, it could mimic the appearance of a matte ping-pong ball. Yet, humans possess an intuitive sense of what materials typically ''look like'' in the real world. This thesis develops computational algorithms with a similar ability to recognize reflectance properties from photographs under unknown, real-world illumination conditions. Real-world illumination is complex, with light typically incident on a surface from every direction. We find, however, that real-world illumination patterns are not arbitrary. They exhibit highly predictable spatial structure, which we describe largely in the wavelet domain. Although they differ in several respects from the typical photographs, illumination patterns share much of the regularity described in the natural image statistics literature. These properties of real-world illumination lead to predictable image statistics for a surface with given reflectance properties. We construct a system that classifies a surface according to its reflectance from a single photograph under unknown illuminination. Our algorithm learns relationships between surface reflectance and certain statistics computed from the observed image. Like the human visual system, we solve the otherwise underconstrained inverse problem of reflectance estimation by taking advantage of the statistical regularity of illumination. For surfaces with homogeneous reflectance properties and known geometry, our system rivals human performance.},
 month        = {oct~22},
 number       = {AITR-2002-009},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-009.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-009.pdf},
}

@techreport{AITR-2002-010,
 author       = {Eepoel, John M. Van},
 title        = {Achieving Real-Time Mode Estimation through Offline Compilation},
 year         = {2002},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {As exploration of our solar system and outerspace move into the future, spacecraft are being developed to venture on increasingly challenging missions with bold objectives. The spacecraft tasked with completing these missions are becoming progressively more complex. This increases the potential for mission failure due to hardware malfunctions and unexpected spacecraft behavior. A solution to this problem lies in the development of an advanced fault management system. Fault management enables spacecraft to respond to failures and take repair actions so that it may continue its mission. The two main approaches developed for spacecraft fault management have been rule-based and model-based systems. Rules map sensor information to system behaviors, thus achieving fast response times, and making the actions of the fault management system explicit. These rules are developed by having a human reason through the interactions between spacecraft components. This process is limited by the number of interactions a human can reason about correctly. In the model-based approach, the human provides component models, and the fault management system reasons automatically about system wide interactions and complex fault combinations. This approach improves correctness, and makes explicit the underlying system models, whereas these are implicit in the rule- based approach. We propose a fault detection engine, Compiled Mode Estimation (CME) that unifies the strengths of the rule-based and model- based approaches. CME uses a compiled model to determine spacecraft behavior more accurately. Reasoning related to fault detection is compiled in an off-line process into a set of concurrent, localized diagnostic rules. These are then combined on-line along with sensor information to reconstruct the diagnosis of the system. These rules enable a human to inspect the diagnostic consequences of CME. Additionally, CME is capable of reasoning through component interactions automatically and still provide fast and correct responses. The implementation of this engine has been tested against the NEAR spacecraft advanced rule-based system, resulting in detection of failures beyond that of the rules. This evolution in fault detection will enable future missions to explore the furthest reaches of the solar system without the burden of human intervention to repair failed components.},
 month        = {oct~22},
 number       = {AITR-2002-010},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-010.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-010.pdf},
}

@techreport{AITR-2002-011,
 author       = {Grossman, J. P.},
 title        = {Design and Evaluation of the Hamal Parallel Computer},
 year         = {2002},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Parallel shared-memory machines with hundreds or thousands of processor-memory nodes have been built; in the future we will see machines with millions or even billions of nodes. Associated with such large systems is a new set of design challenges. Many problems must be addressed by an architecture in order for it to be successful; of these, we focus on three in particular. First, a scalable memory system is required. Second, the network messaging protocol must be fault-tolerant. Third, the overheads of thread creation, thread management and synchronization must be extremely low. This thesis presents the complete system design for Hamal, a shared-memory architecture which addresses these concerns and is directly scalable to one million nodes. Virtual memory and distributed objects are implemented in a manner that requires neither inter-node synchronization nor the storage of globally coherent translations at each node. We develop a lightweight fault-tolerant messaging protocol that guarantees message delivery and idempotence across a discarding network. A number of hardware mechanisms provide efficient support for massive multithreading and fine-grained synchronization. Experiments are conducted in simulation, using a trace-driven network simulator to investigate the messaging protocol and a cycle-accurate simulator to evaluate the Hamal architecture. We determine implementation parameters for the messaging protocol which optimize performance. A discarding network is easier to design and can be clocked at a higher rate, and we find that with this protocol its performance can approach that of a non-discarding network. Our simulations of Hamal demonstrate the effectiveness of its thread management and synchronization primitives. In particular, we find register-based synchronization to be an extremely efficient mechanism which can be used to implement a software barrier with a latency of only 523 cycles on a 512 node machine.},
 month        = {dec~5},
 number       = {AITR-2002-011},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-011.ps; ftp://publications.ai.mit.edu/ai-publications/2002/AITR-2002-011.pdf},
}

@techreport{AITR-2003-001,
 author       = {Kim, Philip Mjong-Hyon Shin},
 title        = {Understanding Subsystems in Biology through Dimensionality Reduction, Graph Partitioning and Analytical Modeling},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Biological systems exhibit rich and complex behavior through the orchestrated interplay of a large array of components. It is hypothesized that separable subsystems with some degree of functional autonomy exist; deciphering their independent behavior and functionality would greatly facilitate understanding the system as a whole. Discovering and analyzing such subsystems are hence pivotal problems in the quest to gain a quantitative understanding of complex biological systems. In this work, using approaches from machine learning, physics and graph theory, methods for the identification and analysis of such subsystems were developed. A novel methodology, based on a recent machine learning algorithm known as non-negative matrix factorization (NMF), was developed to discover such subsystems in a set of large-scale gene expression data. This set of subsystems was then used to predict functional relationships between genes, and this approach was shown to score significantly higher than conventional methods when benchmarking them against existing databases. Moreover, a mathematical treatment was developed to treat simple network subsystems based only on their topology (independent of particular parameter values). Application to a problem of experimental interest demonstrated the need for extentions to the conventional model to fully explain the experimental data. Finally, the notion of a subsystem was evaluated from a topological perspective. A number of different protein networks were examined to analyze their topological properties with respect to separability, seeking to find separable subsystems. These networks were shown to exhibit separability in a nonintuitive fashion, while the separable subsystems were of strong biological significance. It was demonstrated that the separability property found was not due to incomplete or biased data, but is likely to reflect biological structure.},
 month        = {feb~5},
 number       = {AITR-2003-001},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-001.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-001.pdf},
}

@techreport{AITR-2003-002,
 author       = {Chklovski, Timothy},
 title        = {Using Analogy to Acquire Commonsense Knowledge from Human Contributors},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The goal of the work reported here is to capture the commonsense knowledge of non-expert human contributors. Achieving this goal will enable more intelligent human-computer interfaces and pave the way for computers to reason about our world. In the domain of natural language processing, it will provide the world knowledge much needed for semantic processing of natural language. To acquire knowledge from contributors not trained in knowledge engineering, I take the following four steps: (i) develop a knowledge representation (KR) model for simple assertions in natural language, (ii) introduce cumulative analogy, a class of nearest-neighbor based analogical reasoning algorithms over this representation, (iii) argue that cumulative analogy is well suited for knowledge acquisition (KA) based on a theoretical analysis of effectiveness of KA with this approach, and (iv) test the KR model and the effectiveness of the cumulative analogy algorithms empirically. To investigate effectiveness of cumulative analogy for KA empirically, Learner, an open source system for KA by cumulative analogy has been implemented, deployed, and evaluated. (The site ''1001 Questions,'' is available at http://teach-computers.org/learner.html). Learner acquires assertion-level knowledge by constructing shallow semantic analogies between a KA topic and its nearest neighbors and posing these analogies as natural language questions to human contributors. Suppose, for example, that based on the knowledge about ''newspapers'' already present in the knowledge base, Learner judges ''newspaper'' to be similar to ''book'' and ''magazine.'' Further suppose that assertions ''books contain information'' and ''magazines contain information'' are also already in the knowledge base. Then Learner will use cumulative analogy from the similar topics to ask humans whether ''newspapers contain information.'' Because similarity between topics is computed based on what is already known about them, Learner exhibits bootstrapping behavior {\textemdash} the quality of its questions improves as it gathers more knowledge. By summing evidence for and against posing any given question, Learner also exhibits noise tolerance, limiting the effect of incorrect similarities. The KA power of shallow semantic analogy from nearest neighbors is one of the main findings of this thesis. I perform an analysis of commonsense knowledge collected by another research effort that did not rely on analogical reasoning and demonstrate that indeed there is sufficient amount of correlation in the knowledge base to motivate using cumulative analogy from nearest neighbors as a KA method. Empirically, evaluating the percentages of questions answered affirmatively, negatively and judged to be nonsensical in the cumulative analogy case compares favorably with the baseline, no-similarity case that relies on random objects rather than nearest neighbors. Of the questions generated by cumulative analogy, contributors answered 45},
 month        = {feb~12},
 number       = {AITR-2003-002},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-002.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-002.pdf},
}

@techreport{AITR-2003-003,
 author       = {Peshkin, Leonid},
 title        = {Reinforcement Learning by Policy Search},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {One objective of artificial intelligence is to model the behavior of an intelligent agent interacting with its environment. The environment's transformations can be modeled as a Markov chain, whose state is partially observable to the agent and affected by its actions; such processes are known as partially observable Markov decision processes (POMDPs). While the environment's dynamics are assumed to obey certain rules, the agent does not know them and must learn. In this dissertation we focus on the agent's adaptation as captured by the reinforcement learning framework. This means learning a policy{\textemdash}a mapping of observations into actions{\textemdash}based on feedback from the environment. The learning can be viewed as browsing a set of policies while evaluating them by trial through interaction with the environment. The set of policies is constrained by the architecture of the agent's controller. POMDPs require a controller to have a memory. We investigate controllers with memory, including controllers with external memory, finite state controllers and distributed controllers for multi-agent systems. For these various controllers we work out the details of the algorithms which learn by ascending the gradient of expected cumulative reinforcement. Building on statistical learning theory and experiment design theory, a policy evaluation algorithm is developed for the case of experience re-use. We address the question of sufficient experience for uniform convergence of policy evaluation and obtain sample complexity bounds for various estimators. Finally, we demonstrate the performance of the proposed algorithms on several domains, the most complex of which is simulated adaptive packet routing in a telecommunication network.},
 month        = {feb~14},
 number       = {AITR-2003-003},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-003.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-003.pdf},
}

@techreport{AITR-2003-004,
 author       = {Adler, Aaron D.},
 title        = {Segmentation and Alignment of Speech and Sketching in a Design Environment},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Sketches are commonly used in the early stages of design. Our previous system allows users to sketch mechanical systems that the computer interprets. However, some parts of the mechanical system might be too hard or too complicated to express in the sketch. Adding speech recognition to create a multimodal system would move us toward our goal of creating a more natural user interface. This thesis examines the relationship between the verbal and sketch input, particularly how to segment and align the two inputs. Toward this end, subjects were recorded while they sketched and talked. These recordings were transcribed, and a set of rules to perform segmentation and alignment was created. These rules represent the knowledge that the computer needs to perform segmentation and alignment. The rules successfully interpreted the 24 data sets that they were given.},
 month        = {feb~19},
 number       = {AITR-2003-004},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-004.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-004.pdf},
}

@techreport{AITR-2003-006,
 author       = {Morency, Louis-Philippe},
 title        = {Stereo-Based Head Pose Tracking Using Iterative Closest Point and Normal Flow Constraint},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In this text, we present two stereo-based head tracking techniques along with a fast 3D model acquisition system. The first tracking technique is a robust implementation of stereo-based head tracking designed for interactive environments with uncontrolled lighting. We integrate fast face detection and drift reduction algorithms with a gradient-based stereo rigid motion tracking technique. Our system can automatically segment and track a user's head under large rotation and illumination variations. Precision and usability of this approach are compared with previous tracking methods for cursor control and target selection in both desktop and interactive room environments. The second tracking technique is designed to improve the robustness of head pose tracking for fast movements. Our iterative hybrid tracker combines constraints from the ICP (Iterative Closest Point) algorithm and normal flow constraint. This new technique is more precise for small movements and noisy depth than ICP alone, and more robust for large movements than the normal flow constraint alone. We present experiments which test the accuracy of our approach on sequences of real and synthetic stereo images. The 3D model acquisition system we present quickly aligns intensity and depth images, and reconstructs a textured 3D mesh. 3D views are registered with shape alignment based on our iterative hybrid tracker. We reconstruct the 3D model using a new Cubic Ray Projection merging algorithm which takes advantage of a novel data structure: the linked voxel space. We present experiments to test the accuracy of our approach on 3D face modelling using real-time stereo images.},
 month        = {may~17},
 number       = {AITR-2003-006},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-006.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-006.pdf},
}

@techreport{AITR-2003-007,
 author       = {Grauman, Kristen},
 title        = {A Statistical Image-Based Shape Model for Visual Hull Reconstruction and 3D Structure Inference},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We present a statistical image-based ''shape + structure'' model for Bayesian visual hull reconstruction and 3D structure inference. The 3D shape of a class of objects is represented by sets of contours from silhouette views simultaneously observed from multiple calibrated cameras. Bayesian reconstructions of new shapes are then estimated using a prior density constructed with a mixture model and probabilistic principal components analysis. We show how the use of a class-specific prior in a visual hull reconstruction can reduce the effect of segmentation errors from the silhouette extraction process. The proposed method is applied to a data set of pedestrian images, and improvements in the approximate 3D models under various noise conditions are shown. We further augment the shape model to incorporate structural features of interest; unknown structural parameters for a novel set of contours are then inferred via the Bayesian reconstruction process. Model matching and parameter inference are done entirely in the image domain and require no explicit 3D construction. Our shape model enables accurate estimation of structure despite segmentation errors or missing views in the input silhouettes, and works even with only a single input view. Using a data set of thousands of pedestrian images generated from a synthetic model, we can accurately infer the 3D locations of 19 joints on the body based on observed silhouette contours from real images.},
 month        = {may~22},
 number       = {AITR-2003-007},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-007.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-007.pdf},
}

@techreport{AITR-2003-008,
 author       = {Fitzpatrick, Paul},
 title        = {From First Contact to Close Encounters},
 subtitle     = {A Developmentally Deep Perceptual System for a Humanoid Robot},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents a perceptual system for a humanoid robot that integrates abilities such as object localization and recognition with the deeper developmental machinery required to forge those competences out of raw physical experiences. It shows that a robotic platform can build up and maintain a system for object localization, segmentation, and recognition, starting from very little. What the robot starts with is a direct solution to achieving figure/ground separation: it simply 'pokes around' in a region of visual ambiguity and watches what happens. If the arm passes through an area, that area is recognized as free space. If the arm collides with an object, causing it to move, the robot can use that motion to segment the object from the background. Once the robot can acquire reliable segmented views of objects, it learns from them, and from then on recognizes and segments those objects without further contact. Both low-level and high-level visual features can also be learned in this way, and examples are presented for both: orientation detection and affordance recognition, respectively. The motivation for this work is simple. Training on large corpora of annotated real-world data has proven crucial for creating robust solutions to perceptual problems such as speech recognition and face detection. But the powerful tools used during training of such systems are typically stripped away at deployment. Ideally they should remain, particularly for unstable tasks such as object detection, where the set of objects needed in a task tomorrow might be different from the set of objects needed today. The key limiting factor is access to training data, but as this thesis shows, that need not be a problem on a robotic platform that can actively probe its environment, and carry out experiments to resolve ambiguity. This work is an instance of a general approach to learning a new perceptual judgment: find special situations in which the perceptual judgment is easy and study these situations to find correlated features that can be observed more generally.},
 month        = {jun~18},
 number       = {AITR-2003-008},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-008.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-008.pdf},
}

@techreport{AITR-2003-011,
 author       = {Monteleoni, Claire},
 title        = {Online Learning of Non-stationary Sequences},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. The performance of each expert may change over time in a manner unknown to the learner. We formulate a class of universal learning algorithms for this problem by expressing them as simple Bayesian algorithms operating on models analogous to Hidden Markov Models (HMMs). We derive a new performance bound for such algorithms which is considerably simpler than existing bounds. The bound provides the basis for learning the rate at which the identity of the optimal expert switches over time. We find an analytic expression for the a priori resolution at which we need to learn the rate parameter. We extend our scalar switching-rate result to models of the switching-rate that are governed by a matrix of parameters, i.e. arbitrary homogeneous HMMs. We apply and examine our algorithm in the context of the problem of energy management in wireless networks. We analyze the new results in the framework of Information Theory.},
 month        = {jun~12},
 number       = {AITR-2003-011},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-011.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-011.pdf},
}

@techreport{AITR-2003-012,
 author       = {Wehowsky, Andreas F.},
 title        = {Safe Distributed Coordination of Heterogeneous Robots through Dynamic Simple Temporal Networks},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Research on autonomous intelligent systems has focused on how robots can robustly carry out missions in uncertain and harsh environments with very little or no human intervention. Robotic execution languages such as RAPs, ESL, and TDL improve robustness by managing functionally redundant procedures for achieving goals. The model-based programming approach extends this by guaranteeing correctness of execution through pre-planning of non-deterministic timed threads of activities. Executing model-based programs effectively on distributed autonomous platforms requires distributing this pre-planning process. This thesis presents a distributed planner for modelbased programs whose planning and execution is distributed among agents with widely varying levels of processor power and memory resources. We make two key contributions. First, we reformulate a model-based program, which describes cooperative activities, into a hierarchical dynamic simple temporal network. This enables efficient distributed coordination of robots and supports deployment on heterogeneous robots. Second, we introduce a distributed temporal planner, called DTP, which solves hierarchical dynamic simple temporal networks with the assistance of the distributed Bellman- Ford shortest path algorithm. The implementation of DTP has been demonstrated successfully on a wide range of randomly generated examples and on a pursuer-evader challenge problem in simulation.},
 month        = {may~30},
 number       = {AITR-2003-012},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-012.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-012.pdf},
}

@techreport{AITR-2003-013,
 author       = {Marjanovic, Matthew J.},
 title        = {Teaching an Old Robot New Tricks},
 subtitle     = {Learning Novel Tasks via Interaction with People and Things},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {As AI has begun to reach out beyond its symbolic, objectivist roots into the embodied, experientialist realm, many projects are exploring different aspects of creating machines which interact with and respond to the world as humans do. Techniques for visual processing, object recognition, emotional response, gesture production and recognition, etc., are necessary components of a complete humanoid robot. However, most projects invariably concentrate on developing a few of these individual components, neglecting the issue of how all of these pieces would eventually fit together. The focus of the work in this dissertation is on creating a framework into which such specific competencies can be embedded, in a way that they can interact with each other and build layers of new functionality. To be of any practical value, such a framework must satisfy the real-world constraints of functioning in real-time with noisy sensors and actuators. The humanoid robot Cog provides an unapologetically adequate platform from which to take on such a challenge. This work makes three contributions to embodied AI. First, it offers a general-purpose architecture for developing behavior-based systems distributed over networks of PC's. Second, it provides a motor-control system that simulates several biological features which impact the development of motor behavior. Third, it develops a framework for a system which enables a robot to learn new behaviors via interacting with itself and the outside world. A few basic functional modules are built into this framework, enough to demonstrate the robot learning some very simple behaviors taught by a human trainer. A primary motivation for this project is the notion that it is practically impossible to build an ''intelligent'' machine unless it is designed partly to build itself. This work is a proof-of-concept of such an approach to integrating multiple perceptual and motor systems into a complete learning agent.},
 month        = {jun~20},
 number       = {AITR-2003-013},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-013.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-013.pdf},
}

@techreport{AITR-2003-014,
 author       = {Lee, Lily},
 title        = {Gait Analysis for Classification},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis describes a representation of gait appearance for the purpose of person identification and classification. This gait representation is based on simple localized image features such as moments extracted from orthogonal view video silhouettes of human walking motion. A suite of time-integration methods, spanning a range of coarseness of time aggregation and modeling of feature distributions, are applied to these image features to create a suite of gait sequence representations. Despite their simplicity, the resulting feature vectors contain enough information to perform well on human identification and gender classification tasks. We demonstrate the accuracy of recognition on gait video sequences collected over different days and times and under varying lighting environments. Each of the integration methods are investigated for their advantages and disadvantages. An improved gait representation is built based on our experiences with the initial set of gait representations. In addition, we show gender classification results using our gait appearance features, the effect of our heuristic feature selection method, and the significance of individual features.},
 month        = {jun~26},
 number       = {AITR-2003-014},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-014.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-014.pdf},
}

@techreport{AITR-2003-015,
 author       = {Timoner, Samson},
 title        = {Compact Representations for Fast Nonrigid Registration of Medical Images},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We develop efficient techniques for the non-rigid registration of medical images by using representations that adapt to the anatomy found in such images. Images of anatomical structures typically have uniform intensity interiors and smooth boundaries. We create methods to represent such regions compactly using tetrahedra. Unlike voxel-based representations, tetrahedra can accurately describe the expected smooth surfaces of medical objects. Furthermore, the interior of such objects can be represented using a small number of tetrahedra. Rather than describing a medical object using tens of thousands of voxels, our representations generally contain only a few thousand elements. Tetrahedra facilitate the creation of efficient non-rigid registration algorithms based on finite element methods (FEM). We create a fast, FEM-based method to non-rigidly register segmented anatomical structures from two subjects. Using our compact tetrahedral representations, this method generally requires less than one minute of processing time on a desktop PC. We also create a novel method for the non-rigid registration of gray scale images. To facilitate a fast method, we create a tetrahedral representation of a displacement field that automatically adapts to both the anatomy in an image and to the displacement field. The resulting algorithm has a computational cost that is dominated by the number of nodes in the mesh (about 10,000), rather than the number of voxels in an image (nearly 10,000,000). For many non-rigid registration problems, we can find a transformation from one image to another in five minutes. This speed is important as it allows use of the algorithm during surgery. We apply our algorithms to find correlations between the shape of anatomical structures and the presence of schizophrenia. We show that a study based on our representations outperforms studies based on other representations. We also use the results of our non-rigid registration algorithm as the basis of a segmentation algorithm. That algorithm also outperforms other methods in our tests, producing smoother segmentations and more accurately reproducing manual segmentations.},
 month        = {jul~4},
 number       = {AITR-2003-015},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-015.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-015.pdf},
}

@techreport{AITR-2003-016,
 author       = {Felzenszwalb, Pedro F.},
 title        = {Representation and Detection of Shapes in Images},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We present a set of techniques that can be used to represent and detect shapes in images. Our methods revolve around a particular shape representation based on the description of objects using triangulated polygons. This representation is similar to the medial axis transform and has important properties from a computational perspective. The first problem we consider is the detection of non-rigid objects in images using deformable models. We present an efficient algorithm to solve this problem in a wide range of situations, and show examples in both natural and medical images. We also consider the problem of learning an accurate non-rigid shape model for a class of objects from examples. We show how to learn good models while constraining them to the form required by the detection algorithm. Finally, we consider the problem of low-level image segmentation and grouping. We describe a stochastic grammar that generates arbitrary triangulated polygons while capturing Gestalt principles of shape regularity. This grammar is used as a prior model over random shapes in a low level algorithm that detects objects in images.},
 month        = {aug~8},
 number       = {AITR-2003-016},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-016.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-016.pdf},
}

@techreport{AITR-2003-017,
 author       = {Che, Austin},
 title        = {Fluorescence Assay for Polymerase Arrival Rates},
 year         = {2003},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {To engineer complex synthetic biological systems will require modular design, assembly, and characterization strategies. The RNA polymerase arrival rate (PAR) is defined to be the rate that RNA polymerases arrive at a specified location on the DNA. Designing and characterizing biological modules in terms of RNA polymerase arrival rates provides for many advantages in the construction and modeling of biological systems. PARMESAN is an in vitro method for measuring polymerase arrival rates using pyrrolo-dC, a fluorescent DNA base that can substitute for cytosine. Pyrrolo-dC shows a detectable fluorescence difference when in single-stranded versus double-stranded DNA. During transcription, RNA polymerase separates the two strands of DNA, leading to a change in the fluorescence of pyrrolo-dC. By incorporating pyrrolo-dC at specific locations in the DNA, fluorescence changes can be taken as a direct measurement of the polymerase arrival rate.},
 month        = {aug~31},
 number       = {AITR-2003-017},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-017.ps; ftp://publications.ai.mit.edu/ai-publications/2003/AITR-2003-017.pdf},
}

@techreport{AITR-2004-001,
 author       = {Stamatoiu, Oana L.},
 title        = {Learning Commonsense Categorical Knowledge in a Thread Memory System},
 year         = {2004},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {If we are to understand how we can build machines capable of broad purpose learning and reasoning, we must first aim to build systems that can represent, acquire, and reason about the kinds of commonsense knowledge that we humans have about the world. This endeavor suggests steps such as identifying the kinds of knowledge people commonly have about the world, constructing suitable knowledge representations, and exploring the mechanisms that people use to make judgments about the everyday world. In this work, I contribute to these goals by proposing an architecture for a system that can learn commonsense knowledge about the properties and behavior of objects in the world. The architecture described here augments previous machine learning systems in four ways: (1) it relies on a seven dimensional notion of context, built from information recently given to the system, to learn and reason about objects' properties; (2) it has multiple methods that it can use to reason about objects, so that when one method fails, it can fall back on others; (3) it illustrates the usefulness of reasoning about objects by thinking about their similarity to other, better known objects, and by inferring properties of objects from the categories that they belong to; and (4) it represents an attempt to build an autonomous learner and reasoner, that sets its own goals for learning about the world and deduces new facts by reflecting on its acquired knowledge. This thesis describes this architecture, as well as a first implementation, that can learn from sentences such as {\textquotedblleft}A blue bird flew to the tree'' and {\textquotedblleft}The small bird flew to the cage'' that birds can fly. One of the main contributions of this work lies in suggesting a further set of salient ideas about how we can build broader purpose commonsense artificial learners and reasoners.},
 month        = {may~18},
 number       = {AITR-2004-001},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-001.ps; ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-001.pdf},
}

@techreport{AITR-2004-002,
 author       = {Kennell, Jonathan},
 title        = {Generative Temporal Planning with Complex Processes},
 year         = {2004},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Autonomous vehicles are increasingly being used in mission-critical applications, and robust methods are needed for controlling these inherently unreliable and complex systems. This thesis advocates the use of model-based programming, which allows mission designers to program autonomous missions at the level of a coach or wing commander. To support such a system, this thesis presents the Spock generative planner. To generate plans, Spock must be able to piece together vehicle commands and team tactics that have a complex behavior represented by concurrent processes. This is in contrast to traditional planners, whose operators represent simple atomic or durative actions. Spock represents operators using the RMPL language, which describes behaviors using parallel and sequential compositions of state and activity episodes. RMPL is useful for controlling mobile autonomous missions because it allows mission designers to quickly encode expressive activity models using object-oriented design methods and an intuitive set of activity combinators. Spock also is significant in that it uniformly represents operators and plan-space processes in terms of Temporal Plan Networks, which support temporal flexibility for robust plan execution. Finally, Spock is implemented as a forward progression optimal planner that walks monotonically forward through plan processes, closing any open conditions and resolving any conflicts. This thesis describes the Spock algorithm in detail, along with example problems and test results.},
 month        = {may~18},
 number       = {AITR-2004-002},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-002.ps; ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-002.pdf},
}

@techreport{AITR-2004-003,
 author       = {Goler, Jonathan A.},
 title        = {BioJADE},
 subtitle     = {A Design and Simulation Tool for Synthetic Biological Systems},
 year         = {2004},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The next generations of both biological engineering and computer engineering demand that control be exerted at the molecular level. Creating, characterizing and controlling synthetic biological systems may provide us with the ability to build cells that are capable of a plethora of activities, from computation to synthesizing nanostructures. To develop these systems, we must have a set of tools not only for synthesizing systems, but also designing and simulating them. The BioJADE project provides a comprehensive, extensible design and simulation platform for synthetic biology. BioJADE is a graphical design tool built in Java, utilizing a database back end, and supports a range of simulations using an XML communication protocol. BioJADE currently supports a library of over 100 parts with which it can compile designs into actual DNA, and then generate synthesis instructions to build the physical parts. The BioJADE project contributes several tools to Synthetic Biology. BioJADE in itself is a powerful tool for synthetic biology designers. Additionally, we developed and now make use of a centralized BioBricks repository, which enables the sharing of BioBrick components between researchers, and vastly reduces the barriers to entry for aspiring Synthetic Biologists.},
 month        = {may~28},
 number       = {AITR-2004-003},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-003.ps; ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-003.pdf},
}

@techreport{AITR-2004-004,
 author       = {Hearn, Robert A.},
 title        = {Building Grounded Abstractions for Artificial Intelligence Programming},
 year         = {2004},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Most Artificial Intelligence (AI) work can be characterized as either {\textquotedblleft}high-level'' (e.g., logical, symbolic) or {\textquotedblleft}low-level'' (e.g., connectionist networks, behavior-based robotics). Each approach suffers from particular drawbacks. High-level AI uses abstractions that often have no relation to the way real, biological brains work. Low-level AI, on the other hand, tends to lack the powerful abstractions that are needed to express complex structures and relationships. I have tried to combine the best features of both approaches, by building a set of programming abstractions defined in terms of simple, biologically plausible components. At the {\textquotedblleft}ground level'', I define a primitive, perceptron-like computational unit. I then show how more abstract computational units may be implemented in terms of the primitive units, and show the utility of the abstract units in sample networks. The new units make it possible to build networks using concepts such as long-term memories, short-term memories, and frames. As a demonstration of these abstractions, I have implemented a simulator for {\textquotedblleft}creatures'' controlled by a network of abstract units. The creatures exist in a simple 2D world, and exhibit behaviors such as catching mobile prey and sorting colored blocks into matching boxes. This program demonstrates that it is possible to build systems that can interact effectively with a dynamic physical environment, yet use symbolic representations to control aspects of their behavior.},
 month        = {jun~16},
 number       = {AITR-2004-004},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-004.ps; ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-004.pdf},
}

@techreport{AITR-2004-006,
 author       = {Arsenio, Artur Miguel},
 title        = {Cognitive-Developmental Learning for a Humanoid Robot},
 subtitle     = {A Caregiver's Gift},
 year         = {2004},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The goal of this work is to build a cognitive system for the humanoid robot, Cog, that exploits human caregivers as catalysts to perceive and learn about actions, objects, scenes, people, and the robot itself. This thesis addresses a broad spectrum of machine learning problems across several categorization levels. Actions by embodied agents are used to automatically generate training data for the learning mechanisms, so that the robot develops categorization autonomously. Taking inspiration from the human brain, a framework of algorithms and methodologies was implemented to emulate different cognitive capabilities on the humanoid robot Cog. This framework is effectively applied to a collection of AI, computer vision, and signal processing problems. Cognitive capabilities of the humanoid robot are developmentally created, starting from infant-like abilities for detecting, segmenting, and recognizing percepts over multiple sensing modalities. Human caregivers provide a helping hand for communicating such information to the robot. This is done by actions that create meaningful events (by changing the world in which the robot is situated) thus inducing the ''compliant perception'' of objects from these human-robot interactions. Self-exploration of the world extends the robot's knowledge concerning object properties. This thesis argues for enculturating humanoid robots using infant development as a metaphor for building a humanoid robot's cognitive abilities. A human caregiver redesigns a humanoid's brain by teaching the humanoid robot as she would teach a child, using children's learning aids such as books, drawing boards, or other cognitive artifacts. Multi-modal object properties are learned using these tools and inserted into several recognition schemes, which are then applied to developmentally acquire new object representations. The humanoid robot therefore sees the world through the caregiver's eyes. Building an artificial humanoid robot's brain, even at an infant's cognitive level, has been a long quest which still lies only in the realm of our imagination. Our efforts towards such a dimly imaginable task are developed according to two alternate and complementary views: cognitive and developmental.},
 month        = {sep~26},
 number       = {AITR-2004-006},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-006.ps; ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-006.pdf},
}

@techreport{AITR-2004-007,
 author       = {Tucker-Kellogg, Lisa},
 title        = {Systematic Conformational Search with Constraint Satisfaction},
 year         = {2004},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Throughout biological, chemical, and pharmaceutical research, conformational searches are used to explore the possible three-dimensional configurations of molecules. This thesis describes a new systematic method for conformational search, including an application of the method to determining the structure of a peptide via solid-state NMR spectroscopy. A separate portion of the thesis is about protein-DNA binding, with a three-dimensional macromolecular structure determined by x-ray crystallography. The search method in this thesis enumerates all conformations of a molecule (at a given level of torsion angle resolution) that satisfy a set of local geometric constraints, such as constraints derived from NMR experiments. Systematic searches, historically used for small molecules, generally now use some form of divide-and-conquer for application to larger molecules. Our method can achieve a significant improvement in runtime by making some major and counter-intuitive modifications to traditional divide-and-conquer: (1) OmniMerge divides a polymer into many alternative pairs of subchains and searches all the pairs, instead of simply cutting in half and searching two subchains. Although the extra searches may appear wasteful, the bottleneck stage of the overall search, which is to re-connect the conformations of the largest subchains, can be greatly accelerated by the availability of alternative pairs of sidechains. (2) Propagation of disqualified conformations across overlapping subchains can disqualify infeasible conformations very rapidly, which further offsets the cost of searching the extra subchains of OmniMerge. (3) The search may be run in two stages, once at low-resolution using a side-effect of OmniMerge to determine an optimal partitioning of the molecule into efficient subchains; then again at high-resolution while making use of the precomputed subchains. (4) An A* function prioritizes each subchain based on estimated future search costs. Subchains with sufficiently low priority can be omitted from the search, which improves efficiency. A common theme of these four ideas is to make good choices about how to break the large search problem into lower-dimensional subproblems. In addition, the search method uses heuristic local searches within the overall systematic framework, to maintain the systematic guarantee while providing the empirical efficiency of stochastic search. These novel algorithms were implemented and the effectiveness of each innovation is demonstrated on a highly constrained peptide with 40 degrees of freedom.},
 month        = {oct~1},
 number       = {AITR-2004-007},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-007.ps; ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-007.pdf},
}

@techreport{AITR-2004-008,
 author       = {Werfel, Justin},
 title        = {Neural Network Models for Zebra Finch Song Production and Reinforcement Learning},
 year         = {2004},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The zebra finch is a standard experimental system for studying learning and generation of temporally extended motor patterns. The first part of this project concerned the evaluation of simple models for the operation and structure of the network in the motor nucleus RA. A directed excitatory chain with a global inhibitory network, for which experimental evidence exists, was found to produce waves of activity similar to those observed in RA; this similarity included one particularly important feature of the measured activity, synchrony between the onset of bursting in one neuron and the offset of bursting in another. Other models, which were simpler and more analytically tractable, were also able to exhibit this feature, but not for parameter values quantitatively close to those observed. Another issue of interest concerns how these networks are initially learned by the bird during song acquisition. The second part of the project concerned the analysis of exemplars of REINFORCE algorithms, a general class of algorithms for reinforcement learning in neural networks, which are on several counts more biologically plausible than standard prescriptions such as backpropagation. The former compared favorably with backpropagation on tasks involving single input-output pairs, though a noise analysis suggested it should not perform so well. On tasks involving trajectory learning, REINFORCE algorithms meet with some success, though the analysis that predicts their success on input-output-pair tasks fails to explain it for trajectories.},
 month        = {nov~9},
 number       = {AITR-2004-008},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-008.ps; ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-008.pdf},
}

@techreport{AITR-2004-009,
 author       = {Srebro, Nathan},
 title        = {Learning with Matrix Factorizations},
 year         = {2004},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Matrices that can be factored into a product of two simpler matrices can serve as a useful and often natural model in the analysis of tabulated or high-dimensional data. Models based on matrix factorization (Factor Analysis, PCA) have been extensively used in statistical analysis and machine learning for over a century, with many new formulations and models suggested in recent years (Latent Semantic Indexing, Aspect Models, Probabilistic PCA, Exponential PCA, Non-Negative Matrix Factorization and others). In this thesis we address several issues related to learning with matrix factorizations: we study the asymptotic behavior and generalization ability of existing methods, suggest new optimization methods, and present a novel maximum-margin high-dimensional matrix factorization formulation.},
 month        = {nov~22},
 number       = {AITR-2004-009},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-009.ps; ftp://publications.ai.mit.edu/ai-publications/2004/AITR-2004-009.pdf},
}

@techreport{AITR-2005-001,
 author       = {Kondacs, Attila},
 title        = {Determining articulator configuration in voiced stop consonants by matching time-domain patterns in pitch periods},
 year         = {2005},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In this thesis I will be concerned with linking the observed speech signal to the configuration of articulators. Due to the potentially rapid motion of the articulators, the speech signal can be highly non-stationary. The typical linear analysis techniques that assume quasi-stationarity may not have sufficient time-frequency resolution to determine the place of articulation. I argue that the traditional low and high-level primitives of speech processing, frequency and phonemes, are inadequate and should be replaced by a representation with three layers: 1. short pitch period resonances and other spatio-temporal patterns 2. articulator configuration trajectories 3. syllables. The patterns indicate articulator configuration trajectories (how the tongue, jaws, etc. are moving), which are interpreted as syllables and words. My patterns are an alternative to frequency. I use short time-domain features of the sound waveform, which can be extracted from each vowel pitch period pattern, to identify the positions of the articulators with high reliability. These features are important because by capitalizing on detailed measurements within a single pitch period, the rapid articulator movements can be tracked. No linear signal processing approach can achieve the combination of sensitivity to short term changes and measurement accuracy resulting from these nonlinear techniques. The measurements I use are neurophysiologically plausible: the auditory system could be using similar methods. I have demonstrated this approach by constructing a robust technique for categorizing the English voiced stops as the consonants B, D, or G based on the vocalic portions of their releases. The classification recognizes 93.5},
 month        = {jan~28},
 number       = {AITR-2005-001},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2005/AITR-2005-001.ps; ftp://publications.ai.mit.edu/ai-publications/2005/AITR-2005-001.pdf},
}

@techreport{AITR-2005-003,
 author       = {Taylor, Christopher J.},
 title        = {Simultaneous Localization and Tracking in Wireless Ad-hoc Sensor Networks},
 year         = {2005},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In this thesis we present LaSLAT, a sensor network algorithm that simultaneously localizes sensors, calibrates sensing hardware, and tracks unconstrained moving targets using only range measurements between the sensors and the target. LaSLAT is based on a Bayesian filter, which updates a probability distribution over the quantities of interest as measurements arrive. The algorithm is distributable, and requires only a constant amount of space with respect to the number of measurements incorporated. LaSLAT is easy to adapt to new types of hardware and new physical environments due to its use of intuitive probability distributions: one adaptation demonstrated in this thesis uses a mixture measurement model to detect and compensate for bad acoustic range measurements due to echoes. We also present results from a centralized Java implementation of LaSLAT on both two- and three-dimensional sensor networks in which ranges are obtained using the Cricket ranging system. LaSLAT is able to localize sensors to within several centimeters of their ground truth positions while recovering a range measurement bias for each sensor and the complete trajectory of the mobile.},
 month        = {may~31},
 number       = {AITR-2005-003},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2005/AITR-2005-003.ps; ftp://publications.ai.mit.edu/ai-publications/2005/AITR-2005-003.pdf},
}

@techreport{AITR-2005-004,
 author       = {Uzuner, Ozlem},
 title        = {Identifying Expression Fingerprints using Linguistic Information},
 year         = {2005},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents a technology to complement taxation-based policy proposals aimed at addressing the digital copyright problem. The approach presented facilitates identification of intellectual property using expression fingerprints. Copyright law protects expression of content. Recognizing literary works for copyright protection requires identification of the expression of their content. The expression fingerprints described in this thesis use a novel set of linguistic features that capture both the content presented in documents and the manner of expression used in conveying this content. These fingerprints consist of both syntactic and semantic elements of language. Examples of the syntactic elements of expression include structures of embedding and embedded verb phrases. The semantic elements of expression consist of high-level, broad semantic categories. Syntactic and semantic elements of expression enable generation of models that correctly identify books and their paraphrases 82},
 month        = {nov~16},
 number       = {AITR-2005-004},
 url          = {ftp://publications.ai.mit.edu/ai-publications/2005/AITR-2005-004.ps; ftp://publications.ai.mit.edu/ai-publications/2005/AITR-2005-004.pdf},
}

@techreport{AITR-219,
 author       = {Bobrow, Daniel G.},
 title        = {Natural Language Input for a Computer Problem Solving System},
 year         = {1964},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The STUDENT problem solving system, programmed in LISP, accepts as input a comfortable but restricted subset of English which can express a wide variety of algebra story problems. STUDENT finds the solution to a large class of these problems. STUDENT can utilize a store of global information not specific to any one problem, and may make assumptions about the interpretation of ambiguities in the wording of the problem being solved. If it uses such information or makes any assumptions, STUDENT communicates this fact to the user. The thesis includes a summary of other English language questions-answering systems. All these systems, and STUDENT, are evaluated according to four standard criteria. The linguistic analysis in STUDENT is a first approximation to the analytic portion of a semantic theory of discourse outlined in the thesis. STUDENT finds the set of kernel sentences which are the base of the input discourse, and transforms this sequence of kernel sentences into a set of simultaneous equations which form the semantic base of the STUDENT system. STUDENT then tries to solve this set of equations for the values of requested unknowns. If it is successful it gives the answers in English. If not, STUDENT asks the user for more information, and indicates the nature of the desired information. The STUDENT system is a first step toward natural language communication with computers. Further work on the semantic theory proposed should result in much more sophisticated systems.},
 month        = {sep~1},
 number       = {AITR-219},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-219.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-219.pdf},
}

@techreport{AITR-220,
 author       = {Raphael, Bertram},
 title        = {SIR},
 subtitle     = {A Computer Program for Semantic Information Retrieval},
 year         = {1964},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {SIR is a computer system, programmed in the LISP language, which accepts information and answers questions expressed in a restricted form of English. This system demonstrates what can reasonably be called an ability to ''understand'' semantic information. SIR`s semantic and deductive ability is based on the construction of an internal model, which uses word associations and property lists, for the relational information normally conveyed in conversational statements. A format-matching procedure extracts semantic content from English sentences. If an input sentence is declarative, the system adds appropriate information to the model. If an input sentence is a question, the system searches the model until it either finds the answer or determines why it cannot find the answer. In all cases SIR reports its conclusions. The system has some capacity to recognize exceptions to general rules, resolve certain semantic ambiguities, and modify its model structure in order to save computer memory space. Judging from its conversational ability, SIR, is a first step toward intelligent man-machine communication. The author proposes a next step by describing how to construct a more general system which is less complex and yet more powerful than SIR. This proposed system contains a generalized version of the SIR model, a formal logical system called SIR1, and a computer program for testing the truth of SIR1 statements with respect to the generalized model by using partial proof procedures in the predicate calculus. The thesis also describes the formal properties of SIR1 and how they relate to the logical structure of SIR.},
 month        = {jun~1},
 number       = {AITR-220},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-220.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-220.pdf},
}

@techreport{AITR-221,
 author       = {Teitelman, Warren},
 title        = {PILOT},
 subtitle     = {A Step Toward Man-Computer Symbiosis},
 year         = {1966},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {PILOT is a programming system constructed in LISP. It is designed to facilitate the development of programs by easing the familiar sequence: write some code, run the program, make some changes, write some more code, run the program again, etc. As a program becomes more complex, making these changes becomes harder and harder because the implications of changes are harder to anticipate. In the PILOT system, the computer plays an active role in this evolutionary process by providing the means whereby changes can be effected immediately, and in ways that seem natural to the user. The user of PILOT feels that he is giving advice, or making suggestions, to the computer about the operation of his programs, and that the system then performs the work necessary. The PILOT system is thus an interface between the user and his program, monitoring both in the requests of the user and operation of his program. The user may easily modify the PILOT system itself by giving it advice about its own operation. This allows him to develop his own language and to shift gradually onto PILOT the burden of performing routine but increasingly complicated tasks. In this way, he can concentrate on the conceptual difficulties in the original problem, rather than on the niggling tasks of editing, rewriting, or adding to his programs. Two detailed examples are presented. PILOT is a first step toward computer systems that will help man to formulate problems in the same way they now help him to solve them. Experience with it supports the claim that such ''symbiotic systems'' allow the programmer to attack and solve more difficult problems.},
 month        = {sep~1},
 number       = {AITR-221},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-221.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-221.pdf},
}

@techreport{AITR-222,
 author       = {Norton, Lewis Mark},
 title        = {ADEPT},
 subtitle     = {A Heuristic Program for Proving Theorems of Group Theory},
 year         = {1966},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A computer program, named ADEPT (A Distinctly Empirical Prover of Theorems), has been written which proves theorems taken from the abstract theory of groups. Its operation is basically heuristic, incorporating many of the techniques of the human mathematician in a ''natural'' way. This program has proved almost 100 theorems, as well as serving as a vehicle for testing and evaluating special-purpose heuristics. A detailed description of the program is supplemented by accounts of its performance on a number of theorems, thus providing many insights into the particular problems inherent in the design of a procedure capable of proving a variety of theorems from this domain. Suggestions have been formulated for further efforts along these lines, and comparisons with related work previously reported in the literature have been made.},
 month        = {sep~1},
 number       = {AITR-222},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-222.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-222.pdf},
}

@techreport{AITR-223,
 author       = {Martin, William A.},
 title        = {Symbolic Mathematical Laboratory},
 year         = {1967},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A large computer program has been developed to aid applied mathematicians in the solution of problems in non-numerical analysis which involve tedious manipulations of mathematical expressions. The mathematician uses typed commands and a light pen to direct the computer in the application of mathematical transformations; the intermediate results are displayed in standard text-book format so that the system user can decide the next step in the problem solution. Three problems selected from the literature have been solved to illustrate the use of the system. A detailed analysis of the problems of input, transformation, and display of mathematical expressions is also presented.},
 month        = {jan~1},
 number       = {AITR-223},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-223.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-223.pdf},
}

@techreport{AITR-224,
 author       = {Guzman-Arenas, Adolfo},
 title        = {Some Aspects of Pattern Recognition by Computer},
 year         = {1967},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A computer may gather a lot of information from its environment in an optical or graphical manner. A scene, as seen for instance from a TV camera or a picture, can be transformed into a symbolic description of points and lines or surfaces. This thesis describes several programs, written in the language CONVERT, for the analysis of such descriptions in order to recognize, differentiate and identify desired objects or classes of objects in the scene. Examples are given in each case. Although the recognition might be in terms of projections of 2-dim and 3-dim objects, we do not deal with stereoscopic information. One of our programs (Polybrick) identifies parallelepipeds in a scene which may contain partially hidden bodies and non- parallelepipedic objects. The program TD works mainly with 2-dimensional figures, although under certain conditions successfully identifies 3-dim objects. Overlapping objects are identified when they are transparent. A third program, DT, works with 3-dim and 2-dim objects, and does not identify objects which are not completely seen. Important restrictions and suppositions are: (a) the input is assumed perfect (noiseless), and in a symbolic format; (b) no perspective deformation is considered. A portion of this thesis is devoted to the study of models (symbolic representations) of the objects we want to identify; different schemes, some of them already in use, are discussed. Focusing our attention on the more general problem of identification of general objects when they substantially overlap, we propose some schemes for their recognition, and also analyze some problems that are met.},
 month        = {feb~1},
 number       = {AITR-224},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-224.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-224.pdf},
}

@techreport{AITR-225,
 author       = {Forte, Allen},
 title        = {Syntax-Based Analytic Reading of Musical Scores},
 year         = {1967},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {As part of a larger research project in musical structure, a program has been written which ''reads'' scores encoded in an input language isomorphic to music notation. The program is believed to be the first of its kind. From a small number of parsing rules the program derives complex configurations, each of which is associated with a set of reference points in a numerical representation of a time- continuum. The logical structure of the program is such that all and only the defined classes of events are represented in the output. Because the basis of the program is syntactic (in the sense that parsing operations are performed on formal structures in the input string), many extensions and refinements can be made without excessive difficulty. The program can be applied to any music which can be represented in the input language. At present, however, it constitutes the first stage in the development of a set of analytic tools for the study of so-called atonal music, the revolutionary and little understood music which has exerted a decisive influence upon contemporary practice of the art. The program and the approach to automatic data- structuring may be of interest to linguists and scholars in other fields concerned with basic studies of complex structures produced by human beings.},
 month        = {apr~1},
 number       = {AITR-225},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-225.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-225.pdf},
}

@techreport{AITR-226,
 author       = {Moses, Joel},
 title        = {Symbolic Integration},
 year         = {1967},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {SIN and SOLDIER are heuristic programs in LISP which solve symbolic integration problems. SIN (Symbolic INtegrator) solves indefinite integration problems at the difficulty approaching those in the larger integral tables. SIN contains several more methods than are used in the previous symbolic integration program SAINT, and solves most of the problems attempted by SAINT in less than one second. SOLDIER (SOLution of Ordinary Differential Equations Routine) solves first order, first degree ordinary differential equations at the level of a good college sophomore and at an average of about five seconds per problem attempted. The differences in philosophy and operation between SAINT and SIN are described, and suggestions for extending the work presented are made.},
 month        = {sep~1},
 number       = {AITR-226},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-226.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-226.pdf},
}

@techreport{AITR-227,
 author       = {Charniak, Eugene},
 title        = {CARPS},
 subtitle     = {A Program which Solves Calculus Word Problems},
 year         = {1968},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A program was written to solve calculus word problems. The program, CARPS (CALculus Rate Problem Solver), is restricted to rate problems. The overall plan of the program is similar to Bobrow`s STUDENT, the primary difference being the introduction of ''structures'' as the internal model in CARPS. Structures are stored internally as trees. Each structure is designed to hold the information gathered about one object. A description of CARPS is given by working through two problems, one in great detail. Also included is a critical analysis of STUDENT.},
 month        = {jul~1},
 number       = {AITR-227},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-227.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-227.pdf},
}

@techreport{AITR-228,
 author       = {Guzman-Arenas, Adolfo},
 title        = {Computer Recognition of Three-Dimensional Objects in a Visual Scene},
 year         = {1968},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Methods are presented (1) to partition or decompose a visual scene into the bodies forming it; (2) to position these bodies in three-dimensional space, by combining two scenes that make a stereoscopic pair; (3) to find the regions or zones of a visual scene that belong to its background; (4) to carry out the isolation of objects in (1) when the input has inaccuracies. Running computer programs implement the methods, and many examples illustrate their behavior. The input is a two-dimensional line-drawing of the scene, assumed to contain three-dimensional bodies possessing flat faces (polyhedra); some of them may be partially occluded. Suggestions are made for extending the work to curved objects. Some comparisons are made with human visual perception. The main conclusion is that it is possible to separate a picture or scene into the constituent objects exclusively on the basis of monocular geometric properties (on the basis of pure form); in fact, successful methods are shown.},
 month        = {dec~1},
 number       = {AITR-228},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-228.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-228.pdf},
}

@techreport{AITR-229,
 author       = {Beyer, Wendel Terry},
 title        = {Recognition of Topological Invariants by Iterative Arrays},
 year         = {1969},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A study is made of the recognition and transformation of figures by iterative arrays of finite state automata. A figure is a finite rectangular two-dimensional array of symbols. The iterative arrays considered are also finite, rectangular, and two-dimensional. The automata comprising any given array are called cells and are assumed to be isomorphic and to operate synchronously with the state of a cell at time t+1 being a function of the states of it and its four nearest neighbors at time t. At time t=0 each cell is placed in one of a fixed number of initial states. The pattern of initial states thus introduced represents the figure to be processed. The resulting sequence of array states represents a computation based on the input figure. If one waits for a specially designated cell to indicate acceptance or rejection of the figure, the array is said to be working on a recognition problem. If one waits for the array to come to a stable configuration representing an output figure, the array is said to be working on a transformation problem.},
 month        = {oct~1},
 number       = {AITR-229},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-229.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-229.pdf},
}

@techreport{AITR-230,
 author       = {Griffith, Arnold K.},
 title        = {Computer Recognition of Prismatic Solids},
 year         = {1970},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {An investigation is made into the problem of constructing a model of the appearance to an optical input device of scenes consisting of plane-faced geometric solids. The goal is to study algorithms which find the real straight edges in the scenes, taking into account smooth variations in intensity over faces of the solids, blurring of edges and noise. A general mathematical analysis is made of optimal methods for identifying the edge lines in figures, given a raster of intensities covering the entire field of view. There is given in addition a suboptimal statistical decision procedure, based on the model, for the identification of a line within a narrow band on the field of view given an array of intensities from within the band. A computer program has been written and extensively tested which implements this procedure and extracts lines from real scenes. Other programs were written which judge the completeness of extracted sets of lines, and propose and test for additional lines which had escaped initial detection. The performance of these programs is discussed in relation to the theory derived from the model, and with regard to their use of global information in detecting and proposing lines.},
 month        = {aug~1},
 number       = {AITR-230},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-230.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-230.pdf},
}

@techreport{AITR-231,
 author       = {Winston, Patrick H.},
 title        = {Learning Structural Descriptions from Examples},
 year         = {1970},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The research here described centers on how a machine can recognize concepts and learn concepts to be recognized. Explanations are found in computer programs that build and manipulate abstract descriptions of scenes such as those children construct from toy blocks. One program uses sample scenes to create models of simple configurations like the three-brick arch. Another uses the resulting models in making identifications. Throughout emphasis is given to the importance of using good descriptions when exploring how machines can come to perceive and understand the visual environment.},
 month        = {sep~1},
 number       = {AITR-231},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-231.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-231.pdf},
}

@techreport{AITR-232,
 author       = {Horn, Berthold K. P.},
 title        = {Shape from Shading},
 subtitle     = {A Method for Obtaining the Shape of a Smooth Opaque Object from One View},
 year         = {1970},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A method will be described for finding the shape of a smooth apaque object form a monocular image, given a knowledge of the surface photometry, the position of the lightsource and certain auxiliary information to resolve ambiguities. This method is complementary to the use of stereoscopy which relies on matching up sharp detail and will fail on smooth objects. Until now the image processing of single views has been restricted to objects which can meaningfully be considered two-dimensional or bounded by plane surfaces. It is possible to derive a first-order non-linear partial differential equation in two unknowns relating the intensity at the image points to the shape of the objects. This equation can be solved by means of an equivalent set of five ordinary differential equations. A curve traced out by solving this set of equations for one set of starting values is called a characteristic strip. Starting one of these strips from each point on some initial curve will produce the whole solution surface. The initial curves can usually be constructed around so-called singular points. A number of applications of this metod will be discussed including one to lunar topography and one to the scanning electron microscope. In both of these cases great simplifications occur in the equations. A note on polyhedra follows and a quantitative theory of facial make-up is touched upon. An implementation of some of these ideas on the PDP-6 computer with its attached image- dissector camera at the Artificial intelligence Laboratory will be described, and also a nose-recognition program.},
 month        = {nov~1},
 number       = {AITR-232},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-232.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-232.pdf},
}

@techreport{AITR-233,
 author       = {Banks, Edwin Roger},
 title        = {Information Processing and Transmission in Cellular Automata},
 year         = {1971},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A cellular automaton is an iterative array of very simple identical information processing machines called cells. Each cell can communicate with neighboring cells. At discrete moments of time the cells can change from one state to another as a function of the states of the cell and its neighbors. Thus on a global basis, the collection of cells is characterized by some type of behavior. The goal of this investigation was to determine just how simple the individual cells could be while the global behavior achieved some specified criterion of complexity {\textendash} usually the ability to perform a computation or to reproduce some pattern. The chief result described in this thesis is that an array of identical square cells (in two dimensions), each cell of which communicates directly with only its four nearest edge neighbors and each of which can exist in only two states, can perform any computation. This computation proceeds in a straight forward way. A configuration is a specification of the states of all the cells in some area of the iterative array. Another result described in this thesis is the existence of a self-reproducing configuration in an array of four-state cells, a reduction of four states from the previously known eight-state case. The technique of information processing in cellular arrays involves the synthesis of some basic components. Then the desired behaviors are obtained by the interconnection of these components. A chapter on components describes some sets of basic components. Possible applications of the results of this investigation, descriptions of some interesting phenomena (for vanishingly small cells), and suggestions for further study are given later.},
 month        = {jan~1},
 number       = {AITR-233},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-233.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-233.pdf},
}

@techreport{AITR-234,
 author       = {Krakauer, Lawrence W.},
 title        = {Computer Analysis of Visual Properties of Curved Objects},
 year         = {1971},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A method is presented for the visual analysis of objects by computer. It is particularly well suited for opaque objects with smoothly curved surfaces. The method extracts information about the object`s surface properties, including measures of its specularity, texture, and regularity. It also aids in determining the object`s shape. The application of this method to a simple recognition task {\textendash} the recognition of fruit {\textendash} is discussed. The results on a more complex smoothly curved object, a human face, are also considered.},
 month        = {may~1},
 number       = {AITR-234},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-234.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-234.pdf},
}

@techreport{AITR-235,
 author       = {Winograd, Terry},
 title        = {Procedures as a Representation for Data in a Computer Program for Understanding Natural Language},
 year         = {1971},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This paper describes a system for the computer understanding of English. The system answers questions, executes commands, and accepts information in normal English dialog. It uses semantic information and context to understand discourse and to disambiguate sentences. It combines a complete syntactic analysis of each sentence with a ''heuristic understander'' which uses different kinds of information about a sentence, other parts of the discourse, and general information about the world in deciding what the sentence means. It is based on the belief that a computer cannot deal reasonably with language unless it can ''understand'' the subject it is discussing. The program is given a detailed model of the knowledge needed by a simple robot having only a hand and an eye. We can give it instructions to manipulate toy objects, interrogate it about the scene, and give it information it will use in deduction. In addition to knowing the properties of toy objects, the program has a simple model of its own mentality. It can remember and discuss its plans and actions as well as carry them out. It enters into a dialog with a person, responding to English sentences with actions and English replies, and asking for clarification when its heuristic programs cannot understand a sentence through use of context and physical knowledge.},
 month        = {jan~1},
 number       = {AITR-235},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-235.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-235.pdf},
}

@techreport{AITR-242,
 author       = {Smoliar, Stephen W.},
 title        = {A Parallel Processing Model of Musical Structures},
 year         = {1971},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Euterpe is a real-time computer system for the modeling of musical structures. It provides a formalism wherein familiar concepts of musical analysis may be readily expressed. This is verified by its application to the analysis of a wide variety of conventional forms of music: Gregorian chant, Mediaeval polyphony, Back counterpoint, and sonata form. It may be of further assistance in the real-time experiments in various techniques of thematic development. Finally, the system is endowed with sound-synthesis apparatus with which the user may prepare tapes for musical performances.},
 month        = {sep~1},
 number       = {AITR-242},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-242.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-242.pdf},
}

@techreport{AITR-258,
 author       = {Hewitt, Carl},
 title        = {Description and Theoretical Analysis (Using Schemata) of Planner},
 subtitle     = {A Language for Proving Theorems and Manipulating Models in a Robot},
 year         = {1972},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Planner is a formalism for proving theorems and manipulating models in a robot. The formalism is built out of a number of problem- solving primitives together with a hierarchical multiprocess backtrack control structure. Statements can be asserted and perhaps later withdrawn as the state of the world changes. Under BACKTRACK control structure, the hierarchy of activations of functions previously executed is maintained so that it is possible to revert to any previous state. Thus programs can easily manipulate elaborate hypothetical tentative states. In addition PLANNER uses multiprocessing so that there can be multiple loci of changes in state. Goals can be established and dismissed when they are satisfied. The deductive system of PLANNER is subordinate to the hierarchical control structure in order to maintain the desired degree of control. The use of a general-purpose matching language as the basis of the deductive system increases the flexibility of the system. Instead of explicitly naming procedures in calls, procedures can be invoked implicitly by patterns of what the procedure is supposed to accomplish. The language is being applied to solve problems faced by a robot, to write special purpose routines from goal oriented language, to express and prove properties of procedures, to abstract procedures from protocols of their actions, and as a semantic base for English.},
 month        = {apr~6},
 number       = {AITR-258},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-258.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-258.pdf},
}

@techreport{AITR-266,
 author       = {Charniak, Eugene},
 title        = {Toward A Model Of Children's Story Comprehension},
 year         = {1972},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {How does a person answer questions about children's stories? For example, consider 'Janet wanted Jack's paints. She looked at the picture he was painting and said 'Those paints make your picture look funny.' The question to ask is 'Why did Janet say that?'. We propose a model which answers such questions by relating the story to background real world knowledge. The model tries to generate and answer important questions about the story as it goes along. Within this model we examine two questions about the story as it goes along. Within this model we examine two problems, how to organize this real world knowledge, and how it enters into more traditional linguistic questions such as deciding noun phrase reference.},
 month        = {dec~6},
 number       = {AITR-266},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-266.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-266.pdf},
}

@techreport{AITR-271,
 author       = {Waltz, David L.},
 title        = {Generating Semantic Descriptions From Drawings of Scenes With Shadows},
 year         = {1972},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The research reported here concerns the principles used to automatically generate three-dimensional representations from line drawings of scenes. The computer programs involved look at scenes which consist of polyhedra and which may contain shadows and various kinds of coincidentally aligned scene features. Each generated description includes information about edge shape (convex, concave, occluding, shadow, etc.), about the type of illumination for each region (illuminated, projected shadow, or oriented away from the light source), and about the spacial orientation of regions. The methods used are based on the labeling schemes of Huffman and Clowes; this research provides a considerable extension to their work and also gives theoretical explanations to the heuristic scene analysis work of Guzman, Winston, and others.},
 month        = {nov~6},
 number       = {AITR-271},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-271.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-271.pdf},
}

@techreport{AITR-281,
 author       = {(Editor), Patrick H. Winston},
 title        = {Progress in Vision and Robotics},
 year         = {1973},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The Vision Flashes are informal working papers intended primarily to stimulate internal interaction among participants in the A.I. Laboratory`s Vision and Robotics group. Many of them report highly tentative conclusions or incomplete work. Others deal with highly detailed accounts of local equipment and programs that lack general interest. Still others are of great importance, but lack the polish and elaborate attention to proper referencing that characterizes the more formal literature. Nevertheless, the Vision Flashes collectively represent the only documentation of an important fraction of the work done in machine vision and robotics. The purpose of this report is to make the findings more readily available, but since they are not revised as presented here, readers should keep in mind the original purpose of the papers!},
 month        = {may~6},
 number       = {AITR-281},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-281.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-281.pdf},
}

@techreport{AITR-283,
 author       = {Fahlman, Scott E.},
 title        = {A Planning System for Robot Construction Tasks},
 year         = {1973},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This paper describes BUILD, a computer program which generates plans for building specified structures out of simple objects such as toy blocks. A powerful heuristic control structure enables BUILD to use a number of sophisticated construction techniques in its plans. Among these are the incorporation of pre-existing structure into the final design, pre-assembly of movable sub- structures on the table, and use of the extra blocks as temporary supports and counterweights in the course of construction. BUILD does its planning in a modeled 3- space in which blocks of various shapes and sizes can be represented in any orientation and location. The modeling system can maintain several world models at once, and contains modules for displaying states, testing them for inter-object contact and collision, and for checking the stability of complex structures involving frictional forces. Various alternative approaches are discussed, and suggestions are included for the extension of BUILD-like systems to other domains. Also discussed are the merits of BUILD's implementation language, CONNIVER, for this type of problem solving.},
 month        = {may~6},
 number       = {AITR-283},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-283.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-283.pdf},
}

@techreport{AITR-291,
 author       = {McDermott, Drew V.},
 title        = {Assimilation of New Information by a Natural Language Understanding System},
 year         = {1974},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This work describes a program, called TOPLE, which uses a procedural model of the world to understand simple declarative sentences. It accepts sentences in a modified predicate calculus symbolism, and uses plausible reasoning to visualize scenes, resolve ambiguous pronoun and noun phrase references, explain events, and make conditional predications. Because it does plausible deduction, with tentative conclusions, it must contain a formalism for describing its reasons for its conclusions and what the alternatives are. When an inconsistency is detected in its world model, it uses its recorded information to resolve it, one way or another. It uses simulation techniques to make deductions about creatures motivation and behavior, assuming they are goal-directed beings like itself.},
 month        = {feb~6},
 number       = {AITR-291},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-291.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-291.pdf},
}

@techreport{AITR-294,
 author       = {Goldstein, Ira P.},
 title        = {Understanding Simple Picture Programs},
 year         = {1974},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {What are the characteristics of the process by which an intent is transformed into a plan and then a program? How is a program debugged? This paper analyzes these questions in the context of understanding simple turtle programs. To understand and debug a program, a description of its intent is required. For turtle programs, this is a model of the desired geometric picture. a picture language is provided for this purpose. Annotation is necessary for documenting the performance of a program in such a way that the system can examine the procedures behavior as well as consider hypothetical lines of development due to tentative debugging edits. A descriptive framework representing both causality and teleology is developed. To understand the relation between program and model, the plan must be known. The plan is a description of the methodology for accomplishing the model. Concepts are explicated for translating the global intent of a declarative model into the local imperative code of a program. Given the plan, model and program, the system can interpret the picture and recognize inconsistencies. The description of the discrepancies between the picture actually produced by the program and the intended scene is the input to a debugging system. Repair of the program is based on a combination of general debugging techniques and specific fixing knowledge associated with the geometric model primitives. In both the plan and repairing the bugs, the system exhibits an interesting style of analysis. It is capable of debugging itself and reformulating its analysis of a plan or bug in response to self-criticism. In this fashion, it can qualitatively reformulate its theory of the program or error to account for surprises or anomalies.},
 month        = {apr~6},
 number       = {AITR-294},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-294.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-294.pdf},
}

@techreport{AITR-297,
 author       = {Sussman, Gerald J.},
 title        = {A Computational Model of Skill Acquisition},
 year         = {1973},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis confronts the nature of the process of learning an intellectual skill, the ability to solve problems efficiently in a particular domain of discourse. The investigation is synthetic; a computational performance model, HACKER, is displayed. Hacker is a computer problem-solving system whose performance improves with practice. HACKER maintains performance knowledge as a library of procedures indexed by descriptions of the problem types for which the procedures are appropriate. When applied to a problem, HACKER tries to use a procedure from this ''Answer Library''. If no procedure is found to be applicable, HACKER writes one using more general knowledge of the problem domain and of programming techniques. This new program may be generalized and added to the Answer Library.},
 month        = {aug~6},
 number       = {AITR-297},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-297.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-297.pdf},
}

@techreport{AITR-310,
 author       = {Winston, Patrick H.},
 title        = {New Progress in Artificial Intelligence},
 year         = {1974},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report concentrates on progress during the last two years at the M.I.T. Artificial Intelligence Laboratory. Topics covered include the representation of knowledge, understanding English, learning and debugging, understanding vision and productivity technology. It is stressed that these various areas are tied closely together through certain fundamental issues and problems.},
 month        = {sep~6},
 number       = {AITR-310},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-310.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-310.pdf},
}

@techreport{AITR-316,
 author       = {Rubin, Ann D.},
 title        = {Hypothesis Formation and Evaluation in Medical Diagnosis},
 year         = {1975},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis describes some aspects of a computer system for doing medical diagnosis in the specialized field of kidney disease. Because such a system faces the spectre of combinatorial explosion, this discussion concentrates on heuristics which control the number of concurrent hypotheses and efficient ''compiled'' representations of medical knowledge. In particular, the differential diagnosis of hematuria (blood in the urine) is discussed in detail. A protocol of a simulated doctor/patient interaction is presented and analyzed to determine the crucial structures and processes involved in the diagnosis procedure. The data structure proposed for representing medical information revolves around elementary hypotheses which are activated when certain disposing of findings, activating hypotheses, evaluating hypotheses locally and combining hypotheses globally is examined for its heuristic implications. The thesis attempts to fit the problem of medical diagnosis into the framework of other Artifcial Intelligence problems and paradigms and in particular explores the notions of pure search vs. heuristic methods, linearity and interaction, local vs. global knowledge and the structure of hypotheses within the world of kidney disease.},
 month        = {jan~6},
 number       = {AITR-316},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-316.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-316.pdf},
}

@techreport{AITR-345,
 author       = {Freuder, Eugene C.},
 title        = {Computer System for Visual Recognition Using Active Knowledge},
 year         = {1976},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A system for visual recognition is described, with implications for the general problem of representation of knowledge to assist control. The immediate objective is a computer system that will recognize objects in a visual scene, specifically hammers. The computer receives an array of light intensities from a device like a television camera. It is to locate and identify the hammer if one is present. The computer must produce from the numerical ''sensory data'' a symbolic description that constitutes its perception of the scene. Of primary concern is the control of the recognition process. Control decisions should be guided by the partial results obtained on the scene. If a hammer handle is observed this should suggest that the handle is part of a hammer and advise where to look for the hammer head. The particular knowledge that a handle has been found combines with general knowledge about hammers to influence the recognition process. This use of knowledge to direct control is denoted here by the term ''active knowledge''. A descriptive formalism is presented for visual knowledge which identifies the relationships relevant to the active use of the knowledge. A control structure is provided which can apply knowledge organized in this fashion actively to the processing of a given scene.},
 month        = {jun~6},
 number       = {AITR-345},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-345.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-345.pdf},
}

@techreport{AITR-346,
 author       = {Hollerbach, John M.},
 title        = {Hierarchical Shape Description of Objects by Selection and Modification of Prototypes},
 year         = {1975},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {An approach towards shape description, based on prototype modification and generalized cylinders, has been developed and applied to the object domains pottery and polyhedra: (1) A program describes and identifies pottery from vase outlines entered as lists of points. The descriptions have been modeled after descriptions by archeologists, with the result that identifications made by the program are remarkably consisten with those of the archeologists. It has been possible to quantify their shape descriptors, which are everyday terms in our language applied to many sorts of objects besides pottery, so that the resulting descriptions seem very natural. (2) New parsing strategies for polyhedra overcome some limitations of previous work. A special feature is that the processes of parsing and identification are carried out simultaneously.},
 month        = {nov~6},
 number       = {AITR-346},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-346.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-346.pdf},
}

@techreport{AITR-347,
 author       = {Moore, Robert Carter},
 title        = {Reasoning from Incomplete Knowledge in a Procedural Deduction System},
 year         = {1975},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {One very useful idea in AI research has been the notion of an explicit model of a problem situation. Procedural deduction languages, such as PLANNER, have been valuable tools for building these models. But PLANNER and its relatives are very limited in their ability to describe situations which are only partially specified. This thesis explores methods of increasing the ability of procedural deduction systems to deal with incomplete knowledge. The thesis examines in detail, problems involving negation, implication, disjunction, quantification, and equality. Control structure issues and the problem of modelling change under incomplete knowledge are also considered. Extensive comparisons are also made with systems for mechanica theorem proving.},
 month        = {dec~6},
 number       = {AITR-347},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-347.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-347.pdf},
}

@techreport{AITR-352,
 author       = {Kleer, Johan De},
 title        = {Qualitative and Quantitative Knowledge in Classical Mechanics},
 year         = {1975},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis investigates what knowledge is necessary to solve mechanics problems. A program NEWTON is described which understands and solves problems in mechanics mini-world of objects moving on surfaces. Facts and equations such as those given in mechanics text need to be represented. However, this is far from sufficient to solve problems. Human problem solvers rely on ''common sense'' and ''qualitative'' knowledge which the physics text tacitly assumes to be present. A mechanics problem solver must embody such knowledge. Quantitative knowledge given by equations and more qualitative common sense knowledge are the major research points exposited in this thesis. The major issue in solving problems is planning. Planning involves tentatively outlining a possible path to the solution without actually solving the problem. Such a plan needs to be constructed and debugged in the process of solving the problem. Envisionment, or qualitative simulation of the event, plays a central role in this planning process.},
 month        = {dec~6},
 number       = {AITR-352},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-352.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-352.pdf},
}

@techreport{AITR-354,
 author       = {Rich, Charles and Shrobe, Howard E.},
 title        = {Initial Report on a LISP Programmer's Apprentice},
 year         = {1976},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This is an initial report on the design and partial implementation of a LISP programmers apprentice, an interactive programming system to be used by an expert programmer in the design, coding, and maintenance of large, complex programs.},
 month        = {dec~6},
 number       = {AITR-354},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-354.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-354.pdf},
}

@techreport{AITR-362,
 author       = {Brown, Allen},
 title        = {Qualitative Knowledge, Casual Reasoning and the Localization of Failures},
 year         = {1976},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report investigates some techinques appropriate to representing the knowledge necessary for understanding a class of electronic machines {\textendash} radio receivers. A computational performance model - WATSON - is presented. WATSONs task is to isolate failures in radio receivers whose principles of operation have been appropriately described in his knowledge base. The thesis of the report is that hierarchically organized representational structures are essential to the understanding of complex mechanisms. Such structures lead not only to descriptions of machine operation at many levels of detail, but also offer a powerful means of organizing ''specialist'' knowledge for the repair of machines when they are broken.},
 month        = {nov~6},
 number       = {AITR-362},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-362.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-362.pdf},
}

@techreport{AITR-397,
 author       = {Lozano-Perez, Tomas},
 title        = {The Design of a Mechanical Assembly System},
 year         = {1976},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis describes a mechanical assembly system called LAMA (Language for Automatic Mechanical Assembly). The goal of the work was to create a mechanical assembly system that transforms a high-level description of an automatic assembly operation into a program or execution by a computer controlled manipulator. This system allows the initial description of the assembly to be in terms of the desired effects on the parts being assembled. Languages such as WAVE [Bolles     Paul] and MINI [Silver] fail to meet this goal by requiring the assembly operation to be described in terms of manipulator motions. This research concentrates on the spatial complexity of mechanical assembly operations. The assembly problem is seen as the problem of achieving a certain set of geometrical constraints between basic objects while avoiding unwanted collisions. The thesis explores how these two facets, desired constraints and unwanted collisions, affect the primitive operations of the domain.},
 month        = {dec~6},
 number       = {AITR-397},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-397.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-397.pdf},
}

@techreport{AITR-402,
 author       = {Mcdermott, Drew Vincent},
 title        = {Flexibility and Efficiency in a Computer Program for Designing Circuits},
 year         = {1977},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report is concerned with the problem of achieving flexibility (additivity, modularity) and efficiency (performance, expertise) simultaneously in one AI program. It deals with the domain of elementary electronic circuit design. The proposed solution is to provide a deduction-driven problem solver with built-in-control-structure concepts. This problem solver and its knowledge base in the applicaitn areas of design and electronics are descrbed. The prgram embodying it is being used to explore the solutionof some modest problems in circuit design. It is concluded that shallow reasoning about problem-solver plans is necessary for flexibility, and can be implemented with reasonable efficiency.},
 month        = {jun~6},
 number       = {AITR-402},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-402.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-402.pdf},
}

@techreport{AITR-403,
 author       = {Brown, Richard},
 title        = {Use of Analogy to Achieve New Expertise},
 year         = {1977},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We will take the view that the end result of problem solving in some world should be increased expertness. In the context of computers, increasing expertness means writing programs. This thesis is about a process, reasoning by analogy that writes programs. Analogy relates one problem world to another. We will call the world in which we have an expert problem solver the IMAGE world, and the other world the DOMAIN world. Analogy will construct an expert problem solver in the domain world using the image world expert for inspiration.},
 month        = {apr~6},
 number       = {AITR-403},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-403.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-403.pdf},
}

@techreport{AITR-418,
 author       = {Kuipers, Benjamin J.},
 title        = {Representing Knowledge of Large-Scale Space},
 year         = {1977},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This dissertation presents a model of the knowledge a person has about the spatial structure of a large-scale environment: the ''cognitive map''. The functions of the cognitive map are to assimilate new information about the environment, to represent the current position, and to answer route-finding and relative-position problems. This model (called the TOUR model) analyzes the cognitive map in terms of symbolic descriptions of the environment and operations on those descriptions. Knowledge about a particular environment is represented in terms of route descriptions, a topological network of paths and places, multiple frames of reference for relative positions, dividing boundaries, and a structure of containing regions. The current position is described by the ''You Are Here'' pointer, which acts as a working memory and a focus of attention. Operations on the cognitive map are performed by inference rules which act to transfer information among different descriptions and the ''You Are Here'' pointer. The TOUR model shows how the particular descriptions chosen to represent spatial knowledge support assimilation of new information from local observations into the cognitive map, and how the cognitive map solves route-finding and relative-position problems. A central theme of this research is that the states of partial knowledge supported by a representation are responsible for its ability to function with limited information of computational resources. The representations in the TOUR model provide a rich collection of states of partial knowledge, and therefore exhibit flexible, ''common- sense'' behavior.},
 month        = {jul~6},
 number       = {AITR-418},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-418.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-418.pdf},
}

@techreport{AITR-419,
 author       = {Doyle, Jon},
 title        = {Truth Maintenance Systems for Problem Solving},
 year         = {1978},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The thesis developed here is that reasoning programs which take care to record the logical justifications for program beliefs can apply several powerful, but simple, domain- independent algorithms to (1) maintain the consistency of program beliefs, (2) realize substantial search efficiencies, and (3) automatically summarize explanations of program beliefs. These algorithms are the recorded justifications to maintain the consistency and well founded basis of the set of beliefs. The set of beliefs can be efficiently updated in an incremental manner when hypotheses are retracted and when new information is discovered. The recorded justifications also enable the pinpointing of exactly whose assumptions which support any particular belief. The ability to pinpoint the underlying assumptions is the basis for an extremely powerful domain-independent backtracking method. This method, called Dependency-Directed Backtracking, offers vastly improved performance over traditional backtracking algorithms.},
 month        = {jan~6},
 number       = {AITR-419},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-419.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-419.pdf},
}

@techreport{AITR-439,
 author       = {Raibert, Marc H.},
 title        = {Motor Control and Learning by the State Space Model},
 year         = {1977},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A model is presented that deals with problems of motor control, motor learning, and sensorimotor integration. The equations of motion for a limb are parameterized and used in conjunction with a quantized, multi- dimensional memory organized by state variables. Descriptions of desired trajectories are translated into motor commands which will replicate the specified motions. The initial specification of a movement is free of information regarding the mechanics of the effector system. Learning occurs without the use of error correction when practice data are collected and analyzed.},
 month        = {sep~6},
 number       = {AITR-439},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-439.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-439.pdf},
}

@techreport{AITR-450,
 author       = {Fahlman, Scott E.},
 title        = {A System for Representing and Using Real-World Knowledge},
 year         = {1977},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report describes a knowledge-base system in which the information is stored in a network of small parallel processing elements {\textendash} node and link units {\textendash} which are controlled by an external serial computer. This network is similar to the semantic network system of Quillian, but is much more tightly controlled. Such a network can perform certain critical deductions and searches very quickly; it avoids many of the problems of current systems, which must use complex heuristics to limit and guided their searches. It is argued (with examples) that the key operation in a knowledge-base system is the intersection of large explicit and semi-explicit sets. The parallel network system does this in a small, essentially constant number of cycles; a serial machine takes time proportional to the size of the sets, except in special cases.},
 month        = {dec~6},
 number       = {AITR-450},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-450.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-450.pdf},
}

@techreport{AITR-457,
 author       = {Woodham, Robert J.},
 title        = {Reflectance Map Techniques for Analyzing Surface Defects in Metal Castings},
 year         = {1978},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report explores the relation between image intensity and object shape. It is shown that image intensity is related to surface orientation and that a variation in image intensity is related to surface curvature. Computational methods are developed which use the measured intensity variation across surfaces of smooth objects to determine surface orientation. In general, surface orientation is not determined locally by the intensity value recorded at each image point. Tools are needed to explore the problem of determining surface orientation from image intensity. The notion of gradient space , popularized by Huffman and Mackworth, is used to represent surface orientation. The notion of a reflectance map, originated by Horn, is used to represent the relation between surface orientation image intensity. The image Hessian is defined and used to represent surface curvature. Properties of surface curvature are expressed as constraints on possible surface orientations corresponding to a given image point. Methods are presented which embed assumptions about surface curvature in algorithms for determining surface orientation from the intensities recorded in a single view. If additional images of the same object are obtained by varying the direction of incident illumination, then surface orientation is determined locally by the intensity values recorded at each image point. This fact is exploited in a new technique called photometric stereo. The visual inspection of surface defects in metal castings is considered. Two casting applications are discussed. The first is the precision investment casting of turbine blades and vanes for aircraft jet engines. In this application, grain size is an important process variable. The existing industry standard for estimating the average grain size of metals is implemented and demonstrated on a sample turbine vane. Grain size can be computed form the measurements obtained in an image, once the foreshortening effects of surface curvature are accounted for. The second is the green sand mold casting of shuttle eyes for textile looms. Here, physical constraints inherent to the casting process translate into these constraints, it is necessary to interpret features of intensity as features of object shape. Both applications demonstrate that successful visual inspection requires the ability to interpret observed changes in intensity in the context of surface topography. The theoretical tools developed in this report provide a framework for this interpretation.},
 month        = {jun~6},
 number       = {AITR-457},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-457.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-457.pdf},
}

@techreport{AITR-472,
 author       = {Michener, Edwina Rissland},
 title        = {The Structure of Mathematical Knowledge},
 year         = {1978},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report develops a conceptual framework in which to talk about mathematical knowledge. There are several broad categories of mathematical knowledge: results which contain the traditional logical aspects of mathematics; examples which contain illustrative material; and concepts which include formal and informal ideas, that is, definitions and heuristics.},
 month        = {aug~6},
 number       = {AITR-472},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-472.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-472.pdf},
}

@techreport{AITR-474,
 author       = {Steele, Jr. Guy Lewis},
 title        = {RABBIT},
 subtitle     = {A Compiler for SCHEME},
 year         = {1978},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We have developed a compiler for the lexically-scoped dialect of LISP known as SCHEME. The compiler knows relatively little about specific data manipulation primitives such as arithmetic operators, but concentrates on general issues of environment and control. Rather than having specialized knowledge about a large variety of control and environment constructs, the compiler handles only a small basis set which reflects the semantics of lambda- calculus. All of the traditional imperative constructs, such as sequencing, assignment, looping, GOTO, as well as many standard LISP constructs such as AND, OR, and COND, are expressed in macros in terms of the applicative basis set. A small number of optimization techniques, coupled with the treatment of function calls as GOTO statements, serve to produce code as good as that produced by more traditional compilers. The macro approach enables speedy implementation of new constructs as desired without sacrificing efficiency in the generated code. A fair amount of analysis is devoted to determining whether environments may be stack-allocated or must be heap- allocated. Heap-allocated environments are necessary in general because SCHEME (unlike Algol 60 and Algol 68, for example) allows procedures with free lexically scoped variables to be returned as the values of other procedures; the Algol stack-allocation environment strategy does not suffice. The methods used here indicate that a heap- allocating generalization of the ''display'' technique leads to an efficient implementation of such ''upward funargs''. Moreover, compile- time optimization and analysis can eliminate many ''funargs'' entirely, and so far fewer environment structures need be allocated at run time than might be expected. A subset of SCHEME (rather than triples, for example) serves as the representation intermediate between the optimized SCHEME code and the final output code; code is expressed in this subset in the so-called continuation-passing style. As a subset of SCHEME, it enjoys the same theoretical properties; one could even apply the same optimizer used on the input code to the intermediate code. However, the subset is so chosen that all temporary quantities are made manifest as variables, and no control stack is needed to evaluate it. As a result, this apparently applicative representation admits an imperative interpretation which permits easy transcription to final imperative machine code. These qualities suggest that an applicative language like SCHEME is a better candidate for an UNCOL than the more imperative candidates proposed to date.},
 month        = {may~6},
 number       = {AITR-474},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-474.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-474.pdf},
}

@techreport{AITR-483,
 author       = {Vanlehn, Kurt A.},
 title        = {Determining the Scope of English Quantifiers},
 year         = {1978},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {How can one represent the meaning of English sentences in a formal logical notation such that the translation of English into this logical form is simple and general? This report answers this question for a particular kind of meaning, namely quantifier scope, and for a particular part of the translation, namely the syntactic influence on the translation. Rules are presented which predict, for example, that the sentence: Everyone in this room speaks at least two languages. has the quantifier scope AE in standard predicate calculus, while the sentence: At lease two languages are spoken by everyone in this room. has the quantifier scope EA. Three different logical forms are presented, and their translation rules are examined. One of the logical forms is predicate calculus. The translation rules for it were developed by Robert May (May 19 77). The other two logical forms are Skolem form and a simple computer programming language. The translation rules for these two logical forms are new. All three sets of translation rules are shown to be general, in the sense that the same rules express the constraints that syntax imposes on certain other linguistic phenomena. For example, the rules that constrain the translation into Skolem form are shown to constrain definite np anaphora as well. A large body of carefully collected data is presented, and used to assess the empirical accuracy of each of the theories. None of the three theories is vastly superior to the others. However, the report concludes by suggesting that a combination of the two newer theories would have the greatest generality and the highest empirical accuracy.},
 month        = {jun~6},
 number       = {AITR-483},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-483.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-483.pdf},
}

@techreport{AITR-492,
 author       = {Waters, Richard C.},
 title        = {Automatic Analysis of the Logical Structure of Programs},
 year         = {1978},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report presents a method for viewing complex programs as built up out of simpler ones. The central idea is that typical programs are built up in a small number of stereotyped ways. The method is designed to make it easier for an automatic system to work with programs. It focuses on how the primitive operations performed by a program are combined together in order to produce the actions of the program as a whole. It does not address the issue of how complex data structures are built up from simpler ones, nor the relationships between data structures and the operations performed on them.},
 month        = {dec~6},
 number       = {AITR-492},
 url          = {ftp://publications.ai.mit.edu/ai-publications/0-499/AITR-492.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-492.pdf},
}

@techreport{AITR-503,
 author       = {Shrobe, Howard Elliot},
 title        = {Dependency Directed Reasoning for Complex Program Understanding},
 year         = {1979},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Artificial Intelligence research involves the creation of extremely complex programs which must possess the capability to introspect, learn, and improve their expertise. Any truly intelligent program must be able to create procedures and to modify them as it gathers information from its experience. [Sussman, 1975] produced such a system for a 'mini-world'; but truly intelligent programs must be considerably more complex. A crucial stepping stone in AI research is the development of a system which can understand complex programs well enough to modify them. There is also a complexity barrier in the world of commercial software which is making the cost of software production and maintenance prohibitive. Here too a system which is capable of understanding complex programs is a necessary step. The Programmer's Apprentice Project [Rich and Shrobe, 76] is attempting to develop an interactive programming tool which will help expert programmers deal with the complexity involved in engineering a large software system. This report describes REASON, the deductive component of the programmer's apprentice. REASON is intended to help expert programmers in the process of evolutionary program design. REASON utilizes the engineering techniques of modelling, decomposition, and analysis by inspection to determine how modules interact to achieve the desired overall behavior of a program. REASON coordinates its various sources of knowledge by using a dependency-directed structure which records the justification for each deduction it makes. Once a program has been analyzed these justifications can be summarized into a teleological structure called a plan which helps the system understand the impact of a proposed program modification.},
 month        = {apr~6},
 number       = {AITR-503},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-503.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-503.pdf},
}

@techreport{AITR-512,
 author       = {Stevens, Kent A.},
 title        = {Surface Perception from Local Analysis of Texture and Contour},
 year         = {1980},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The visual analysis of surface shape from texture and surface contour is treated within a computational framework. The aim of this study is to determine valid constraints that are sufficient to allow surface orientation and distance (up to a multiplicative constant) to be computed from the image of surface texture and of surface contours.},
 month        = {feb~6},
 number       = {AITR-512},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-512.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-512.pdf},
}

@techreport{AITR-515,
 author       = {Mason, Matthew Thomas},
 title        = {Compliance and Force Control for Computer Controlled Manipulators},
 year         = {1979},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Compliant motion occurs when the manipulator position is constrained by the task geometry. Compliant motion may be produced either by a passive mechanical compliance built in to the manipulator, or by an active compliance implemented in the control servo loop. The second method, called force control, is the subject of this report. In particular, this report presents a theory of force control based on formal models of the manipulator, and the task geometry. The ideal effector is used to model the manipulator, and the task geometry is modeled by the ideal surface, which is the locus of all positions accessible to the ideal effector. Models are also defined for the goal trajectory, position control, and force control.},
 month        = {apr~6},
 number       = {AITR-515},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-515.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-515.pdf},
}

@techreport{AITR-529,
 author       = {de Kleer, Johan},
 title        = {Causal and Teleological Reasoning in Circuit Recognition},
 year         = {1979},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents a theory of human-like reasoning in the general domain of designed physical systems, and in particular, electronic circuits. One aspect of the theory, causal analysis, describes how the behavior of individual components can be combined to explain the behavior of composite systems. Another aspect of the theory, teleological analysis, describes how the notion that the system has a purpose can be used to aid this causal analysis. The theory is implemented as a computer program, which, given a circuit topology, can construct by qualitative causal analysis a mechanism graph describing the functional topology of the system. This functional topology is then parsed by a grammar for common circuit functions. Ambiguities are introduced into the analysis by the approximate qualitative nature of the analysis. For example, there are often several possible mechanisms which might describe the circuit's function. These are disambiguated by teleological analysis. The requirement that each component be assigned an appropriate purpose in the functional topology imposes a severe constraint which eliminates all the ambiguities. Since both analyses are based on heuristics, the chosen mechanism is a rationalization of how the circuit functions, and does not guarantee that the circuit actually does function. This type of coarse understanding of circuits is useful for analysis, design and troubleshooting.},
 month        = {sep~6},
 number       = {AITR-529},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-529.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-529.pdf},
}

@techreport{AITR-534,
 author       = {Hollerbach, John},
 title        = {Theory of Handwriting},
 year         = {1980},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Handwriting production is viewed as a constrained modulation of an underlying oscillatory process. Coupled oscillations in horizontal and vertical directions produce letter forms, and when superimposed on a rightward constant velocity horizontal sweep result in spatially separated letters. Modulation of the vertical oscillation is responsible for control of letter height, either through altering the frequency or altering the acceleration amplitude. Modulation of the horizontal oscillation is responsible for control of corner shape through altering phase or amplitude. The vertical velocity zero crossing in the velocity space diagram is important from the standpoint of control. Changing the horizontal velocity value at this zero crossing controls corner shape, and such changes can be effected through modifying the horizontal oscillation amplitude and phase. Changing the slope at this zero crossing controls writing slant; this slope depends on the horizontal and vertical velocity zero amplitudes and on the relative phase difference. Letter height modulation is also best applied at the vertical velocity zero crossing to preserve an even baseline. The corner shape and slant constraints completely determine the amplitude and phase relations between the two oscillations. Under these constraints interletter separation is not an independent parameter. This theory applies generally to a number of acceleration oscillation patterns such as sinusoidal, rectangular and trapezoidal oscillations. The oscillation theory also provides an explanation for how handwriting might degenerate with speed. An implementation of the theory in the context of the spring muscle model is developed. Here sinusoidal oscillations arise from a purely mechanical sources; orthogonal antagonistic spring pairs generate particular cycloids depending on the initial conditions. Modulating between cycloids can be achieved by changing the spring zero settings at the appropriate times. Frequency can be modulated either by shifting between coactivation and alternating activation of the antagonistic springs or by presuming variable spring constant springs. An acceleration and position measuring apparatus was developed for measurements of human handwriting. Measurements of human writing are consistent with the oscillation theory. It is shown that the minimum energy movement for the spring muscle is bang-coast-bang. For certain parameter values a singular arc solution can be shown to be minimizing. Experimental measurements however indicate that handwriting is not a minimum energy movement.},
 month        = {mar~6},
 number       = {AITR-534},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-534.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-534.pdf},
}

@techreport{AITR-537,
 author       = {Sidner, Candace Lee},
 title        = {Towards a Computational Theory of Definite Anaphora Comprehension in English Discourse},
 year         = {1979},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report investigates the process of focussing as a description and explanation of the comprehension of certain anaphoric expressions in English discourse. The investigation centers on the interpretation of definite anaphora, that is, on the personal pronouns, and noun phrases used with a definite article the, this or that. Focussing is formalized as a process in which a speaker centers attention on a particular aspect of the discourse. An algorithmic description specifies what the speaker can focus on and how the speaker may change the focus of the discourse as the discourse unfolds. The algorithm allows for a simple focussing mechanism to be constructed: and element in focus, an ordered collection of alternate foci, and a stack of old foci. The data structure for the element in focus is a representation which encodes a limted set of associations between it and other elements from teh discourse as well as from general knowledge.},
 month        = {jun~6},
 number       = {AITR-537},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-537.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-537.pdf},
}

@techreport{AITR-540,
 author       = {Kahn, Kenneth Michael},
 title        = {Creation of Computer Animation from Story Descriptions},
 year         = {1979},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report describes a computer system that creates simple computer animation in response to high-level, vague, and incomplete descriptions of films. It makes its films by collecting and evaluating suggestions from several different bodies of knowledge. The order in which it makes its choices is influenced by the focus of the film. Difficult choices are postponed to be resumed when more of the film has been determined. The system was implemented in an object- oriented language based upon computational entities called ''actors''. The goal behind the construction of the system is that, whenever faced with a choice, it should sensibly choose between alternatives based upon the description of the film and as much general knowledge as possible. The system is presented as a computational model of creativity and aesthetics.},
 month        = {aug~6},
 number       = {AITR-540},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-540.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-540.pdf},
}

@techreport{AITR-542,
 author       = {Steels, Luc},
 title        = {Reasoning Modeled as a Society of Communicating Experts},
 year         = {1979},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report describes a domain independent reasoning system. The system uses a frame- based knowledge representation language and various reasoning techniques including constraint propagation, progressive refinement, natural deduction and explicit control of reasoning. A computational architecture based on active objects which operate by exchanging messages is developed and it is shown how this architecture supports reasoning activity. The user interacts with the system by specifying frames and by giving descriptions defining the problem situation. The system uses its reasoning capacity to build up a model of the problem situation from which a solution can interactively be extracted. Examples are discussed from a variety of domains, including electronic circuits, mechanical devices and music. The main thesis is that a reasoning system is best viewed as a parallel system whose control and data are distributed over a large network of processors that interact by exchanging messages. Such a system will be metaphorically described as a society of communicating experts.},
 month        = {jun~6},
 number       = {AITR-542},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-542.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-542.pdf},
}

@techreport{AITR-550,
 author       = {McAllester, David Allen},
 title        = {The Use of Equality in Deduction and Knowledge Representation},
 year         = {1980},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report describes a system which maintains canonical expressions for designators under a set of equalities. Substitution is used to maintain all knowledge in terms of these canonical expressions. A partial order on designators, termed the better-name relation, is used in the choice of canonical expressions. It is shown that with an appropriate better-name relation an important engineering reasoning technique, propagation of constraints, can be implemented as a special case of this substitution process. Special purpose algebraic simplification procedures are embedded such that they interact effectively with the equality system. An electrical circuit analysis system is developed which relies upon constraint propagation and algebraic simplification as primary reasoning techniques. The reasoning is guided by a better-name relation in which referentially transparent terms are preferred to referentially opaque ones. Multiple description of subcircuits are shown to interact strongly with the reasoning mechanism.},
 month        = {jan~6},
 number       = {AITR-550},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-550.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-550.pdf},
}

@techreport{AITR-579,
 author       = {Hildreth, Ellen C.},
 title        = {Implementation of a Theory of Edge Detection},
 year         = {1980},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report describes the implementation of a theory of edge detection, proposed by Marr and Hildreth (1979). According to this theory, the image is first processed independently through a set of different size filters, whose shape is the Laplacian of a Gaussian, ***. Zero-crossings in the output of these filters mark the positions of intensity changes at different resolutions. Information about these zero-crossings is then used for deriving a full symbolic description of changes in intensity in the image, called the raw primal sketch. The theory is closely tied with early processing in the human visual systems. In this report, we first examine the critical properties of the initial filters used in the edge detection process, both from a theoretical and practical standpoint. The implementation is then used as a test bed for exploring aspects of the human visual system; in particular, acuity and hyperacuity. Finally, we present some preliminary results concerning the relationship between zero-crossings detected at different resolutions, and some observations relevant to the process by which the human visual system integrates descriptions of intensity changes obtained at different resolutions.},
 month        = {apr~6},
 number       = {AITR-579},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-579.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-579.pdf},
}

@techreport{AITR-581,
 author       = {Doyle, Jon},
 title        = {A Model for Deliberation, Action, and Introspection},
 year         = {1980},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis investigates the problem of controlling or directing the reasoning and actions of a computer program. The basic approach explored is to view reasoning as a species of action, so that a program might apply its reasoning powers to the task of deciding what inferences to make as well as deciding what other actions to take. A design for the architecture of reasoning programs is proposed. This architecture involves self- consciousness, intentional actions, deliberate adaptations, and a form of decision-making based on dialectical argumentation. A program based on this architecture inspects itself, describes aspects of itself, and uses this self-reference and these self-descriptions in making decisions and taking actions. The program`s mental life includes awareness of its own concepts, beliefs, desires, intentions, inferences, actions, and skills. All of these are represented by self-descriptions in a single sort of language, so that the program has access to all of these aspects of itself, and can reason about them in the same terms.},
 month        = {may~6},
 number       = {AITR-581},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-581.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-581.pdf},
}

@techreport{AITR-589,
 author       = {Witkin, Andrew P.},
 title        = {Shape from Contour},
 year         = {1980},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The problem of using image contours to infer the shapes and orientations of surfaces is treated as a problem of statistical estimation. The basis for solving this problem lies in an understanding of the geometry of contour formation, coupled with simple statistical models of the contour generating process. This approach is first applied to the special case of surfaces known to be planar. The distortion of contour shape imposed by projection is treated as a signal to be estimated, and variations of non-projective origin are treated as noise. The resulting method is then extended to the estimation of curved surfaces, and applied successfully to natural images. Next, the geometric treatment is further extended by relating countour curvature to surface curvature, using cast shadows as a model for contour generation. This geometric relation, combined with a statistical model, provides a measure of goodness-of-fit between a surface and an image contour. The goodness-of-fit measure is applied to the problem of establishing registration between an image and a surface model. Finally, the statistical estimation strategy is experimentally compared to human perception of orientation: human observers' judgements of tilt correspond closely to the estimates produced by the planar strategy.},
 month        = {nov~6},
 number       = {AITR-589},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-589.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-589.pdf},
}

@techreport{AITR-595,
 author       = {Jr., Guy Lewis Steele},
 title        = {The Definition and Implementation of a Computer Programming Language Based on Constraints},
 year         = {1980},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The constraint paradigm is a model of computation in which values are deduced whenever possible, under the limitation that deductions be local in a certain sense. One may visualize a constraint 'program' as a network of devices connected by wires. Data values may flow along the wires, and computation is performed by the devices. A device computes using only locally available information (with a few exceptions), and places newly derived values on other, locally attached wires. In this way computed values are propagated. An advantage of the constraint paradigm (not unique to it) is that a single relationship can be used in more than one direction. The connections to a device are not labelled as inputs and outputs; a device will compute with whatever values are available, and produce as many new values as it can. General theorem provers are capable of such behavior, but tend to suffer from combinatorial explosion; it is not usually useful to derive all the possible consequences of a set of hypotheses. The constraint paradigm places a certain kind of limitation on the deduction process. The limitations imposed by the constraint paradigm are not the only one possible. It is argued, however, that they are restrictive enough to forestall combinatorial explosion in many interesting computational situations, yet permissive enough to allow useful computations in practical situations. Moreover, the paradigm is intuitive: It is easy to visualize the computational effects of these particular limitations, and the paradigm is a natural way of expressing programs for certain applications, in particular relationships arising in computer-aided design. A number of implementations of constraint-based programming languages are presented. A progression of ever more powerful languages is described, complete implementations are presented and design difficulties and alternatives are discussed. The goal approached, though not quite reached, is a complete programming system which will implicitly support the constraint paradigm to the same extent that LISP, say, supports automatic storage management.},
 month        = {aug~6},
 number       = {AITR-595},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-595.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-595.pdf},
}

@techreport{AITR-604,
 author       = {Rich, Charles},
 title        = {Inspection Methods in Programming},
 year         = {1981},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The work reported here lies in the area of overlap between artificial intelligence software engineering. As research in artificial intelligence, it is a step towards a model of problem solving in the domain of programming. In particular, this work focuses on the routine aspects of programming which involve the application of previous experience with similar programs. I call this programming by inspection. Programming is viewed here as a kind of engineering activity. Analysis and synthesis by inspection area prominent part of expert problem solving in many other engineering disciplines, such as electrical and mechanical engineering. The notion of inspections methods in programming developed in this work is motivated by similar notions in other areas of engineering. This work is also motivated by current practical concerns in the area of software engineering. The inadequacy of current programming technology is universally recognized. Part of the solution to this problem will be to increase the level of automation in programming. I believe that the next major step in the evolution of more automated programming will be interactive systems which provide a mixture of partially automated program analysis, synthesis and verification. One such system being developed at MIT, called the programmer`s apprentice, is the immediate intended application of this work. This report concentrates on the knowledge are of the programmer`s apprentice, which is the form of a taxonomy of commonly used algorithms and data structures. To the extent that a programmer is able to construct and manipulate programs in terms of the forms in such a taxonomy, he may relieve himself of many details and generally raise the conceptual level of his interaction with the system, as compared with present day programming environments. Also, since it is practical to expand a great deal of effort pre- analyzing the entries in a library, the difficulty of verifying the correctness of programs constructed this way is correspondingly reduced. The feasibility of this approach is demonstrated by the design of an initial library of common techniques for manipulating symbolic data. This document also reports on the further development of a formalism called the plan calculus for specifying computations in a programming language independent manner. This formalism combines both data and control abstraction in a uniform framework that has facilities for representing multiple points of view and side effects.},
 month        = {jun~6},
 number       = {AITR-604},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-604.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-604.pdf},
}

@techreport{AITR-610,
 author       = {Brown, Richard},
 title        = {Coherent Behavior from Incoherent Knowledge Sources in the Automatic Synthesis of Numerical Computer Programs},
 year         = {1981},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A fundamental problem in artificial intelligence is obtaining coherent behavior in rule-based problem solving systems. A good quantitative measure of coherence is time behavior; a system that never, in retrospect, applied a rule needlessly is certainly coherent; a system suffering from combinatorial blowup is certainly behaving incoherently. This report describes a rule-based problem solving system for automatically writing and improving numerical computer programs from specifications. The specifications are in terms of ''constraints'' among inputs and outputs. The system has solved program synthesis problems involving systems of equations, determining that methods of successive approximation converge, transforming recursion to iteration, and manipulating power series (using differing organizations, control structures, and argument-passing techniques).},
 month        = {jan~6},
 number       = {AITR-610},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-610.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-610.pdf},
}

@techreport{AITR-615,
 author       = {Forbus, Kenneth D.},
 title        = {A Study of Qualitative and Geometric Knowledge in Reasoning about Motion},
 year         = {1981},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Reasoning about motion is an important part of our commonsense knowledge, involving fluent spatial reasoning. This work studies the qualitative and geometric knowledge required to reason in a world that consists of balls moving through space constrained by collisions with surfaces, including dissipative forces and multiple moving objects. An analog geometry representation serves the program as a diagram, allowing many spatial questions to be answered by numeric calculation. It also provides the foundation for the construction and use of place vocabulary, the symbolic descriptions of space required to do qualitative reasoning about motion in the domain. The actual motion of a ball is described as a network consisting of descriptions of qualitatively distinct types of motion. Implementing the elements of these networks in a constraint language allows the same elements to be used for both analysis and simulation of motion. A qualitative description of the actual motion is also used to check the consistency of assumptions about motion. A process of qualitative simulation is used to describe the kinds of motion possible from some state. The ambiguity inherent in such a description can be reduced by assumptions about physical properties of the ball or assumptions about its motion. Each assumption directly rules out some kinds of motion, but other knowledge is required to determine the indirect consequences of making these assumptions. Some of this knowledge is domain dependent and relies heavily on spatial descriptions.},
 month        = {feb~6},
 number       = {AITR-615},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-615.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-615.pdf},
}

@techreport{AITR-619,
 author       = {White, Barbara Y.},
 title        = {Designing Computer Games to Facilitate Learning},
 year         = {1981},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The aim of this thesis was to explore the design of interactive computer learning environments. The particular learning domain selected was Newtonian dynamics. Newtonian dynamics was chosen because it is an important area of physics with which many students have difficulty and because controlling Newtonian motion takes advantage of the computer`s graphics and interactive capabilities. The learning environment involved games which simulated the motion of a spaceship on a display screen. The purpose of the games was to focus the students` attention on various aspects of the implications of Newton`s laws.},
 month        = {feb~6},
 number       = {AITR-619},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-619.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-619.pdf},
}

@techreport{AITR-623,
 author       = {Bruss, Anna R.},
 title        = {The Image Irradiance Equation},
 subtitle     = {Its Solution and Application},
 year         = {1981},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {How much information about the shape of an object can be inferred from its image? In particular, can the shape of an object be reconstructed by measuring the light it reflects from points on its surface? These questions were raised by Horn [HO70] who formulated a set of conditions such that the image formation can be described in terms of a first order partial differential equation, the image irradiance equation. In general, an image irradiance equation has infinitely many solutions. Thus constraints necessary to find a unique solution need to be identified. First we study the continuous image irradiance equation. It is demonstrated when and how the knowledge of the position of edges on a surface can be used to reconstruct the surface. Furthermore we show how much about the shape of a surface can be deduced from so called singular points. At these points the surface orientation is uniquely determined by the measured brightness. Then we investigate images in which certain types of silhouettes, which we call b-silhouettes, can be detected. In particular we answer the following question in the affirmative: Is there a set of constraints which assure that if an image irradiance equation has a solution, it is unique? To this end we postulate three constraints upon the image irradiance equation and prove that they are sufficient to uniquely reconstruct the surface from its image. Furthermore it is shown that any two of these constraints are insufficient to assure a unique solution to an image irradiance equation. Examples are given which illustrate the different issues. Finally, an overview of known numerical methods for computing solutions to an image irradiance equation are presented.},
 month        = {jun~6},
 number       = {AITR-623},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-623.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-623.pdf},
}

@techreport{AITR-633,
 author       = {Clinger, William Douglas},
 title        = {Foundations of Actor Semantics},
 year         = {1981},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The actor message-passing model of concurrent computation has inspired new ideas in the areas of knowledge-based systems, programming languages and their semantics, and computer systems architecture. The model itself grew out of computer languages such as Planner, Smalltalk, and Simula, and out of the use of continuations to interpret imperative constructs within A-calculus. The mathematical content of the model has been developed by Carl Hewitt, Irene Greif, Henry Baker, and Giuseppe Attardi. This thesis extends and unifies their work through the following observations. The ordering laws postulated by Hewitt and Baker can be proved using a notion of global time. The most general ordering laws are in fact equivalent to an axiom of realizability in global time. Independence results suggest that some notion of global time is essential to any model of concurrent computation. Since nondeterministic concurrency is more fundamental than deterministic sequential computation, there may be no need to take fixed points in the underlying domain of a power domain. Power domains built from incomplete domains can solve the problem of providing a fixed point semantics for a class of nondeterministic programming languages in which a fair merge can be written. The event diagrams of Greif's behavioral semantics, augmented by Baker's pending events, form an incomplete domain. Its power domain is the semantic domain in which programs written in actor-based languages are assigned meanings. This denotational semantics is compatible with behavioral semantics. The locality laws postulated by Hewitt and Baker may be proved for the semantics of an actor-based language. Altering the semantics slightly can falsify the locality laws. The locality laws thus constrain what counts as an actor semantics.},
 month        = {may~6},
 number       = {AITR-633},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-633.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-633.pdf},
}

@techreport{AITR-636,
 author       = {Steele, Barbara Sue Kerne},
 title        = {An Accountable Source-To-Source Transformation System},
 year         = {1981},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Though one is led to believe that program transformation systems which perform source-to-source transformations enable the user to understand and appreciate the resulting source program, this is not always the case. Transformations are capable of behaving and/or interacting in unexpected ways. The user who is interested in understanding the whats, whys, wheres, and hows of the transformation process is left without tools for discovering them. I provide an initial step towards the solution of this problem in the form of an accountable source- to-source transformation system. It carefully records the information necessary to answer such questions, and provides mechanisms for the retrieval of this information. It is observed that though this accountable system allows the user access to relevant facts from which he may draw conclusions, further study is necessary to make the system capable of analyzing these facts itself.},
 month        = {jun~6},
 number       = {AITR-636},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-636.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-636.pdf},
}

@techreport{AITR-649,
 author       = {Riley, Michael Dennis},
 title        = {The Representation of Image Texture},
 year         = {1981},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis explores how to represent image texture in order to obtain information about the geometry and structure of surfaces, with particular emphasis on locating surface discontinuities. Theoretical and psychophysical results lead to the following conclusions for the representation of image texture: (1) A texture edge primitive is needed to identify texture change contours, which are formed by an abrupt change in the 2-D organization of similar items in an image. The texture edge can be used for locating discontinuities in surface structure and surface geometry and for establishing motion correspondence. (2) Abrupt changes in attributes that vary with changing surface geometry {\textendash} orientation, density, length, and width {\textendash} should be used to identify discontinuities in surface geometry and surface structure. (3) Texture tokens are needed to separate the effects of different physical processes operating on a surface. They represent the local structure of the image texture. Their spatial variation can be used in the detection of texture discontinuities and texture gradients, and their temporal variation may be used for establishing motion correspondence. What precisely constitutes the texture tokens is unknown; it appears, however, that the intensity changes alone will not suffice, but local groupings of them may. (4) The above primitives need to be assigned rapidly over a large range in an image.},
 month        = {sep~6},
 number       = {AITR-649},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-649.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-649.pdf},
}

@techreport{AITR-688,
 author       = {Sjoberg, Robert W.},
 title        = {Atmospheric Effects in Satellite Imaging of Mountainous Terrain},
 year         = {1982},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {\ensuremath{<}hr\ensuremath{>}},
 month        = {sep~6},
 number       = {AITR-688},
 url          = {http://ncstrl.mit.edu;},
}

@techreport{AITR-690,
 author       = {Mason, Matthew Thomas},
 title        = {Manipulator Grasping and Pushing Operations},
 year         = {1982},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The primary goal of this research is to develop theoretical tools for analysis, synthesis, application of primitive manipulator operations. The primary method is to extend and apply traditional tools of classical mechanics. The results are of such a general nature that they address many different aspects of industrial robotics, including effector and sensor design, planning and programming tools and design of auxiliary equipment. Some of the manipulator operations studied are: (1) Grasping an object. The object will usually slide and rotate during the period between first contact and prehension. (2) Placing an object. The object may slip slightly in the fingers upon contact with the table as the base aligns with the table. (3) Pushing. Often the final stage of mating two parts involves pushing one object into the other.},
 month        = {jun~6},
 number       = {AITR-690},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-690.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-690.pdf},
}

@techreport{AITR-703,
 author       = {Roylance, Gerald},
 title        = {A Simple Model of Circuit Design},
 year         = {1980},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A simple analog circuit designer has been implemented as a rule based system. The system can design voltage followers. Miller integrators, and bootstrap ramp generators from functional descriptions of what these circuits do. While the designer works in a simple domain where all components are ideal, it demonstrates the abilities of skilled designers. While the domain is electronics, the design ideas are useful in many other engineering domains, such as mechanical engineering, chemical engineering, and numerical programming. Most circuit design systems are given the circuit schematic and use arithmetic constraints to select component values. This circuit designer is different because it designs the schematic. The designer uses a unidirectional CONTROL relation to find the schematic. The circuit designs are built around this relation; it restricts the search space, assigns purposes to components and finds design bugs.},
 month        = {may~6},
 number       = {AITR-703},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-703.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-703.pdf},
}

@techreport{AITR-704,
 author       = {Brotsky, Daniel Carl},
 title        = {An Algorithm for Parsing Flow Graphs},
 year         = {1984},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This report describes research about flow graphs - labeled, directed, acyclic graphs which abstract representations used in a variety of Artificial Intelligence applications. Flow graphs may be derived from flow grammars much as strings may be derived from string grammars; this derivation process forms a useful model for the stepwise refinement processes used in programming and other engineering domains. The central result of this report is a parsing algorithm for flow graphs. Given a flow grammar and a flow graph, the algorithm determines whether the grammar generates the graph and, if so, finds all possible derivations for it. The author has implemented the algorithm in LISP. The intent of this report is to make flow-graph parsing available as an analytic tool for researchers in Artificial Intelligence. The report explores the intuitions behind the parsing algorithm, contains numerous, extensive examples of its behavior, and provides some guidance for those who wish to customize the algorithm to their own uses.},
 month        = {mar~6},
 number       = {AITR-704},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-704.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-704.pdf},
}

@techreport{AITR-707,
 author       = {Hamscher, Walter},
 title        = {Using Structural and Functional Information in Diagnostic Design},
 year         = {1983},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We wish to design a diagnostic for a device from knowledge of its structure and function. the diagnostic should achieve both coverage of the faults that can occur in the device, and should strive to achieve specificity in its diagnosis when it detects a fault. A system is described that uses a simple model of hardware structure and function, representing the device in terms of its internal primitive functions and connections. The system designs a diagnostic in three steps. First, an extension of path sensitization is used to design a test for each of the connections in teh device. Next, the resulting tests are improved by increasing their specificity. Finally the tests are ordered so that each relies on the fewest possible connections. We describe an implementation of this system and show examples of the results for some simple devices.},
 month        = {jun~6},
 number       = {AITR-707},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-707.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-707.pdf},
}

@techreport{AITR-715,
 author       = {Barton, Jr. George Edward},
 title        = {A Multiple-Context Equality-Based Reasoning System},
 year         = {1983},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Expert systems are too slow. This work attacks that problem by speeding up a useful system component that remembers facts and tracks down simple consequences. The redesigned component can assimilate new facts more quickly because it uses a compact, grammar-based internal representation to deal with whole classes of equivalent expressions at once. It can support faster hypothetical reasoning because it remembers the consequences of several assumption sets at once. The new design is targeted for situations in which many of the stored facts are equalities. The deductive machinery considered here supplements stored premises with simple new conclusions. The stored premises include permanently asserted facts and temporarily adopted assumptions. The new conclusions are derived by substituting equals for equals and using the properties of the logical connectives AND, Or, and NOT. The deductive system provides supporting premises for its derived conclusions. Reasoning that involves quantifiers is beyond the scope of its limited and automatic operation. The expert system of which the reasoning system is a component is expected to be responsible for overall control of reasoning.},
 month        = {apr~6},
 number       = {AITR-715},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-715.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-715.pdf},
}

@techreport{AITR-720,
 author       = {Canny, John Francis},
 title        = {Finding Edges and Lines in Images},
 year         = {1983},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The problem of detecting intensity changes in images is canonical in vision. Edge detection operators are typically designed to optimally estimate first or second derivative over some (usually small) support. Other criteria such as output signal to noise ratio or bandwidth have also been argued for. This thesis is an attempt to formulate a set of edge detection criteria that capture as directly as possible the desirable properties of an edge operator. Variational techniques are used to find a solution over the space of all linear shift invariant operators. The first criterion is that the detector have low probability of error i.e. failing to mark edges or falsely marking non- edges. The second is that the marked points should be as close as possible to the centre of the true edge. The third criterion is that there should be low probability of more than one response to a single edge. The technique is used to find optimal operators for step edges and for extended impulse profiles (ridges or valleys in two dimensions). The extension of the one dimensional operators to two dimentions is then discussed. The result is a set of operators of varying width, length and orientation. The problem of combining these outputs into a single description is discussed, and a set of heuristics for the integration are given.},
 month        = {jun~6},
 number       = {AITR-720},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-720.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-720.pdf},
}

@techreport{AITR-728,
 author       = {Theriault, Daniel G.},
 title        = {Issues in the Design and Implementation of Act 2},
 year         = {1983},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Act2 is a highly concurrent programming language designed to exploit the processing power available from parallel computer architectures. The language supports advanced concepts in software engineering, providing high-level constructs suitable for implementing artificially-intelligent applications. Act2 is based on the Actor model of computation, consisting of virtual computational agents which communicate by message-passing. Act2 serves as a framework in which to integrate an actor language, a description and reasoning system, and a problem-solving and resource management system. This document describes issues in Act2`s design and the implementation of an interpreter for the language.},
 month        = {jun~6},
 number       = {AITR-728},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-728.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-728.pdf},
}

@techreport{AITR-735,
 author       = {Abelson, Harold and Sussman, Gerald Jay},
 title        = {Structure and Interpretation of Computer Programs},
 year         = {1983},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {''The Structure and Interpretation of Computer Programs'' is the entry-level subject in Computer Science at the Massachusetts Institute of Technology. It is required of all students at MIT who major in Electrical Engineering or in Computer Science, as one fourth of the ''common core curriculum,'' which also includes two subjects on circuits and linear systems and a subject on the design of digital systems. We have been involved in the development of this subject since 1978, and we have taught this material in its present form since the fall of 1980 to approximately 600 students each year. Most of these students have had little or no prior formal training in computation, although most have played with computers a bit and a few have had extensive programming or hardware design experience. Our design of this introductory Computer Science subject reflects two major concerns. First we want to establish the idea that a computer language is not just a way of getting a computer to perform operations, but rather that it is a novel formal medium for expressing ideas about methodology. Thus, programs must be written for people to read, and only incidentally for machines to execute. Secondly, we believe that the essential material to be addressed by a subject at this level, is not the syntax of particular programming language constructs, nor clever algorithms for computing particular functions of efficiently, not even the mathematical analysis of algorithms and the foundations of computing, but rather the techniques used to control the intellectual complexity of large software systems.},
 month        = {jul~6},
 number       = {AITR-735},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-735.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-735.pdf},
}

@techreport{AITR-749,
 author       = {Simmons, Reid Gordon},
 title        = {Representing and Reasoning About Change in Geologic Interpretation},
 year         = {1983},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Geologic interpretation is the task of inferring a sequence of events to explain how a given geologic region could have been formed. This report describes the design and implementation of one part of a geologic interpretation problem solver {\textendash} a system which uses a simulation technique called imagining to check the validity of a candidate sequence of events. Imagining uses a combination of qualitative and quantitative simulations to reason about the changes which occured to the geologic region. The spatial changes which occur are simulated by constructing a sequence of diagrams. The quantitative simulation needs numeric parameters which are determined by using the qualitative simulation to establish the cumulative changes to an object and by using a description of the current geologic region to make quantitative measurements. The diversity of reasoning skills used in imagining has necessitated the development of multiple representations, each specialized for a different task. Representations to facilitate doing temporal, spatial and numeric reasoning are described in detail. We have also found it useful to explicitly represent processes. Both the qualitative and quantitative simulations use a discrete 'layer cake' model of geologic processes, but each uses a separate representation, specialized to support the type of simulation. These multiple representations have enabled us to develop a powerful, yet modular, system for reasoning about change.},
 month        = {dec~6},
 number       = {AITR-749},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-749.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-749.pdf},
}

@techreport{AITR-753,
 author       = {Waters, Richard C.},
 title        = {KBEmacs},
 subtitle     = {A Step Toward the Programmer's Apprentice},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The Knowledge-Based Editor in Emacs (KBEmacs) is the current demonstration system implemented as part of the Programmer`s Apprentice project. KBEmacs is capable of acting as a semi-expert assistant to a person who is writing a program {\textendash} taking over some parts of the programming task. Using KBEmacs, it is possible to construct a program by issuing a series of high level commands. This series of commands can be as much as an order of magnitude shorter than the program is describes. KBEmacs is capable of operating on Ada and Lisp programs of realistic size and complexity. Although KBEmacs is neither fast enough nor robust enough to be considered a true prototype, both of these problems could be overcome if the system were to be reimplemented.},
 month        = {may~6},
 number       = {AITR-753},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-753.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-753.pdf},
}

@techreport{AITR-754,
 author       = {Lathrop, Richard D.},
 title        = {Parallelism in Manipulator Dynamics},
 year         = {1984},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This paper addresses the problem of efficiently computing the motor torques required to drive a lower-pair kinematic chain (e.g., a typical manipulator arm in free motion, or a mechanical leg in the swing phase) given the desired trajectory; i.e., the Inverse Dynamics problem. It investigates the high degree of parallelism inherent in the computations, and presents two ''mathematically exact'' formulations especially suited to high-speed, highly parallel implementations using special-purpose hardware or VLSI devices. In principle, the formulations should permit the calculations to run at a speed bounded only by I/O. The first presented is a parallel version of the recent linear Newton-Euler recursive algorithm. The time cost is also linear in the number of joints, but the real-time coefficients are reduced by almost two orders of magnitude. The second formulation reports a new parallel algorithm which shows that it is possible to improve upon the linear time dependency. The real time required to perform the calculations increases only as the [log2] of the number of joints. Either formulation is susceptible to a systolic pipelined architecture in which complete sets of joint torques emerge at successive intervals of four floating-point operations. Hardware requirements necessary to support the algorithm are considered and found not to be excessive, and a VLSI implementation architecture is suggested. We indicate possible applications to incorporating dynamical considerations into trajectory planning, e.g. it may be possible to build an on-line trajectory optimizer.},
 month        = {dec~6},
 number       = {AITR-754},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-754.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-754.pdf},
}

@techreport{AITR-767,
 author       = {Williams, Brian C.},
 title        = {Qualitative Analysis of MOS Circuits},
 year         = {1984},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {With the push towards sub-micron technology, transistor models have become increasingly complex. The number of components in integrated circuits has forced designer`s efforts and skills towards higher levels of design. This has created a gap between design expertise and the performance demands increasingly imposed by the technology. To alleviate this problem, software tools must be developed that provide the designer with expert advice on circuit performance and design. This requires a theory that links the intuitions of an expert circuit analyst with the corresponding principles of formal theory (i.e. algebra, calculus, feedback analysis, network theory, and electrodynamics), and that makes each underlying assumption explicit.},
 month        = {jul~6},
 number       = {AITR-767},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-767.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-767.pdf},
}

@techreport{AITR-789,
 author       = {Forbus, Kenneth D.},
 title        = {Qualitative Process Theory},
 year         = {1984},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Objects move, collide, flow, bend, heat up, cool down, stretch, compress and boil. These and other things that cause changes in objects over time are intuitively characterized as processes. To understand common sense physical reasoning and make programs that interact with the physical world as well as people do we must understand qualitative reasoning about processes, when they will occur, their effects, and when they will stop. Qualitative Process theory defines a simple notion of physical process that appears useful as a language in which to write dynamical theories. Reasoning about processes also motivates a new qualitative representation for quantity in terms of inequalities, called quantity space. This report describes the basic concepts of Qualitative Process theory, several different kinds of reasoning that can be performed with them, and discusses its impact on other issues in common sense reasoning about the physical world, such as causal reasoning and measurement interpretation. Several extended examples illustrate the utility of the theory, including figuring out that a boiler can blow up, that an oscillator with friction will eventually stop, and how to say that you can pull with a string but not push with it. This report also describes GIZMO, an implemented computer program which uses Qualitative Process theory to make predictions and interpret simple measurements. The represnetations and algorithms used in GIZMO are described in detail, and illustrated using several examples.},
 month        = {jul~6},
 number       = {AITR-789},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-789.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-789.pdf},
}

@techreport{AITR-791,
 author       = {Donald, Bruce R.},
 title        = {Motion Planning with Six Degrees of Freedom},
 year         = {1984},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The motion planning problem is of central importance to the fields of robotics, spatial planning, and automated design. In robotics we are interested in the automatic synthesis of robot motions, given high-level specifications of tasks and geometric models of the robot and obstacles. The Mover`s problem is to find a continuous, collision-free path for a moving object through an environment containing obstacles. We present an implemented algorithm for the classical formulation of the three-dimensional Mover`s problem: given an arbitrary rigid polyhedral moving object P with three translational and three rotational degrees of freedom, find a continuous, collision-free path taking P from some initial configuration to a desired goal configuration. This thesis describes the first known implementation of a complete algorithm (at a given resolution) for the full six degree of freedom Movers` problem. The algorithm transforms the six degree of freedom planning problem into a point navigation problem in a six-dimensional configuration space (called C-Space). The C-Space obstacles, which characterize the physically unachievable configurations, are directly represented by six-dimensional manifolds whose boundaries are five dimensional C- surfaces. By characterizing these surfaces and their intersections, collision-free paths may be found by the closure of three operators which (i) slide along 5-dimensional intersections of level C-Space obstacles; (ii) slide along 1- to 4-dimensional intersections of level C-surfaces; and (iii) jump between 6 dimensional obstacles. Implementing the point navigation operators requires solving fundamental representational and algorithmic questions: we will derive new structural properties of the C-Space constraints and shoe how to construct and represent C-Surfaces and their intersection manifolds. A definition and new theoretical results are presented for a six- dimensional C-Space extension of the generalized Voronoi diagram, called the C- Voronoi diagram, whose structure we relate to the C-surface intersection manifolds. The representations and algorithms we develop impact many geometric planning problems, and extend to Cartesian manipulators with six degrees of freedom.},
 month        = {may~6},
 number       = {AITR-791},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-791.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-791.pdf},
}

@techreport{AITR-793,
 author       = {Weld, Daniel Sabey},
 title        = {Switching Between Discrete and Continuous Process Models to Predict Molecular Genetic Activity},
 year         = {1984},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Two kinds of process models have been used in programs that reason about change: Discrete and continuous models. We describe the design and implementation of a qualitative simulator, PEPTIDE, which uses both kinds of process models to predict the behavior of molecular energetic systems. The program uses a discrete process model to simulate both situations involving abrupt changes in quantities and the actions of small numbers of molecules. It uses a continuous process model to predict gradual changes in quantities. A novel technique, called aggregation, allows the simulator to switch between theses models through the recognition and summary of cycles. The flexibility of PEPTIDE`s aggregator allows the program to detect cycles within cycles and predict the behavior of complex situations.},
 month        = {may~6},
 number       = {AITR-793},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-793.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-793.pdf},
}

@techreport{AITR-794,
 author       = {Ciccarelli, IV Eugene C.},
 title        = {Presentation Based User Interface},
 year         = {1984},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A prototype presentation system base is described. It offers mechanisms, tools, and ready-made parts for building user interfaces. A general user interface model underlies the base, organized around the concept of a presentation: a visible text or graphic for conveying information. Te base and model emphasize domain independence and style independence, to apply to the widest possible range of interfaces. The primitive presentation system model treats the interface as a system of processes maintaining a semantic relation between an application data base and a presentation data base, the symbolic screen description containing presentations. A presenter continually updates the presentation data base from the application data base. The user manipulates presentations with a presentation editor. A recognizer translates the user`s presentation manipulation into application data base commands. The primitive presentation system can be extended to model more complex systems by attaching additional presentation systems. In order to illustrate the model`s generality and descriptive capabilities, extended model structures for several existing user interfaces are discussed. The base provides support for building the application and presentation data bases, linked together into a single, uniform network, including descriptions of classes of objects as we as the objects themselves. The base provides an initial presentation data base network graphics to continually display it, and editing functions. A variety of tools and mechanisms help create and control presenters and recognizers. To demonstrate the base`s utility, three interfaces to an operating system were constructed, embodying different styles: icons, menu, and graphical annotation.},
 month        = {aug~6},
 number       = {AITR-794},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-794.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-794.pdf},
}

@techreport{AITR-802,
 author       = {Chapman, David},
 title        = {Planning for Conjunctive Goals},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The problem of achieving conjunctive goals has been central to domain independent planning research; the nonlinear constraint- posting approach has been most successful. Previous planners of this type have been comlicated, heuristic, and ill-defined. I have combined and distilled the state of the art into a simple, precise, implemented algorithm (TWEAK) which I have proved correct and complete. I analyze previous work on domain- independent conjunctive planning; in retrospect it becomes clear that all conjunctive planners, linear and nonlinear, work the same way. The efficiency of these planners depends on the traditional add/delete-list representation for actions, which drastically limits their usefulness. I present theorems that suggest that efficient general purpose planning with more expressive action representations is impossible, and suggest ways to avoid this problem.},
 month        = {nov~6},
 number       = {AITR-802},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-802.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-802.pdf},
}

@techreport{AITR-807,
 author       = {Ressler, Andrew Lewis},
 title        = {A Circuit Grammar For Operational Amplifier Design},
 year         = {1984},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Electrical circuit designers seldom create really new topologies or use old ones in a novel way. Most designs are known combinations of common configurations tailored for the particular problem at hand. In this thesis I show that much of the behavior of a designer engaged in such ordinary design can be modelled by a clearly defined computational mechanism executing a set of stylized rules. Each of my rules embodies a particular piece of the designer`s knowledge. A circuit is represented as a hierarchy of abstract objects, each of which is composed of other objects. The leaves of this tree represent the physical devices from which physical circuits are fabricated. By analogy with context-free languages, a class of circuits is generated by a phrase-structure grammar of which each rule describes how one type of abstract object can be expanded into a combination of more concrete parts. Circuits are designed by first postulating an abstract object which meets the particular design requirements. This object is then expanded into a concrete circuit by successive refinement using rules of my grammar. There are in general many rules which can be used to expand a given abstract component. Analysis must be done at each level of the expansion to constrain the search to a reasonable set. Thus the rule of my circuit grammar provide constraints which allow the approximate qualitative analysis of partially instantiated circuits. Later, more careful analysis in terms of more concrete components may lead to the rejection of a line of expansion which at first looked promising. I provide special failure rules to direct the repair in this case.},
 month        = {jan~6},
 number       = {AITR-807},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-807.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-807.pdf},
}

@techreport{AITR-810,
 author       = {Erdmann, Michael Andreas},
 title        = {On Motion Planning with Uncertainty},
 year         = {1984},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Robots must successfully plan and execute tasks in the presence of uncertainty. Uncertainty arises from errors in modeling, sensing, and control. Planning in the presence of uncertainty constitutes one facet of the general motion planning problem in robotics. This problem is concerned with the automatic synthesis of motion strategies from high level task specification and geometric models of environments. In order to develop successful motion strategies, it is necessary to understand the effect of uncertainty on the geometry of object interactions. Object interactions, both static and dynamic, may be represented in geometrical terms. This thesis investigates geometrical tools for modeling and overcoming uncertainty. The thesis describes an algorithm for computing backprojections o desired task configurations. Task goals and motion states are specified in terms of a moving object`s configuration space. Backprojections specify regions in configuration space from which particular motions are guaranteed to accomplish a desired task. The backprojection algorithm considers surfaces in configuration space that facilitate sliding towards the goal, while avoiding surfaces on which motions may prematurely halt. In executing a motion for a backprojection region, a plan executor must be able to recognize that a desired task has been accomplished. Since sensors are subject to uncertainty, recognition of task success is not always possible. The thesis considers the structure of backprojection regions and of task goals that ensures goal recognizability. The thesis also develops a representation of friction in configuration space, in terms of a friction cone analogous to the real space friction cone. The friction cone provides the backprojection algorithm with a geometrical tool for determining points at which motions may halt.},
 month        = {aug~6},
 number       = {AITR-810},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-810.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-810.pdf},
}

@techreport{AITR-834,
 author       = {Andreae, Peter Merrett},
 title        = {Justified Generalization},
 subtitle     = {Acquiring Procedures from Examples},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis describes an implemented system called NODDY for acquiring procedures from examples presented by a teacher. Acquiring procedures form examples involves several different generalization tasks. Generalization is an underconstrained task, and the main issue of machine learning is how to deal with this underconstraint. The thesis presents two principles for constraining generalization on which NODDY is based. The first principle is to exploit domain based constraints. NODDY demonstrated how such constraints can be used both to reduce the space of possible generalizations to manageable size, and how to generate negative examples out of positive examples to further constrain the generalization. The second principle is to avoid spurious generalizations by requiring justification before adopting a generalization. NODDY demonstrates several different ways of justifying a generalization and proposes a way of ordering and searching a space of candidate generalizations based on how much evidence would be required to justify each generalization. Acquiring procedures also involves three types of constructive generalizations: inferring loops (a kind of group), inferring complex relations and state variables, and inferring predicates. NODDY demonstrates three constructive generalization methods for these kinds of generalization.},
 month        = {jan~6},
 number       = {AITR-834},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-834.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-834.pdf},
}

@techreport{AITR-843,
 author       = {Sterpe, Peter J.},
 title        = {TEMPEST},
 subtitle     = {A Template Editor for Structured Text},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {TEMPEST is a full-screen text editor that incorporates a structural paradigm in addition to the more traditional textual paradigm provided by most editors. While the textual paradigm treats the text as a sequence of characters, the structural paradigm treats it as a collection of named blocks which the user can define, group, and manipulate. Blocks can be defined to correspond to the structural features of he text, thereby providing more meaningful objects to operate on than characters of lines. The structural representation of the text is kept in the background, giving TEMPEST the appearance of a typical text editor. The structural and textual interfaces coexist equally, however, so one can always operate on the text from wither point of view. TEMPEST`s representation scheme provides no semantic understanding of structure. This approach sacrifices depth, but affords a broad range of applicability and requires very little computational overhead. A prototype has been implemented to illustrate the feasibility and potential areas of application of the central ideas. It was developed and runs on an IBM Personal Computer.},
 month        = {jun~6},
 number       = {AITR-843},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-843.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-843.pdf},
}

@techreport{AITR-844,
 author       = {Agha, Gul Abdulnabi},
 title        = {ACTORS},
 subtitle     = {A Model of Concurrent Computation in Distributed Systems},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A foundational model of concurrency is developed in this thesis. We examine issues in the design of parallel systems and show why the actor model is suitable for exploiting large-scale parallelism. Concurrency in actors is constrained only by the availability of hardware resources and by the logical dependence inherent in the computation. Unlike dataflow and functional programming, however, actors are dynamically reconfigurable and can model shared resources with changing local state. Concurrency is spawned in actors using asynchronous message-passing, pipelining, and the dynamic creation of actors. This thesis deals with some central issues in distributed computing. Specifically, problems of divergence and deadlock are addressed. For example, actors permit dynamic deadlock detection and removal. The problem of divergence is contained because independent transactions can execute concurrently and potentially infinite processes are nevertheless available for interaction.},
 month        = {jun~6},
 number       = {AITR-844},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-844.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-844.pdf},
}

@techreport{AITR-852,
 author       = {Fleck, Margaret Morrison},
 title        = {Local Rotational Symmetries},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis describes a new representation for two-dimensional round regions called Local Rotational Symmetries. Local Rotational Symmetries are intended as a companion to Brady`s Smoothed Local Symmetry Representation for elongated shapes. An algorithm for computing Local Rotational Symmetry representations at multiple scales of resolution has been implemented and results of this implementation are presented. These results suggest that Local Rotational Symmetries provide a more robustly computable and perceptually accurate description of round regions than previous proposed representations. In the course of developing this representation, it has been necessary to modify the way both Smoothed Local Symmetries and Local Rotational Symmetries are computed. First, grey-scale image smoothing proves to be better than boundary smoothing for creating representations at multiple scales of resolution, because it is more robust and it allows qualitative changes in representations between scales. Secondly, it is proposed that shape representations at different scales of resolution be explicitly related, so that information can be passed between scales and computation at each scale can be kept local. Such a model for multi-scale computation is desirable both to allow efficient computation and to accurately model human perceptions.},
 month        = {aug~6},
 number       = {AITR-852},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-852.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-852.pdf},
}

@techreport{AITR-853,
 author       = {Connell, Jonathan Hudson},
 title        = {Learning Shape Descriptions},
 subtitle     = {Generating and Generalizing Models of Visual Objects},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We present the results of an implemented system for learning structural prototypes from grey-scale images. We show how to divide an object into subparts and how to encode the properties of these subparts and the relations between them. We discuss the importance of hierarchy and grouping in representing objects and show how a notion of visual similarities can be embedded in the description language. Finally we exhibit a learning algorithm that forms class models from the descriptions produced and uses these models to recognize new members of the class.},
 month        = {sep~6},
 number       = {AITR-853},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-853.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-853.pdf},
}

@techreport{AITR-859,
 author       = {Flynn, Anita M.},
 title        = {Redundant Sensors for Mobile Robot Navigation},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Redundant sensors are needed on a mobile robot so that the accuracy with which it perceives its surroundings can be increased. Sonar and infrared sensors are used here in tandem, each compensating for deficiencies in the other. The robot combines the data from both sensors to build a representation which is more accurate than if either sensor were used alone. Another representation, the curvature primal sketch, is extracted from this perceived workspace and is used as the input to two path planning programs: one based on configuration space and one based on a generalized cone formulation of free space.},
 month        = {sep~6},
 number       = {AITR-859},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-859.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-859.pdf},
}

@techreport{AITR-860,
 author       = {Marroquin, Jose Luis},
 title        = {Probabilistic Solution of Inverse Problems},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In this thesis we study the general problem of reconstructing a function, defined on a finite lattice from a set of incomplete, noisy and/or ambiguous observations. The goal of this work is to demonstrate the generality and practical value of a probabilistic (in particular, Bayesian) approach to this problem, particularly in the context of Computer Vision. In this approach, the prior knowledge about the solution is expressed in the form of a Gibbsian probability distribution on the space of all possible functions, so that the reconstruction task is formulated as an estimation problem. Our main contributions are the following: (1) We introduce the use of specific error criteria for the design of the optimal Bayesian estimators for several classes of problems, and propose a general (Monte Carlo) procedure for approximating them. This new approach leads to a substantial improvement over the existing schemes, both regarding the quality of the results (particularly for low signal to noise ratios) and the computational efficiency. (2) We apply the Bayesian appraoch to the solution of several problems, some of which are formulated and solved in these terms for the first time. Specifically, these applications are: teh reconstruction of piecewise constant surfaces from sparse and noisy observationsl; the reconstruction of depth from stereoscopic pairs of images and the formation of perceptual clusters. (3) For each one of these applications, we develop fast, deterministic algorithms that approximate the optimal estimators, and illustrate their performance on both synthetic and real data. (4) We propose a new method, based on the analysis of the residual process, for estimating the parameters of the probabilistic models directly from the noisy observations. This scheme leads to an algorithm, which has no free parameters, for the restoration of piecewise uniform images. (5) We analyze the implementation of the algorithms that we develop in non-conventional hardware, such as massively parallel digital machines, and analog and hybrid networks.},
 month        = {sep~6},
 number       = {AITR-860},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-860.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-860.pdf},
}

@techreport{AITR-874,
 author       = {Robbins, Richard Elliot},
 title        = {BUILD},
 subtitle     = {A Tool for Maintaining Consistency in Modular Systems},
 year         = {1985},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Build is a tool for keeping modular systems in a consistent state by managing the construction tasks (e.g. compilation, linking, etc.) associated with such systems. It employs a user supplied system model and a procedural description of a task to be performed in order to perform the task. This differs from existing tools which do not explicitly separate knowledge about systems from knowledge about how systems are manipulated. BUILD provides a static framework for modeling systems and handling construction requests that makes use of programming environment specific definitions. By altering the set of definitions, BUILD can be extended to work with new programming environments to perform new tasks.},
 month        = {nov~6},
 number       = {AITR-874},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-874.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-874.pdf},
}

@techreport{AITR-900,
 author       = {Siegel, David Mark},
 title        = {Contact Sensors for Dexterous Robotic Hands},
 year         = {1986},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis examines a tactile sensor and a thermal sensor for use with the Utah-MIT dexterous four fingered hand. Sensory feedback is critical or full utilization of its advanced manipulatory capabilities. The hand itself provides tendon tensions and joint angles information. However, planned control algorithms require more information than these sources can provide. The tactile sensor utilizes capacitive transduction with a novel design based entirely on silicone elastomers. It provides an 8 x 8 array of force cells with 1.9 mm center-to-center spacing. A pressure resolution of 8 significant bits is available over a 0 to 200 grams per square mm range. The thermal sensor measures a material`s heat conductivity by radiating heat into an object and measuring the resulting temperature variations. This sensor has a 4 x 4 array of temperature cells with 3.5 mm center-to- center spacing. Experiments show that the thermal sensor can discriminate among material by detecting differences in their thermal conduction properties. Both sensors meet the stringent mounting requirements posed by the Utah-MIT hand. Combining them together to form a sensor with both tactile and thermal capabilities will ultimately be possible. The computational requirements for controlling a sensor equipped dexterous hand are severe. Conventional single processor computers do not provide adequate performance. To overcome these difficulties, a computational architecture based on interconnecting high performance microcomputers and a set of software primitives tailored for sensor driven control has been proposed. The system has been implemented and tested on the Utah-MIT hand. The hand, equipped with tactile and thermal sensors and controlled by its computational architecture, is one of the most advanced robotic manipulatory devices available worldwide. Other ongoing projects will exploit these tools and allow the hand to perform tasks that exceed the capabilities of current generation robots.},
 month        = {jun~6},
 number       = {AITR-900},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-900.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-900.pdf},
}

@techreport{AITR-901,
 author       = {Haase, Jr. Kenneth W.},
 title        = {ARLO},
 subtitle     = {Another Representation Language Offer},
 year         = {1986},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This paper describes ARLO, a representation language loosely modelled after Greiner and Lenant`s RLL-1. ARLO is a structure-based representation language for describing structure-based representation languages, including itself. A given representation language is specified in ARLO by a collection of structures describing how its descriptions are interpreted, defaulted, and verified. This high level description is compiles into lisp code and ARLO structures whose interpretation fulfills the specified semantics of the representation. In addition, ARLO itself- as a representation language for expressing and compiling partial and complete language specifications- is described and interpreted in the same manner as the language it describes and implements. This self- description can be extended of modified to expand or alter the expressive power of ARLO`s initial configuration. Languages which describe themselves like ARLO- provide powerful mediums for systems which perform automatic self-modification, optimization, debugging, or documentation. AI systems implemented in such a self- descriptive language can reflect on their own capabilities and limitations, applying general learning and problem solving strategies to enlarge or alleviate them.},
 month        = {oct~6},
 number       = {AITR-901},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-901.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-901.pdf},
}

@techreport{AITR-904,
 author       = {Wills, Linda M.},
 title        = {Automated Program Recognition},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The key to understanding a program is recognizing familiar algorithmic fragments and data structures in it. Automating this recognition process will make it easier to perform many tasks which require program understanding, e.g., maintenance, modification, and debugging. This report describes a recognition system, called the Recognizer, which automatically identifies occurrences of stereotyped computational fragments and data structures in programs. The Recognizer is able to identify these familiar fragments and structures, even though they may be expressed in a wide range of syntactic forms. It does so systematically and efficiently by using a parsing technique. Two important advances have made this possible. The first is a language-independent graphical representation for programs and programming structures which canonicalizes many syntactic features of programs. The second is an efficient graph parsing algorithm.},
 month        = {feb~6},
 number       = {AITR-904},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-904.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-904.pdf},
}

@techreport{AITR-905,
 author       = {Nguyen, Van-Duc},
 title        = {The Synthesis of Stable Force-Closure Grasps},
 year         = {1986},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis addresses the problem of synthesizing grasps that are force-closure and stable. The synthesis of force-closure grasps constructs independent regions of contact for the fingertips, such that the motion of the grasped object is totally constrained. The synthesis of stable grasps constructs virtual springs at the contacts, such that the grasped object is stable, and has a desired stiffness matrix about its stable equilibrium. A grasp on an object is force-closure if and only if we can exert, through the set of contacts, arbitrary forces and moments on the object. So force-closure implies equilibrium exists because zero forces and moment is spanned. In the reverse direction, we prove that a non-marginal equilibrium grasp is also a force-closure grasp, if it has at least two point contacts with friction in 2D, or two soft- finger contacts or three hard-finger contacts in 3D. Next, we prove that all force-closure grasps can be made stable, by using either active or passive springs at the contacts. The thesis develops a simple relation between the stability and stiffness of the grasp and the spatial configuration of the virtual springs at the contacts. The stiffness of the grasp depends also on whether the points of contact stick, or slide without friction on straight or curved surfaces of the object. The thesis presents fast and simple algorithms for directly constructing stable fore- closure grasps based on the shape of the grasped object. The formal framework of force-closure and stable grasps provides a partial explanation to why we stably grasp objects to easily, and to why our fingers are better soft than hard.},
 month        = {jul~6},
 number       = {AITR-905},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-905.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-905.pdf},
}

@techreport{AITR-906,
 author       = {Hall, Robert Joseph},
 title        = {Learning by Failing to Explain},
 year         = {1986},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Explanation-based Generalization requires that the learner obtain an explanation of why a precedent exemplifies a concept. It is, therefore, useless if the system fails to find this explanation. However, it is not necessary to give up and resort to purely empirical generalization methods. In fact, the system may already know almost everything it needs to explain the precedent. Learning by Failing to Explain is a method which is able to exploit current knowledge to prune complex precedents, isolating the mysterious parts of the precedent. The idea has two parts: the notion of partially analyzing a precedent to get rid of the parts which are already explainable, and the notion of re-analyzing old rules in terms of new ones, so that more general rules are obtained.},
 month        = {may~6},
 number       = {AITR-906},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-906.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-906.pdf},
}

@techreport{AITR-908,
 author       = {Harris, John G.},
 title        = {The Coupled Depth/Slope Approach to Surface Reconstruction},
 year         = {1986},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Reconstructing a surface from sparse sensory data is a well known problem in computer vision. Early vision modules typically supply sparse depth, orientation and discontinuity information. The surface reconstruction module incorporates these sparse and possibly conflicting measurements of a surface into a consistent, dense depth map. The coupled depth/slope model developed here provides a novel computational solution to the surface reconstruction problem. This method explicitly computes dense slope representation as well as dense depth representations. This marked change from previous surface reconstruction algorithms allows a natural integration of orientation constraints into the surface description, a feature not easily incorporated into earlier algorithms. In addition, the coupled depth/ slope model generalizes to allow for varying amounts of smoothness at different locations on the surface. This computational model helps conceptualize the problem and leads to two possible implementations- analog and digital. The model can be implemented as an electrical or biological analog network since the only computations required at each locally connected node are averages, additions and subtractions. A parallel digital algorithm can be derived by using finite difference approximations. The resulting system of coupled equations can be solved iteratively on a mesh-pf-processors computer, such as the Connection Machine. Furthermore, concurrent multi-grid methods are designed to speed the convergence of this digital algorithm.},
 month        = {jun~6},
 number       = {AITR-908},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-908.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-908.pdf},
}

@techreport{AITR-912,
 author       = {An, Chae Hun},
 title        = {Trajectory and Force Control of a Direct Drive Arm},
 year         = {1986},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Using the MIT Serial Link Direct Drive Arm as the main experimental device, various issues in trajectory and force control of manipulators were studied in this thesis. Since accurate modeling is important for any controller, issues of estimating the dynamic model of a manipulator and its load were addressed first. Practical and effective algorithms were developed fro the Newton-Euler equations to estimate the inertial parameters of manipulator rigid-body loads and links. Load estimation was implemented both on PUMA 600 robot and on the MIT Serial Link Direct Drive Arm. With the link estimation algorithm, the inertial parameters of the direct drive arm were obtained. For both load and link estimation results, the estimated parameters are good models of the actual system for control purposes since torques and forces can be predicted accurately from these estimated parameters. The estimated model of the direct drive arm was them used to evaluate trajectory following performance by feedforward and computed torque control algorithms. The experimental evaluations showed that the dynamic compensation can greatly improve trajectory following accuracy. Various stability issues of force control were studied next. It was determined that there are two types of instability in force control. Dynamic instability, present in all of the previous force control algorithms discussed in this thesis, is caused by the interaction of a manipulator with a stiff environment. Kinematics instability is present only in the hybrid control algorithm of Raibert and Craig, and is caused by the interaction of the inertia matrix with the Jacobian inverse coordinate transformation in the feedback path. Several methods were suggested and demonstrated experimentally to solve these stability problems. The result of the stability analyses were then incorporated in implementing a stable force/position controller on the direct drive arm by the modified resolved acceleration method using both joint torque and wrist force sensor feedbacks.},
 month        = {sep~6},
 number       = {AITR-912},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-912/AITR-912.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-912.pdf},
}

@techreport{AITR-918,
 author       = {Blelloch, Guy},
 title        = {AFL-1},
 subtitle     = {A Programming Language for Massively Concurrent Computers},
 year         = {1986},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Computational models are arising is which programs are constructed by specifying large networks of very simple computational devices. Although such models can potentially make use of a massive amount of concurrency, their usefulness as a programming model for the design of complex systems will ultimately be decided by the ease in which such networks can be programmed (constructed). This thesis outlines a language for specifying computational networks. The language (AFL- 1) consists of a set of primitives, ad a mechanism to group these elements into higher level structures. An implementation of this language runs on the Thinking Machines Corporation, Connection machine. Two significant examples were programmed in the language, an expert system (CIS), and a planning system (AFPLAN). These systems are explained and analyzed in terms of how they compare with similar systems written in conventional languages.},
 month        = {nov~6},
 number       = {AITR-918},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-918.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-918.pdf},
}

@techreport{AITR-925,
 author       = {Rozas, Guillermo Juan},
 title        = {A Computational Model for Observation in Quantum Mechanics},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A computational model of observation in quantum mechanics is presented. The model provides a clean and simple computational paradigm which can be used to illustrate and possibly explain some of the unintuitive and unexpected behavior of some quantum mechanical systems. As examples, the model is used to simulate three seminal quantum mechanical experiments. The results obtained agree with the predictions of quantum mechanics (and physical measurements), yet the model is perfectly deterministic and maintains a notion of locality.},
 month        = {mar~6},
 number       = {AITR-925},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-925.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-925.pdf},
}

@techreport{AITR-932,
 author       = {Gordon, Steven Jeffrey},
 title        = {Automated Assembly Using Feature Localization},
 year         = {1986},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Automated assembly of mechanical devices is studies by researching methods of operating assembly equipment in a variable manner; that is, systems which may be configured to perform many different assembly operations are studied. The general parts assembly operation involves the removal of alignment errors within some tolerance and without damaging the parts. Two methods for eliminating alignment errors are discussed: a priori suppression and measurement and removal. Both methods are studied with the more novel measurement and removal technique being studied in greater detail. During the study of this technique, a fast and accurate six degree-of- freedom position sensor based on a light- stripe vision technique was developed. Specifications for the sensor were derived from an assembly-system error analysis. Studies on extracting accurate information from the sensor by optimally reducing redundant information, filtering quantization noise, and careful calibration procedures were performed. Prototype assembly systems for both error elimination techniques were implemented and used to assemble several products. The assembly system based on the a priori suppression technique uses a number of mechanical assembly tools and software systems which extend the capabilities of industrial robots. The need for the tools was determined through an assembly task analysis of several consumer and automotive products. The assembly system based on the measurement and removal technique used the six degree-of- freedom position sensor to measure part misalignments. Robot commands for aligning the parts were automatically calculated based on the sensor data and executed.},
 month        = {dec~6},
 number       = {AITR-932},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-932.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-932.pdf},
}

@techreport{AITR-936,
 author       = {Buckley, Stephen J.},
 title        = {Planning and Teaching Compliant Motion Strategies},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis presents a new high level robot programming system. The programming system can be used to construct strategies consisting of compliant motions, in which a moving robot slides along obstacles in its environment. The programming system is referred to as high level because the user is spared of many robot-level details, such as the specification of conditional tests, motion termination conditions, and compliance parameters. Instead, the user specifies task- level information, including a geometric model of the robot and its environment. The user may also have to specify some suggested motions. There are two main system components. The first component is an interactive teaching system which accepts motion commands from a user and attempts to build a compliant motion strategy using the specified motions as building blocks. The second component is an autonomous compliant motion planner, which is intended to spare the user from dealing with ''simple'' problems. The planner simplifies the representation of the environment by decomposing the configuration space of the robot into a finite state space, whose states are vertices, edges, faces, and combinations thereof. States are inked to each other by arcs, which represent reliable compliant motions. Using best first search, states are expanded until a strategy is found from the start state to a global state. This component represents one of the first implemented compliant motion planners. The programming system has been implemented on a Symbolics 3600 computer, and tested on several examples. One of the resulting compliant motion strategies was successfully executed on an IBM 7565 robot manipulator.},
 month        = {jan~6},
 number       = {AITR-936},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-936.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-936.pdf},
}

@techreport{AITR-942,
 author       = {Atkeson, Christopher Granger},
 title        = {Roles of Knowledge in Motor Learning},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The goal of this thesis is to apply the computational approach to motor learning, i.e., describe the constraints that enable performance improvement with experience and also the constraints that must be satisfied by a motor learning system, describe what is being computed in order to achieve learning, and why it is being computed. The particular tasks used to assess motor learning are loaded and unloaded free arm movement, and the thesis includes work on rigid body load estimation, arm model estimation, optimal filtering for model parameter estimation, and trajectory learning from practice. Learning algorithms have been developed and implemented in the context of robot arm control. The thesis demonstrates some of the roles of knowledge in learning. Powerful generalizations can be made on the basis of knowledge of system structure, as is demonstrated in the load and arm model estimation algorithms. Improving the performance of parameter estimation algorithms used in learning involves knowledge of the measurement noise characteristics, as is shown in the derivation of optimal filters. Using trajectory errors to correct commands requires knowledge of how command errors are transformed into performance errors, i.e., an accurate model of the dynamics of the controlled system, as is demonstrated in the trajectory learning work. The performance demonstrated by the algorithms developed in this thesis should be compared with algorithms that use less knowledge, such as table based schemes to learn arm dynamics, previous single trajectory learning algorithms, and much of traditional adaptive control.},
 month        = {feb~6},
 number       = {AITR-942},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-942.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-942.pdf},
}

@techreport{AITR-963,
 author       = {Ettinger, Gil J.},
 title        = {Hierarchical Object Recognition Using Libraries of Parameterized Model Sub-Parts},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This thesis describes the development of a model-based vision system that exploits hierarchies of both object structure and object scale. The focus of the research is to use these hierarchies to achieve robust recognition based on effective organization and indexing schemes for model libraries. The goal of the system is to recognize parameterized instances of non-rigid model objects contained in a large knowledge base despite the presence of noise and occlusion. Robustness is achieved by developing a system that can recognize viewed objects that are scaled or mirror-image instances of the known models or that contain components sub-parts with different relative scaling, rotation, or translation than in models. The approach taken in this thesis is to develop an object shape representation that incorporates a component sub-part hierarchy- to allow for efficient and correct indexing into an automatically generated model library as well as for relative parameterization among sub- parts, and a scale hierarchy- to allow for a general to specific recognition procedure. After analysis of the issues and inherent tradeoffs in the recognition process, a system is implemented using a representation based on significant contour curvature changes and a recognition engine based on geometric constraints of feature properties. Examples of the system`s performance are given, followed by an analysis of the results. In conclusion, the system`s benefits and limitations are presented.},
 month        = {may~6},
 number       = {AITR-963},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-963.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-963.pdf},
}

@techreport{AITR-968,
 author       = {Voorhees, Harry},
 title        = {Finding Texture Boundaries in Images},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Texture provides one cue for identifying the physical cause of an intensity edge, such as occlusion, shadow, surface orientation or reflectance change. Marr, Julesz, and others have proposed that texture is represented by small lines or blobs, called 'textons' by Julesz [1981a], together with their attributes, such as orientation, elongation, and intensity. Psychophysical studies suggest that texture boundaries are perceived where distributions of attributes over neighborhoods of textons differ significantly. However, these studies, which deal with synthetic images, neglect to consider two important questions: How can these textons be extracted from images of natural scenes? And how, exactly, are texture boundaries then found? This thesis proposes answers to these questions by presenting an algorithm for computing blobs from natural images and a statistic for measuring the difference between two sample distributions of blob attributes. As part of the blob detection algorithm, methods for estimating image noise are presented, which are applicable to edge detection as well.},
 month        = {jun~6},
 number       = {AITR-968},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-968.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-968.pdf},
}

@techreport{AITR-972,
 author       = {Berwick, Robert C.},
 title        = {Principle-Based Parsing},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {During the past few years, there has been much discussion of a shift from rule-based systems to principle-based systems for natural language processing. This paper outlines the major computational advantages of principle-based parsing, its differences from the usual rule-based approach, and surveys several existing principle-based parsing systems used for handling languages as diverse as Warlpiri, English, and Spanish, as well as language translation.},
 month        = {jun~6},
 number       = {AITR-972},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-972.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-972.pdf},
}

@techreport{AITR-974,
 author       = {Riley, Michael D.},
 title        = {Time-Frequency Representations for Speech Signals},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {This work addresses two related questions. The first question is what joint time-frequency energy representations are most appropriate for auditory signals, in particular, for speech signals in sonorant regions. The quadratic transforms of the signal are examined, a large class that includes, for example, the spectrograms and the Wigner distribution. Quasi-stationarity is not assumed, since this would neglect dynamic regions. A set of desired properties is proposed for the representation: (1) shift-invariance, (2) positivity, (3) superposition, (4) locality, and (5) smoothness. Several relations among these properties are proved: shift-invariance and positivity imply the transform is a superposition of spectrograms; positivity and superposition are equivalent conditions when the transform is real; positivity limits the simultaneous time and frequency resolution (locality) possible for the transform, defining an uncertainty relation for joint time-frequency energy representations; and locality and smoothness tradeoff by the 2-D generalization of the classical uncertainty relation. The transform that best meets these criteria is derived, which consists of two-dimensionally smoothed Wigner distributions with (possibly oriented) 2-D guassian kernels. These transforms are then related to time-frequency filtering, a method for estimating the time- varying transfer function` of the vocal tract, which is somewhat analogous to ceptstral filtering generalized to the time-varying case. Natural speech examples are provided. The second question addressed is how to obtain a rich, symbolic description of the phonetically relevant features in these time-frequency energy surfaces, the so-called schematic spectrogram. Time-frequency ridges, the 2-D analog of spectral peaks, are one feature that is proposed. If non-oriented kernels are used for the energy representation, then the ridge tops can be identified, with zero-crossings in the inner product of the gradient vector and the direction of greatest downward curvature. If oriented kernels are used, the method can be generalized to give better orientation selectivity (e.g., at intersecting ridges) at the cost of poorer time-frequency locality. Many speech examples are given showing the performance for some traditionally difficult cases: semi- vowels and glides, nasalized vowels, consonant-vowel transitions, female speech, and imperfect transmission channels.},
 month        = {may~6},
 number       = {AITR-974},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-974.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-974.pdf},
}

@techreport{AITR-978,
 author       = {Weise, Daniel Wayne},
 title        = {Formal Multilevel Hierarchical Verification of Synchronous MOS Circuits},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {I have designed and implemented a system for the multilevel verification of synchronous MOS VLSI circuits. The system, called Silica Pithecus, accepts the schematic of an MOS circuit and a specification of the circuit`s intended digital behavior. Silica Pithecus determines if the circuit meets its specification. If the circuit fails to meet its specification Silica Pithecus returns to the designer the reason for the failure. Unlike earlier verifiers which modelled primitives (e.g., transistors) as unidirectional digital devices, Silica Pithecus models primitives more realistically. Transistors are modelled as bidirectional devices of varying resistances, and nodes are modelled as capacitors. Silica Pithecus operates hierarchically, interactively, and incrementally. Major contributions of this research include a formal understanding of the relationship between different behavioral descriptions (e.g., signal, boolean, and arithmetic descriptions) of the same device, and a formalization of the relationship between the structure, behavior, and context of device. Given these formal structures my methods find sufficient conditions on the inputs of circuits which guarantee the correct operation of the circuit in the desired descriptive domain. These methods are algorithmic and complete. They also handle complex phenomena such as races and charge sharing. Informal notions such as races and hazards are shown to be derivable from the correctness conditions used by my methods.},
 month        = {jun~6},
 number       = {AITR-978},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-978.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-978.pdf},
}

@techreport{AITR-979,
 author       = {McAllester, David Allen},
 title        = {ONTIC},
 subtitle     = {A Knowledge Representation System for Mathematics},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Ontic is an interactive system for developing and verifying mathematics. Ontic`s verification mechanism is capable of automatically finding and applying information from a library containing hundreds of mathematical facts. Starting with only the axioms of Zermelo- Fraenkel set theory, the Ontic system has been used to build a data base of definitions and lemmas leading to a proof of the Stone representation theorem for Boolean lattices. The Ontic system has been used to explore issues in knowledge representation, automated deduction, and the automatic use of large data bases.},
 month        = {jul~6},
 number       = {AITR-979},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-979.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-979.pdf},
}

@techreport{AITR-980,
 author       = {Mahoney, James V.},
 title        = {Image Chunking},
 subtitle     = {Defining Spatial Building Blocks for Scene Analysis},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Rapid judgments about the properties and spatial relations of objects are the crux of visually guided interaction with the world. Vision begins, however, with essentially pointwise representations of the scene, such as arrays of pixels or small edge fragments. For adequate time-performance in recognition, manipulation, navigation, and reasoning, the processes that extract meaningful entities from the pointwise representations must exploit parallelism. This report develops a framework for the fast extraction of scene entities, based on a simple, local model of parallel computation.sAn image chunk is a subset of an image that can act as a unit in the course of spatial analysis. A parallel preprocessing stage constructs a variety of simple chunks uniformly over the visual array. On the basis of these chunks, subsequent serial processes locate relevant scene components and assemble detailed descriptions of them rapidly. This thesis defines image chunks that facilitate the most potentially time- consuming operations of spatial analysis{\textemdash} boundary tracing, area coloring, and the selection of locations at which to apply detailed analysis. Fast parallel processes for computing these chunks from images, and chunk-based formulations of indexing, tracing, and coloring, are presented. These processes have been simulated and evaluated on the lisp machine and the connection machine.},
 month        = {aug~6},
 number       = {AITR-980},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-980.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-980.pdf},
}

@techreport{AITR-982,
 author       = {Donald, Bruce Randall},
 title        = {Error Detection and Recovery for Robot Motion Planning with Uncertainty},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Robots must plan and execute tasks in the presence of uncertainty. Uncertainty arises from sensing errors, control errors, and uncertainty in the geometry of the environment. The last, which is called model error, has received little previous attention. We present a framework for computing motion strategies that are guaranteed to succeed in the presence of all three kinds of uncertainty. The motion strategies comprise sensor- based gross motions, compliant motions, and simple pushing motions.},
 month        = {jul~6},
 number       = {AITR-982},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-982.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-982.pdf},
}

@techreport{AITR-988,
 author       = {Haase, Jr. Kenneth W.},
 title        = {TYPICAL},
 subtitle     = {A Knowledge Representation System for Automated Discovery and Inference},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {TYPICAL is a package for describing and making automatic inferences about a broad class of SCHEME predicate functions. These functions, called types following popular usage, delineate classes of primitive SCHEME objects, composite data structures, and abstract descriptions. TYPICAL types are generated by an extensible combinator language from either existing types or primitive terminals. These generated types are located in a lattice of predicate subsumption which captures necessary entailment between types; if satisfaction of one type necessarily entail satisfaction of another, the first type is below the second in the lattice. The inferences make by TYPICAL computes the position of the new definition within the lattice and establishes it there. This information is then accessible to both later inferences and other programs (reasoning systems, code analyzers, etc) which may need the information for their own purposes. TYPICAL was developed as a representation language for the discovery program Cyrano; particular examples are given of TYPICAL`s application in the Cyrano program.},
 month        = {aug~6},
 number       = {AITR-988},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-988.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-988.pdf},
}

@techreport{AITR-992,
 author       = {Brock, David L.},
 title        = {Enhancing the Dexterity of a Robot Hand Using Controlled Slip},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Humans can effortlessly manipulate objects in their hands, dexterously sliding and twisting them within their grasp. Robots, however, have none of these capabilities, they simply grasp objects rigidly in their end effectors. To investigate this common form of human manipulation, an analysis of controlled slipping of a grasped object within a robot hand was performed. The Salisbury robot hand demonstrated many of these controlled slipping techniques, illustrating many results of this analysis. First, the possible slipping motions were found as a function of the location, orientation, and types of contact between the hand and object. Second, for a given grasp, the contact types were determined as a function of the grasping force and the external forces on the object. Finally, by changing the grasping force, the robot modified the constraints on the object and affect controlled slipping slipping motions.},
 month        = {may~6},
 number       = {AITR-992},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-992.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-992.pdf},
}

@techreport{AITR-993,
 author       = {Kashket, Michael B.},
 title        = {A Government-Binding Based Parser for Warlpiri, a Free-Word Order Language},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Free-word order languages have long posed significant problems for standard parsing algorithms. This thesis presents an implemented parser, based on Government- Binding (GB) theory, for a particular free-word order language, Warlpiri, an aboriginal language of central Australia. The words in a sentence of a free-word order language may swap about relatively freely with little effect on meaning: the permutations of a sentence mean essentially the same thing. It is assumed that this similarity in meaning is directly reflected in the syntax. The parser presented here properly processes free word order because it assigns the same syntactic structure to the permutations of a single sentence. The parser also handles fixed word order, as well as other phenomena. On the view presented here, there is no such thing as a ''configurational'' or ''non-configurational'' language. Rather, there is a spectrum of languages that are more or less ordered. The operation of this parsing system is quite different in character from that of more traditional rule-based parsing systems, e.g., context-free parsers. In this system, parsing is carried out via the construction of two different structures, one encoding precedence information and one encoding hierarchical information. This bipartite representation is the key to handling both free- and fixed-order phenomena. This thesis first presents an overview of the portion of Warlpiri that can be parsed. Following this is a description of the linguistic theory on which the parser is based. The chapter after that describes the representations and algorithms of the parser. In conclusion, the parser is compared to related work. The appendix contains a substantial list of test cases {\textendash} both grammatical and ungrammatical {\textendash} that the parser has actually processed.},
 month        = {jan~6},
 number       = {AITR-993},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-993.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-993.pdf},
}

@techreport{AITR-995,
 author       = {Zhao, Feng},
 title        = {An O(N) Algorithm for Three-Dimensional N-Body Simulations},
 year         = {1987},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We develop an algorithm that computes the gravitational potentials and forces on N point- masses interacting in three-dimensional space. The algorithm, based on analytical techniques developed by Rokhlin and Greengard, runs in order N time. In contrast to other fast N-body methods such as tree codes, which only approximate the interaction potentials and forces, this method is exact {\textendash} it computes the potentials and forces to within any prespecified tolerance up to machine precision. We present an implementation of the algorithm for a sequential machine. We numerically verify the algorithm, and compare its speed with that of an O(N2) direct force computation. We also describe a parallel version of the algorithm that runs on the Connection Machine in order 0(logN) time. We compare experimental results with those of the sequential implementation and discuss how to minimize communication overhead on the parallel machine.},
 month        = {oct~6},
 number       = {AITR-995},
 url          = {ftp://publications.ai.mit.edu/ai-publications/500-999/AITR-995.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AITR-995.pdf},
}

@techreport{CBCL-101,
 author       = {Miyano, Takaya and Girosi, Federico},
 title        = {Forecasting Global Temperature Variations by Neural Networks},
 year         = {1994},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Global temperature variations between 1861 and 1984 are forecast usingsregularization networks, multilayer perceptrons and linearsautoregression. The regularization network, optimized by stochasticsgradient descent associated with colored noise, gives the bestsforecasts. For all the models, prediction errors noticeably increasesafter 1965. These results are consistent with the hypothesis that thesclimate dynamics is characterized by low-dimensional chaos and thatsthe it may have changed at some point after 1965, which is alsosconsistent with the recent idea of climate change.s},
 month        = {aug~23},
 number       = {CBCL-101},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1000-1499/AIM-1447.ps.Z; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1447.pdf},
}

@techreport{CBCL-103,
 author       = {Poggio, M. and Poggio, T.},
 title        = {Cooperative Physics of Fly Swarms},
 subtitle     = {An Emergent Behavior},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We have simulated the behavior of several artificial flies, interacting visually with each other. Each fly is described by a simple tracking system (Poggio and Reichardt, 1973; Land and Collett, 1974) which summarizes behavioral experiments in which individual flies fixate a target. Our main finding is that the interaction of theses implemodules gives rise to a variety of relatively complex behaviors. In particular, we observe a swarm-like behavior of a group of many artificial flies for certain reasonable ranges of our tracking system parameters.},
 month        = {apr~11},
 number       = {CBCL-103},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1512.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1512.pdf},
}

@techreport{CBCL-104,
 author       = {Sinha, Pawan},
 title        = {Reciprocal Interactions Between Motion and Form Perception},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {The processes underlying the perceptual analysis of visual form are believed to have minimal interaction with those subserving the perception of visual motion (Livingstone and Hubel, 1987; Victor and Conte, 1990). Recent reports of functionally and anatomically segregated parallel streams in the primate visual cortex seem to support this hypothesis (Ungerlieder and Mishkin, 1982; VanEssen and Maunsell, 1983; Shipp and Zeki, 1985; Zeki and Shipp, 1988; De Yoe et al., 1994). Here we present perceptual evidence that is at odds with this view and instead suggests strong symmetric interactions between the form and motion processes. In one direction, we show that the introduction of specific static figural elements, say 'F', in a simple motion sequence biases an observer to perceive a particular motion field, say 'M'. In the reverse direction, the imposition of the same motion field 'M' on the original sequence leads the observer to perceive illusory static figural elements 'F'. A specific implication of these findings concerns the possible existence of (what we call) motion end-stopped units in the primate visual system. Such units might constitute part of a mechanism for signalling subjective occluding contours based on motion-field discontinuities.},
 month        = {apr~21},
 number       = {CBCL-104},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1506.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1506.pdf},
}

@techreport{CBCL-106,
 author       = {Sinha, Pawan and Poggio, Tomaso},
 title        = {View-Based Strategies for 3D Object Recognition},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A persistent issue of debate in the area of 3D object recognition concerns the nature of the experientially acquired object models in the primate visual system. One prominent proposal in this regard has expounded the use of object centered models, such as representations of the objects` 3D structures in a coordinate frame independent of the viewing parameters [Marr and Nishihara, 1978]. In contrast to this is another proposal which suggests that the viewing parameters encountered during the learning phase might be inextricably linked to subsequent performance on a recognition task [Tarr and Pinker, 1989; Poggio and Edelman, 1990]. The object model`, according to this idea, is simply a collection of the sample views encountered during training. Given that object centered recognition strategies have the attractive feature of leading to viewpoint independence, they have garnered much of the research effort in the field of computational vision. Furthermore, since human recognition performance seems remarkably robust in the face of imaging variations [Ellis et al., 1989], it has often been implicitly assumed that the visual system employs an object centered strategy. In the present study we examine this assumption more closely. Our experimental results with a class of novel 3D structures strongly suggest the use of a view-based strategy by the human visual system even when it has the opportunity of constructing and using object-centered models. In fact, for our chosen class of objects, the results seem to support a stronger claim: 3D object recognition is 2D view-based.},
 month        = {apr~21},
 number       = {CBCL-106},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1518.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1518.pdf},
}

@techreport{CBCL-108,
 author       = {Ghahramani, Zoubin and Jordan, Michael I.},
 title        = {Learning from Incomplete Data},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Real-world learning tasks often involve high- dimensional data sets with complex patterns of missing features. In this paper we review the problem of learning from incomplete data from two statistical perspectives{\textemdash}the likelihood-based and the Bayesian. The goal is two-fold: to place current neural network approaches to missing data within a statistical framework, and to describe a set of algorithms, derived from the likelihood-based framework, that handle clustering, classification, and function approximation from incomplete data in a principled and efficient manner. These algorithms are based on mixture modeling and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster, Laird, and Rubin 1977){\textendash} -both for the estimation of mixture components and for coping with the missing data.},
 month        = {jan~24},
 number       = {CBCL-108},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1509.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1509.pdf},
}

@techreport{CBCL-109,
 author       = {Betke, Margrit and Makris, Nicholas},
 title        = {Fast Object Recognition in Noisy Images Using Simulated Annealing},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {A fast simulated annealing algorithm is developed for automatic object recognition. The normalized correlation coefficient is used as a measure of the match between a hypothesized object and an image. Templates are generated on-line during the search by transforming model images. Simulated annealing reduces the search time by orders of magnitude with respect to an exhaustive search. The algorithm is applied to the problem of how landmarks, for example, traffic signs, can be recognized by an autonomous vehicle or a navigating robot. The algorithm works well in noisy, real-world images of complicated scenes for model images with high information content.},
 month        = {jan~25},
 number       = {CBCL-109},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1510.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1510.pdf},
}

@techreport{CBCL-110,
 author       = {Cohn, Zoubin Ghahramani David A. and Jordan, Michael I.},
 title        = {Active Learning with Statistical Models},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {For many types of learners one can compute the statistically 'optimal' way to select data. We review how these techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate.},
 month        = {mar~21},
 number       = {CBCL-110},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1522.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1522.pdf},
}

@techreport{CBCL-111,
 author       = {Jordan, Michael and Xu, Lei},
 title        = {On Convergence Properties of the EM Algorithm for Gaussian Mixtures},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {''Expectation-Maximization'' (EM) algorithm and gradient-based approaches for maximum likelihood learning of finite Gaussian mixtures. We show that the EM step in parameter space is obtained from the gradient via a projection matrix $P$, and we provide an explicit expression for the matrix. We then analyze the convergence of EM in terms of special properties of $P$ and provide new results analyzing the effect that $P$ has on the likelihood surface. Based on these mathematical results, we present a comparative discussion of the advantages and disadvantages of EM and other algorithms for the learning of Gaussian mixture models.},
 month        = {apr~21},
 number       = {CBCL-111},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1520.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1520.pdf},
}

@techreport{CBCL-112,
 author       = {Sung, Kah Kay and Poggio, Tomaso},
 title        = {Example Based Learning for View-Based Human Face Detection},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based ''face'' and ''non-face'' prototype clusters. At each image location, the local pattern is matched against the distribution-based model, and a trained classifier determines, based on the local difference measurements, whether or not a human face exists at the current image location. We provide an analysis that helps identify the critical components of our system.},
 month        = {jan~24},
 number       = {CBCL-112},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1521.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1521.pdf},
}

@techreport{CBCL-113,
 author       = {Niyogi, Partha},
 title        = {Sequential Optimal Recovery},
 subtitle     = {A Paradigm for Active Learning},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {In most classical frameworks for learning from examples, it is assumed that examples are randomly drawn and presented to the learner. In this paper, we consider the possibility of a more active learner who is allowed to choose his/her own examples. Our investigations are carried out in a function approximation setting. In particular, using arguments from optimal recovery (Micchelli and Rivlin, 1976), we develop an adaptive sampling strategy (equivalent to adaptive approximation) for arbitrary approximation schemes. We provide a general formulation of the problem and show how it can be regarded as sequential optimal recovery. We demonstrate the application of this general formulation to two special cases of functions on the real line 1) monotonically increasing functions and 2) functions with bounded derivative. An extensive investigation of the sample complexity of approximating these functions is conducted yielding both theoretical and empirical results on test functions. Our theoretical results (stated insPAC-style), along with the simulations demonstrate the superiority of our active scheme over both passive learning as well as classical optimal recovery. The analysis of active function approximation is conducted in a worst-case setting, in contrast with other Bayesian paradigms obtained from optimal design (Mackay, 1992).},
 month        = {may~12},
 number       = {CBCL-113},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1514.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1514.pdf},
}

@techreport{CBCL-114,
 author       = {Niyogi, Partha and Berwick, Robert},
 title        = {A Dynamical Systems Model for Language Change},
 year         = {1995},
 institution  = {MIT Artificial Intelligence Laboratory},
 tags         = {__untagged},
 abstract     = {Formalizing linguists' intuitions of language change as a dynamical system, we quantify the time course of language change including sudden vs. gradual changes in languages. We apply the computer model to the historical loss of Verb Second from Old French to modern French, showing that otherwise adequate grammatical theories can fail our new evolutionary criterion.},
 month        = {dec~6},
 number       = {CBCL-114},
 url          = {ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1515.ps; ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-1515.pdf},
}
